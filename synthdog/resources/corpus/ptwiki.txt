Expedição polar de S. A. Andrée foi uma malfadada tentativa de alcançar o Polo Norte em 1897, na qual todos os três exploradores pereceram. S. A. Andrée, o primeiro balonista sueco, propôs uma jornada em um balão de hidrogênio partindo de Svalbard até à Rússia ou Canadá, cujo trajeto incluiria, com sorte, uma travessia sem escalas sobre o Polo Norte. O plano foi recebido com entusiasmo patriótico na Suécia, uma nação setentrional que ficara para trás na corrida para alcançar as zonas polares. Andrée negligenciou a maioria dos sinais de perigo relacionados a seu projeto. A capacidade de até certo ponto guiar o balão era essencial à segurança da viagem, e havia evidências o suficiente de que a técnica de empuxo de cordas desenvolvida por ele era ineficaz; ainda assim, ele confiou o destino da expedição ao mecanismo. Para piorar a situação, o balão polar Örnen (Águia) foi entregue diretamente em Svalbard por seu fabricante em Paris sem ter sido testado; quando as medições demonstraram que ele vazava mais do que o esperado, Andrée recusou-se a reconhecer as implicações alarmantes deste fato. A maioria dos estudiosos modernos da expedição vêem o otimismo, fé no poder da tecnologia e desconsideração pelas forças da natureza de Andrée como os principais fatores em uma série de eventos que provocaram sua morte e a de seus dois companheiros, Nils Strindberg e Knut Frænkel. Depois de Andrée, Strindberg e Frænkel terem decolado de Svalbard em julho de 1897, o balão perdeu hidrogênio rapidamente, caindo em um banco de gelo apenas dois dias depois da partida. Os exploradores não ficaram feridos, mas depararam-se com uma árdua caminhada de volta ao sul. Vestidos, preparados e equipados de forma inadequada, e surpreendidos pela dificuldade do terreno, eles não conseguiram safar-se da jornada. Enquanto o inverno ártico os envolvia em outubro, o grupo acabou exaurido na deserta Kvitøya (Ilha Branca), morrendo no local. Por trinta e três anos, o destino da expedição de Andrée permaneceu como um dos mistérios mais insolúveis do Ártico. A descoberta ao acaso em 1930 de seu derradeiro acampamento provocou uma histeria midiática na Suécia, onde os mortos foram lamentados e idolatrados. As motivações de Andrée foram posteriormente reconsideradas, juntamente com o papel das regiões polares como um campo de provas para a masculinidade e patriotismo. Um dos primeiros exemplos foi o romance Ingenjör Andrées luftfärd (O Voo da Águia), de Per Olof Sundman, que retrata Andrée como fraco e cínico, à mercê de seus patrocinadores e da mídia. O veredicto de estudiosos modernos a respeito de Andrée ter virtualmente sacrificado as vidas de seus dois jovens companheiros varia em aspereza, dependendo se ele é visto como o manipulador ou como a vítima do fervor nacionalista sueco da virada do século XX.
O acidente do ônibus espacial (português brasileiro) ou vaivém (português europeu) Columbia ocorreu no dia 1 de fevereiro de 2003, durante a fase de reentrada na atmosfera terrestre, a apenas dezesseis minutos de tocar o solo no regresso da missão STS-107, causando a destruição total da nave e a morte dos sete astronautas que compunham a tripulação. Esta missão, de objectivo científico, teve a duração de dezesseis dias, ao longo dos quais foram cumpridas com sucesso, as cerca de oitenta experiências programadas. Momentos após a desintegração do Columbia, milhares de destroços em chamas caíram sobre uma extensa faixa terrestre, essencialmente no estado do Texas, e na Louisiana, alguns dos quais atingiram casas de habitação, empresas e escolas. Entre a população ninguém ficou ferido. A recolha dos destroços prolongou-se de forma intensiva até meados de Abril daquele ano, ao longo de 40000 km², dos quais 2850 km² foram percorridos a pé, e os restantes utilizando meios aéreos ou navais junto à linha costeira da Califórnia. Foram recolhidos 83 mil pedaços do Columbia, correspondentes a 37% da massa total da nave. Entre os destroços, encontravam-se também parte dos restos mortais dos astronautas. Foi constituída uma comissão independente de inquérito ao acidente, a Columbia Accident Investigation Board (CAIB), que produziu um relatório oficial de quatrocentas páginas após quase sete meses de investigação, no qual foram apontadas as causas técnicas e organizacionais que estiveram directa ou indirectamente envolvidas na origem da destruição do Columbia. Foram ainda perspectivadas hipotéticas soluções de resgate da tripulação e elaboradas 29 recomendações a implementar, quinze das quais de cumprimento obrigatório, sem o qual não poderia haver um regresso aos voos.
O voo TAM 402 (ICAO: TAM 402) foi uma rota comercial doméstica do Brasil, operada pela TAM Linhas Aéreas, utilizando uma aeronave modelo Fokker 100 e que partia do Aeroporto de Caxias do Sul com destino ao Aeroporto Internacional do Recife, com escalas no Aeroporto Internacional Afonso Pena, em Curitiba, Aeroporto de Congonhas, em São Paulo, e no Aeroporto Santos Dumont, no Rio de Janeiro. Em 31 de outubro de 1996, enquanto decolava de São Paulo, o reversor de empuxo de um dos motores foi acionado, fazendo com que a aeronave perdesse velocidade e sustentação, caindo na Rua Luís Orsini de Castro. Todos os 96 passageiros e tripulantes a bordo do Fokker 100 mais três pessoas em solo morreram. É o acidente aéreo com mais mortes envolvendo um avião Fokker 100. O acidente, investigado pelo Centro de Investigação e Prevenção de Acidentes Aeronáuticos, órgão filiado à Força Aérea Brasileira, teve seu relatório final divulgado em dezembro de 1997, apontando como causa principal do acidente o acionamento do reversor de empuxo durante a decolagem, devido a manutenção inadequada aplicada pela empresa, o que causou a perda de sustentação e a queda.
A Asa N.º 90 foi uma asa da Real Força Aérea Australiana (RAAF) que operou durante os primeiros anos da Emergência Malaia. O seu propósito era servir como um comando para as unidades da RAAF destacadas no conflito, o Esquadrão N.º 1, que operava aviões Avro Lincoln, e o Esquadrão N.º 38, que operava aviões Douglas C-47 Dakota. Esta asa foi criada em Julho de 1950 e o seu quartel-general foi estabelecido em Changi, na costa leste de Singapura. O Esquadrão N.º 1 operava a partir da Base aérea de Tengah, no oeste de Singapura, e o Esquadrão N.º 38 a partir de Changi, exceto entre Abril de 1951 e Fevereiro de 1952, que operou a partir de Kuala Lumpur. Os aviões Lincoln realizavam geralmente operações de bombardeamento aéreo e bombardeamento estratégico, de modo a atormentar os insurgentes comunistas; os aviões Dakota ficaram responsáveis pelo transporte aéreo de mercadorias, tropas, feridos, personalidades VIP e correio. Depois de o Esquadrão N.º 38 ter cessado, em Dezembro de 1952, a sua participação no conflito, deixando o Esquadrão N.º 1 sozinho com a tarefa de realizar todas as operações aéreas, a Asa N.º 90 foi desativada, ficando o Esquadrão N.º 1 como a única unidade australiana no conflito até Julho de 1958.
A Unidade de Conversão Operacional N.º 2 (No. 2 OCU, mas também referida por OCU) (em inglês: No. 2 Operational Conversion Unit) é uma unidade de treino de aviões de combate da Real Força Aérea Australiana (RAAF). Localizada na base aérea de Williamtown, em Nova Gales do Sul, a unidade treina militares para a pilotagem de aviões McDonnel Douglas F/A-18 Hornet, conduz cursos de refrescamento para pilotos que voltam a pilotar a aeronave, e treina ainda futuros instrutores de pilotagem em Hornet. Os pilotos que nunca pilotaram esta aeronave entram na OCU depois de se qualificarem na pilotagem de aviões a jato no Esquadrão N.º 79 e depois de por uma instrução de aviões de combate aéreo no Esquadrão N.º 76. Uma vez qualificados no F/A-18, são colocados numa das unidades operacionais da Asa N.º 81: o Esquadrão N.º 3, o Esquadrão N.º 75 ou o Esquadrão N.º 77. Esta unidade foi criada como Unidade de Treino Operacional N.º 2 (No. 2 OTU) em Abril de 1942, em Port Pirie, no Sul da Austrália e, no mês seguinte, foi transferida para a base aérea de Mildura, em Vitória. Durante a Segunda Guerra Mundial, providenciou treino de pilotagem numa grande variedade de aeronaves como no P-40 Kittyhawk, Vultee Vengeance, Avro Anson, CAC Boomerang, Supermarine Spitfire e Airspeed Oxford. Extinta em Março de 1947, a No. 2 OTU voltou a iniciar operações em Williamtown em Março de 1952 em resposta à necessidade de uma melhor instrução de pilotagem, devido à participação australiana na Guerra da Coreia. Foi rebatizada como Unidade de Conversão Operacional N.º 2 em Setembro de 1958 e, ao longo da sua existência, conduziu treinos em aeronaves CAC Sabre, Dassault Mirage III e Macchi MB-326, conduzindo atualmente treino numa única aeronave, o Hornet.
O Academismo no Brasil foi a expressão institucionalizada de todo o sistema de arte que prevaleceu no Brasil do início do século XIX até o início do século XX, baseado nos princípios das academias de arte europeias. Nasceu com a Escola Real de Ciências, Artes e Ofícios fundada por Dom João VI em 1816 por incentivo da Missão Artística Francesa, floresceu com a Academia Imperial de Belas Artes e o mecenato de Dom Pedro II e encerrou-se com a incorporação de sua sucessora republicana, a Escola Nacional de Belas Artes, pela Universidade Federal do Rio de Janeiro, em 1931. Antes que um estilo específico, o Academismo é, estritamente falando, um método de ensino artístico profissionalizante de nível superior, equivalente ao ensino universitário moderno. No Brasil tal sistema foi introduzido no período de vigência do Neoclassicismo, estilo do qual foi um dos principais motores de difusão, e depois absorveu estéticas românticas, realistas, simbolistas e outras que deram o tom à virada do século XIX para o XX, expurgando delas o que não se enquadrasse na formalidade da Academia. A estreita ligação da arte acadêmica brasileira com o poder constituído ampliou o significado do termo fazendo do Academismo nacional tanto um sistema de ensino quanto um movimento filosófico e um ato político, como um laboratório para a formulação de importantes símbolos da identidade nacional e uma vitrine para a sua divulgação, contribuindo para tornar o seu tempo de vigência um dos mais ricos, complexos, movimentados e interessantes da história da arte brasileira. Seu pujante legado em arte permanece significante até os dias de hoje. Apesar de o termo Academismo se aplicar mais comumente na História da Arte brasileira ao período acima delimitado, o sistema acadêmico de ensino sobreviveu aos atropelos do Modernismo e das correntes vanguardistas do século XX, embora se tenha modificado, inserindo-se no ambiente das escolas de arte das modernas universidades, que hoje produzem e teorizam a arte em alto nível e são filhas diretas da Escola fundada por Dom João e os franceses.
Lisa Gherardini (Florença, 15 de junho de 1479 - Florença, 15 de julho de 1542 ou 1551) também conhecida como Mona Lisa, Lisa del Giocondo, Lisa di Antonio Maria, Antonmaria, era um membro da família Gherardini de Florença e da Toscana. Seu nome foi dado ao quadro Mona Lisa, trabalho encomendado por seu marido a Leonardo Da Vinci durante a Renascença italiana. Existem poucas informações sobre sua vida. Nascida em Florença, na Itália, casou-se ainda adolescente com um mercador de têxteis e de sedas, que viria a tornar-se uma autoridade local, era mãe de seis filhos e levava uma vida normal e comum da classe média da época, sendo bem mais jovem que seu marido. Séculos após sua morte, o quadro que a retrata, Mona Lisa, tornou-se a mais famosa pintura do mundo ganhando uma vida à parte da Lisa, a mulher. Devido especulações de estudiosos, a obra de arte tornou-se um ícone artístico mundial e, um objeto de comercialização. Em 2005, senhora Lisa foi definitivamente identificada como sendo a Mona Lisa. Na época do movimento Quatrocento, Florença era uma das maiores cidades da Europa, considerada rica e de economia desenvolvida, porém com grande disparidade em nível econômico e social. A família de Lisa era antiga e aristocrática, mas com o tempo diminuiu a influência na cidade. Tinham uma vida confortável mas não eram ricos, vivendo de rendimentos agrícolas. Lisa Gherardini era filha de Antonmaria di Noldo Gherardini e Lucrezia del Caccia (terceira esposa com quem se casou em 1476). Era a mais velha de sete filhos, tinha três irmãs, uma delas chamada Ginevra, e três irmãos, Giovangualberto, Francesco e Noldo. Lisa nasceu em Florença, na Via Maggio, apesar de por muitos anos os historiadores pensarem que ela havia nascido numa das propriedades rurais da família em Villa Vignamaggio e recebeu o nome de Lisa, uma esposa de seu avô paterno. A família viveu primeiro perto da Igreja da Santa Trindade e depois num espaço alugado perto da Basílica do Espírito Santo, mudando-se para onde hoje é conhecido como Via dei Pepi e depois para Santa Croce, um dos seis bairros centrais da cidade, onde viviam perto de Ser Piero Da Vinci, o pai de Leonardo. Eles também possuíam uma pequena casa de campo em Poggio a Caiano, cerca de 32 km ao sul da cidade. Noldo, o avô de Lisa, tinha arrendado uma pequena fazenda do hospital de Santa Maria Nuova e a família passava alguns verões lá, numa casa chamada Ca' di Pesa.
Doménikos Theotokópoulos (em grego: Δομήνικος Θεοτοκόπουλος), mais conhecido como El Greco, ("O Grego"; Heraclião ou Fodele, 1 de outubro de 1541 — Toledo, 7 de abril de 1614) foi um pintor, escultor e arquiteto grego que desenvolveu a maior parte da sua carreira na Espanha. Assinava suas obras com o nome original, ressaltando sua origem. Nasceu em Creta, que naquela época pertencia à República de Veneza e era um centro artístico pós-bizantino. Treinou ali e tornou-se um mestre dentro dessa tradição artística, antes de viajar, aos vinte e seis anos, para Veneza, como já tinham feito outros artistas gregos. Em 1570 mudou-se para Roma, onde abriu um ateliê e executou algumas séries de trabalhos. Durante sua permanência na Itália, enriqueceu seu estilo com elementos do maneirismo e da renascença veneziana. Mudou-se finalmente em 1577 para Toledo, na Espanha, onde viveu e trabalhou até sua morte. Ali, El Greco recebeu diversas encomendas e produziu suas melhores pinturas conhecidas. O estilo dramático e expressivo de El Greco foi considerado estranho por seus contemporâneos, mas encontrou grande apreciação no século XX, sendo considerado um precursor do expressionismo e do cubismo, ao mesmo tempo em que sua personalidade e trabalhos eram fonte de inspiração a poetas e escritores como Rainer Maria Rilke e Nikos Kazantzakis. El Greco é considerado pelo modernos estudiosos como um artista tão individual que não o consideram como pertencente a nenhuma das escolas convencionais. É mais conhecido por suas figuras tortuosamente alongadas e uso frequente de pigmentação fantástica ou mesmo fantasmagórica, unindo tradições bizantinas com a pintura ocidental. Em sua época teve somente dois seguidores de seu estilo: o seu filho Jorge Manuel Theotokópoulos e Luis Tristán. Seu nascimento em 1541 deu-se na vila de Fodele ou de Candia (nome veneziano para Chandax), presentemente chamada Heraclião, em Creta, era descendente de uma próspera família urbana, que provavelmente teria se deslocado de Chania para Candia depois de uma insurreição fracassada contra o domínio veneziano, entre 1526 e 1528. Seu pai, Geórgios Theotokópoulos (morto em 1558), era comerciante e cobrador de impostos. Nada se sabe, porém, sobre sua mãe ou sobre sua primeira esposa, uma grega. Seu irmão mais velho, Manoússos Theotokópoulos (1531 - 13 de dezembro de 1604), foi um comerciante, e passou seus últimos anos de vida na casa de El Greco, em Toledo. El Greco recebeu seus treinamentos iniciais como pintor de ícones na Escola cretense, o principal centro de arte pós-bizantina. Além da pintura, estudou provavelmente os clássicos da Grécia Antiga, e talvez os clássicos latinos também; quando morreu deixou uma "biblioteca de trabalho" com cerca de 130 volumes, inclusive um exemplar da Bíblia em grego e um Vasari anotado. Candia era então um centro das atividades artísticas onde as culturas ocidental e oriental conviviam harmoniosamente, e cerca de duzentos pintores eram ativos ali, durante o século XVI - inclusive tendo organizado uma guilda de pintores, ao molde corporativo italiano. Em 1563, com a idade de 22 anos, El Greco foi descrito num documento como um "mestre" ("maestro Domenigo"), significando com isso, provavelmente, que já era um membro de guilda e presumivelmente trabalhava em seu próprio estúdio. Três anos depois, em junho de 1566, como testemunha num contrato, ele assinou seu nome como "Mestre Menégos Theotokópoulos, pintor".
José Joaquim da Rocha (c. 1737 – Salvador, 12 de outubro de 1807) foi um pintor, encarnador, dourador e restaurador brasileiro. Tendo como exclusivo mecenas a Igreja católico-romana, sua produção se desenvolveu toda no domínio da arte sacra. Deixou obra numerosa e de caráter erudito, afastando-se da tradição popular tão comum nos tempos coloniais, e embora tenha muitos momentos de alto nível, é desigual, em parte porque desde que se tornou reconhecido sempre contou com muitos discípulos e aprendizes que o auxiliavam, aos quais entregava grandes porções do trabalho, e em parte pelo uso, como inspiração, de uma iconografia variada em gravura de qualidade irregular. Ambas as práticas eram, no entanto, comuns na época. Pintou muitas peças de cavalete, mas suas composições mais afamadas são os grandes tetos de igrejas realizados na técnica da perspectiva ilusionística, organizando complexas estruturas arquitetônicas virtuais ornamentadas com guirlandas e rocalhas, que sustentam um medalhão central, onde aparece a cena principal do conjunto, em geral apresentando Cristo ou a Virgem Maria em situações glorificantes. Como foi a praxe do período Barroco em que atuou, a pintura deveria edificar o observador e instruí-lo nos preceitos da Igreja, fazendo uso de uma plasticidade suntuosa e atraente ao olhar, que através da sedução dos sentidos levasse o devoto à contemplação das belezas do espírito. Apesar de já ter recebido a atenção crítica de vários especialistas de renome, o estudo de sua produção ainda carece de aprofundamento e muito ainda permanece no terreno da conjetura, em particular no que diz respeito à autoria, pois não assinou nenhuma obra e grande parte do que deixou é-lhe atribuído apenas com base na tradição oral, sem documentação corroborante, o que dificulta o entendimento da sua trajetória e estilo. A despeito dessas incertezas, a partir do que se conhece com mais segurança, José Joaquim da Rocha já foi reconhecido como artista de importância superior, considerado o fundador da escola baiana de pintura, o maior de seus integrantes e um dos grandes mestres do Barroco brasileiro. Deixou vários discípulos e influenciou duas gerações de continuadores, que preservaram princípios da sua estética até meados do século XIX.
Pedro Américo de Figueiredo e Melo (Areia, 29 de abril de 1843 — Florença, 7 de outubro de 1905) foi um romancista, poeta, cientista, teórico de arte, ensaísta, filósofo, político e professor brasileiro, mas é mais lembrado como um dos mais importantes pintores acadêmicos do Brasil, deixando obras de impacto nacional. Desde cedo demonstrou inclinação para as artes, sendo considerado um menino-prodígio. Ainda muito jovem participou como desenhista de uma expedição de naturalistas pelo nordeste, e recebeu apoio do governo para estudar na Academia Imperial de Belas Artes. Fez seu aperfeiçoamento artístico em Paris, estudando com mestres célebres, mas se dedicou também à ciência e à filosofia. Logo após seu retorno ao Brasil passou a dar aulas na Academia e iniciou uma carreira de sucesso, ganhando projeção com grandes pinturas de caráter cívico e heróico, inserindo-se no programa civilizador e modernizador do país fomentado pelo imperador Dom Pedro II, do qual a Academia Imperial era o braço regulador e executivo na esfera artística. Seu estilo na pintura, em consonância com as grandes tendências de seu tempo, fundia elementos neoclássicos, românticos e realistas, e sua produção é uma das primeiras grandes expressões do Academismo no Brasil em sua fase de apogeu, deixando obras que permanecem vivas até hoje no imaginário coletivo da nação, como Batalha de Avaí, Fala do Trono, Independência ou Morte! e Tiradentes esquartejado, reproduzidas em livros escolares de todo o país. Na segunda metade de sua carreira se concentrou em temas orientalizantes, alegóricos e bíblicos, que preferia pessoalmente e cujo mercado estava em expansão, mas esta parte de sua obra, em sua época muito popular, rápido saiu de moda, não recebeu atenção dos especialistas em tempos recentes e permanece muito pouco conhecida. Passou sua carreira entre o Brasil e a Europa, e em ambos os lugares seu talento foi reconhecido, recebendo grandes favores da crítica e do público mas também levantando polêmicas apaixonadas e criando tenazes adversários. Para as novas vanguardas Pedro Américo era um pintor de dotes inegavelmente raros, mas acima de tudo se tornou um dos principais símbolos de tudo o que o sistema acadêmico alegadamente tinha de conservador, elitista e distante da realidade nacional. Embora os modernistas tenham tentado ofuscar sua estrela — como a de todos os acadêmicos —, seus grandes méritos artísticos fazem dele um dos maiores pintores que o país já produziu, e sua imensa fama e influência em vida, os candentes debates que despertou em sua atuação institucional, cultural e política, em um momento crítico de articulação de um novo sistema de símbolos para um país há pouco emergente da condição de colônia e de consolidação de um novo sistema de arte sobre bases metodológicas e conceituais modernas, o destacam como um dos nomes mais importantes da história da cultura brasileira do fim do século XIX. Adquiriu uma sofisticação intelectual absolutamente incomum para os artistas brasileiros de seu tempo, interessando-se por uma ampla variedade de temas e buscando preparo sólido. Obteve Bacharelado em Ciências Sociais pela Sorbonne e Doutoramento em Ciências Naturais pela Universidade Livre de Bruxelas. Foi diretor da seção de antiguidades e numismática do Museu Imperial e Nacional; professor de desenho, estética e história da arte na Academia Imperial, e deputado constituinte por Pernambuco. Deixou volumosa produção escrita sobre estética, história da arte e filosofia, onde, inspirado no modelo clássico, deu especial atenção à educação como a base de todo o progresso e reservou um papel superior para a arte na evolução da humanidade. Ganhou diversas homenagens e honrarias, entre elas o título de Pintor Histórico da Imperial Câmara, a Ordem da Rosa e a Ordem do Santo Sepulcro. Também deixou algumas poesias e quatro romances, mas assim como seus textos teóricos, hoje são pouco lembrados.
A Igreja Matriz de Trindade, também conhecida como Igreja Matriz do Divino Pai Eterno, é uma igreja católica localizada na cidade brasileira de Trindade, em Goiás. Foi inaugurada em 8 de setembro de 1912 pelo missionário redentorista Antão Jorge e considerada Patrimônio Cultural do Brasil pelo Instituto do Patrimônio Histórico e Artístico Nacional em 24 de setembro de 2014. O templo está relacionado ao crescimento populacional e à instalação formal do município, a qual ocorreu oito anos depois, em 1920, em razão da atração de inúmeros fiéis que desejassem venerar ao Divino Pai Eterno. A Igreja Matriz de Trindade é, portanto, um dos pontos turísticos da cidade, principalmente durante a Festa do Divino Pai Eterno, repercutido evento religioso que acontece anualmente no final de junho e início de julho. Sua atual configuração data da última restauração, executada em 2013, ressaltando o estilo barroco e a tonalidade azul da fachada. Supervisionada pela Arquidiocese de Goiânia, a nave, o altar-mor e as alas laterais explicitam a simplicidade e a rusticidade dos objetos utilizados na construção da igreja.
A escultura do Classicismo grego tem sido longamente considerada como o ponto mais alto do desenvolvimento da arte escultórica na Grécia Antiga, tornando-se quase um sinônimo para "escultura grega" e eclipsando outros estilos que por lá foram cultivados em sua longa história. O Cânone, um tratado sobre as proporções do corpo humano escrito por Policleto em torno de 450 a.C., é tido geralmente como seu marco inicial, e seu fim é assinalado com a conquista da Grécia pelos macedônios, em 338 a.C., quando a arte grega começa uma grande difusão para o oriente, de onde recebe influências, muda seu caráter e se torna cosmopolita, na fase conhecida como Helenismo. Nesse intervalo é quando se consolida a tradição do Classicismo grego, tendo o homem como a nova medida do universo, e o reflexo disso na escultura é a primazia absoluta da representação do corpo humano nu. A escultura do Classicismo elaborou uma estética que conjugava valores idealistas com uma fidedigna representação da natureza, evitando, embora, a caracterização excessivamente realista e o retrato de extremos emocionais, mantendo-se geralmente numa atmosfera formal de equilíbrio e harmonia. Mesmo quando o personagem se encontra imerso em cenas de batalha, sua expressão parece pouco tocada pela violência dos acontecimentos. O Classicismo elevou o homem a um nível de dignidade sem precedentes, ao mesmo tempo em que lhe atribuiu a responsabilidade de criar seu próprio destino e ofereceu um modelo de vida harmonioso, num espírito de educação integral para uma cidadania exemplar. Esses valores, junto com sua tradicional associação de Beleza com Virtude, encontraram na escultura do período Clássico, com seu retrato idealizado do ser humano, um veículo particularmente apto para expressão, e um eficiente instrumento de educação cívica e ética, bem como estética. Com ela se inaugurou uma forma de representação do corpo humano que foi um dos fulcros para o nascimento de um ramo filosófico novo, a Estética, além de ter sido o fundamento estilístico de movimentos revivalistas posteriores de enorme importância, como o Renascimento e o Neoclassicismo, e permanecer influente até os dias de hoje. Assim, seu impacto sobre a cultura ocidental não pode ser suficientemente enfatizado, sendo uma referência central para o estudo da história da arte do ocidente. Mas, além de seu valor histórico, sua qualidade artística intrínseca poucas vezes foi questionada, a vasta maioria dos críticos antigos e modernos a enaltece com veemência, e os museus que a preservam são visitados por milhões de pessoas todos os anos. A escultura do Classicismo grego, embora sendo por vezes alvo de algumas críticas que relacionam suas bases ideológicas a preconceitos raciais, dogmatismos estéticos e outros exclusivismos, ainda pode ter um papel positivo e renovador a desempenhar para a arte e a sociedade contemporâneas.
Alfred North Whitehead (Ramsgate, 15 de fevereiro de 1861 – Cambridge, 30 de dezembro de 1947) foi um filósofo, lógico e matemático britânico. É o fundador da escola filosófica conhecida como a filosofia do processo, atualmente aplicada em vários campos da ciência, como dentre outros na ecologia, teologia, pedagogia, física, biologia, economia e psicologia. No início de sua carreira dedicou-se à matemática, à lógica e à física. Seu primeiro grande trabalho foi O Tratado sobre a Álgebra Universal (1898), onde se propôs a unificar a álgebra, a exemplo do que David Hilbert fez com a geometria não euclidiana. Seu trabalho mais notável sobre o assunto é o Principia mathematica (1910–1913), escrito com a colaboração de seu ex-aluno Bertrand Russell. O Principia Mathematica é considerado uma das obras mais importantes do século XX. Durante o período entre o final dos anos 1910 e o início dos anos 1920, Whitehead enveredou-se gradualmente para a filosofia da ciência e para a metafísica. Durante esse período, afastou-se do logicismo e passou a se dedicar à filosofia da natureza como mostrado nas obras Os Princípios do Conhecimento Natural (1919) e O Conceito da Natureza (1920). Em Os Princípios da Relatividade (1922) ele faz uma abordagem crítica à teoria da relatividade de Albert Einstein. Desenvolveu um sistema completo de metafísica que ocorre em meio à mudança e ao dinamismo, algo radicalmente diferente de tudo visto na filosofia ocidental até então. Atualmente a obra filosófica de Whitehead — principalmente sua Magnum Opus, Processo e Realidade (1929) — é considerada a fundadora da filosofia do processo. Sua metafísica é centrada nos conceitos de "apertos" (expressão que ele usa para indicar que uma percepção consciente ou inconsciente incorpora alguns aspectos do objeto percebido). Whitehead não busca explicar a teoria do conhecimento, e sim a experiência em si, distinguindo-se da metafísica de Immanuel Kant. A filosofia do processo de Whitehead pressupõe que "é urgente ver o mundo como uma rede de processos interdependentes da qual fazemos parte, e todas as nossas escolhas e nossas ações têm consequências onde vivemos". Por essa razão Whitehead foi muito influente nos estudos da ecologia, sobretudo na ética ambiental de John B. Cobb.
Charles Robert Darwin, FRS FGRS FLS FLZ (Shrewsbury, 12 de fevereiro de 1809 – Downe, 19 de abril de 1882) foi um naturalista, geólogo e biólogo britânico, célebre por seus avanços sobre evolução nas ciências biológicas. Juntamente com Alfred Wallace, Darwin estabeleceu a ideia que todos os seres vivos descendem de um ancestral em comum, argumento agora amplamente aceito e considerado um conceito fundamental no meio científico, e propôs a teoria de que os ramos evolutivos são resultados de seleção natural e sexual, onde a luta pela sobrevivência resulta em consequências similares às da seleção artificial. Seu livro de 1859, A Origem das Espécies, causou espanto na sociedade e comunidade científica da época, mas conseguiu grande aceitação nas décadas seguintes, superando a rejeição que os cientistas tinham pela transmutação de espécies. Já em 1870, a evolução por seleção natural tinha apoio da maioria dos intelectuais. Sua aceitação quase universal, entretanto, não foi atingida até à emergência da síntese evolutiva moderna entre as décadas de 1930 e 1950 quando um grande consenso consolidou a seleção natural como o mecanismo básico da evolução. A teoria de Darwin é considerada o mecanismo unificador para explicar a vida e a diversidade na Terra. Em seus primeiros anos, Darwin recusou cursar medicina na Universidade de Edimburgo; ao invés disso, focou-se em pesquisar sobre animais invertebrados. Pela Universidade de Cambridge (Christ's College), ele tomou a iniciativa pelas ciências naturais e viajou durante cinco anos pelo HMS Beagle, projeto que o lançou como eminente geólogo e cujas observações sustentaram as ideias de Charles Lyell; as publicações de seus diários sobre os trajetos percorridos consolidaram sua fama. Intrigado com a distribuição geográfica da vida selvagem e dos fósseis coletados durante sua viagem, Darwin começou investigações detalhadas e, em 1838, concebeu a teoria da seleção natural. Depois de discutir suas ideias com vários naturalistas, Darwin precisava de mais tempo para tornar sua ideia pública, algo que entrava em conflito com seu extensivo trabalho geológico que tinha prioridade. Em 1858, o naturalista Alfred Wallace manda um ensaio científico para Darwin estabelecendo as mesmas ideias e sugere uma publicação em conjunto. Consagrada a publicação, a teoria evolutiva darwiniana determinou drasticamente o cenário das ciências biológicas, tornando-se a explicação dominante sobre o porquê da diversidade natural do planeta. Em 1871, Darwin volta a publicar livros significativos, desta vez começando sobre a sexualidade humana e sua descendência, intitulado A Descendência do Homem e Seleção em Relação ao Sexo, seguido por A Expressão da Emoção em Homens e Animais em 1872. Sua dedicação pelas plantas resultaram em várias publicações de livros, e seu último seria The Formation of Vegetable Mould through the Action of Worms em 1881, meses antes de sua morte no ano seguinte. Em reconhecimento à importância do seu trabalho, Darwin foi enterrado na Abadia de Westminster, próximo a Charles Lyell, William Herschel e Isaac Newton. Foi uma das cinco pessoas não ligadas à família real inglesa a ter um funeral de Estado no século XIX. Por seu papel científico, Darwin é considerado uma das maiores personalidades da história.
A atmosfera de Júpiter é a maior atmosfera planetária do Sistema Solar. É composta principalmente de hidrogênio molecular e hélio em proporções similares às do Sol. Outros elementos e compostos químicos estão presentes em pequenas quantidades e incluem metano, amônia, sulfeto de hidrogênio e água. Embora acredite-se que a água esteja presente nas profundezas da atmosfera, sua concentração é muito baixa. A atmosfera joviana também possui oxigênio, nitrogênio, enxofre e gases nobres. A abundância destes elementos excede três vezes a do Sol. A atmosfera joviana não possui um limite interno, gradualmente transicionando em fluido no interior do planeta. De baixo para cima, as camadas atmosféricas são troposfera, estratosfera, termosfera e exosfera. Cada camada possui seu gradiente de temperatura característico. A camada mais baixa, a troposfera, possui um sistema complicado de nuvens, com camadas de amônia, hidrosulfeto de amônia e água. As nuvens superiores de amônia são visíveis da superfície do planeta e estão organizadas em um sistema de bandas paralelas ao equador, sendo limitadas por fortes correntes atmosféricas (ventos), conhecidas como jatos. As bandas alternam-se em cor: as bandas de cor mais escura são chamadas de cinturões, e as bandas de cor mais clara, de zonas. Zonas, que são mais frias que cinturões, correspondem às regiões nas quais o ar está se movendo para cima, enquanto nos cinturões o ar está se movendo em direção ao interior do planeta. Acredita-se que a cor das zonas seja resultado de gelo de amônia; não se sabe ainda com certeza o mecanismo que dá aos cinturões suas cores típicas. A origem das bandas e dos jatos não é bem entendida, mas existem dois modelos. O modelo superficial argumenta que tais bandas são fenômenos de superfície ocorrendo sobre um interior estável. Já no modelo profundo, as bandas e os jatos são fenômenos que resultam do movimento do hidrogênio molecular no interior do planeta. A atmosfera jupiteriana possui vários tipos de fenômenos ativos, incluindo instabilidades das bandas, vórtices (ciclones e anticiclones), tempestades e raios. Os vórtices são grandes ovais vermelhas, brancas ou marrons, sendo que os maiores são a Grande Mancha Vermelha e a Oval BA, ambas de cor vermelha, e, como a maioria dos vórtices de tamanho considerável, são anticiclônicos. Anticiclones menores tendem a ser brancos. Acredita-se que os vórtices sejam estruturas relativamente rasas, com profundidade não excedendo várias centenas de quilômetros. Localizada no hemisfério sul jupiteriano, a Grande Mancha Vermelha é o maior vórtice do Sistema Solar, podendo abrigar várias Terras dentro de si, e já existe por pelo menos três séculos. A Oval BA, localizada ao sul da Grande Mancha, possui um terço do tamanho, e foi formada em 2000 através da fusão de três ovais brancas menores. Júpiter possui fortes tempestades, sempre acompanhadas por raios. As tempestades são o resultado de convecção úmida na atmosfera, em conjunto com a evaporação e condensação de água. Estas regiões possuem fortes correntes de ar, que correm para cima, levando à formação de nuvens brilhantes e densas. As tempestades formam-se primariamente em cinturões. Os raios de Júpiter são mais potentes que os da Terra, mas ocorrem em menos quantidade, e os níveis de atividade de raios são comparáveis aos da Terra.
Mercúrio é o menor e mais interno planeta do Sistema Solar, orbitando o Sol a cada 87,969 dias terrestres. A sua órbita tem a maior excentricidade e o seu eixo apresenta a menor inclinação em relação ao plano da órbita dentre todos os planetas do Sistema Solar. Mercúrio completa três rotações em torno de seu eixo a cada duas órbitas. O periélio da órbita de Mercúrio apresenta uma precessão de 5.600 segundos de arco por século, um fenômeno completamente explicado apenas a partir do século XX pela Teoria da Relatividade Geral formulada por Albert Einstein. A sua aparência é brilhante quando observado da Terra, tendo uma magnitude aparente que varia de −2,6 a 5,7, embora não seja facilmente observado pois sua separação angular do Sol é de apenas 28,3°. Uma vez que Mercúrio normalmente se perde no intenso brilho solar, exceto em eclipses solares, só pode ser observado a olho nu durante o crepúsculo matutino ou vespertino. Em uma média ao longo do tempo, Mercúrio (e não Vênus) é o planeta mais próximo da Terra, do que os outros planetas do Sistema Solar, como demostrado em um estudo publicado em março de 2019 na revista Physics Today. Segundo os pesquisadores Tom Stockman, Gabriel Monroe e Samuel Cordner, os métodos convencionais para o cálculo do "planeta mais próximo da Terra" são simples demais. Popularizou-se na ciência que Vênus seria o planeta mais próximo da Terra, por uma suposição errônea sobre a distância média entre os planetas. Um método matemático criado pelos pesquisadores, determinou que, quando se calcula a média ao longo do tempo, o vizinho mais próximo da Terra é, na verdade Mercúrio. Essa correção é relevante para mais do que apenas os vizinhos da Terra. A solução pode ser generalizada para incluir quaisquer dois corpos em órbitas aproximadamente circulares, concêntricas e coplanares. Usando o método mais preciso para estimar a distância média entre dois corpos em órbita, a conclusão foi que que essa distância é proporcional ao raio relativo das órbitas internas. Em relação a outros planetas, pouco se sabe a respeito de Mercúrio, pois telescópios em solo terrestre revelam apenas um crescente iluminado com detalhes limitados. As duas primeiras espaçonaves a explorar o planeta foram a Mariner 10, que mapeou aproximadamente 45% da superfície do planeta entre 1974 e 1975, e a MESSENGER, que mapeou outros 30% da superfície durante um sobrevoo em 14 de janeiro de 2008. O último sobrevoo ocorreu em setembro de 2009 e a nave entrou em órbita do planeta em 18 de março de 2011, quando começou a mapear o restante do planeta, numa missão com duração nominal de um ano terrestre. Mercúrio tem uma aparência similar à da Lua com crateras de impacto e planícies lisas, não possuindo satélites naturais nem uma atmosfera substancial. Entretanto, diferentemente da Lua, possui uma grande quantidade de ferro no núcleo que gera um campo magnético, cuja intensidade é cerca de 1% da intensidade do campo magnético da Terra. É um planeta excepcionalmente denso devido ao tamanho relativo de seu núcleo. A temperatura em sua superfície varia de 90 a 700 K (−183 °C a 427 °C). O ponto subsolar é a região mais quente e o fundo das crateras perto dos polos as regiões mais frias. As primeiras observações registradas de Mercúrio datam pelo menos do primeiro milênio antes de Cristo. Antes do século IV a.C., astrônomos gregos acreditavam que se tratasse de dois objetos distintos: um visível no nascer do sol, ao qual chamavam Apolo, e outro visível ao pôr do Sol, chamado de Hermes. O nome em português para o planeta provém da Roma Antiga, onde o astro recebeu o nome do deus romano Mercúrio, que tinha na mitologia grega o nome de Hermes. O símbolo astronômico de Mercúrio é uma versão estilizada do caduceu de Hermes.
A Terra é o terceiro planeta mais próximo do Sol, o mais denso e o quinto maior dos oito planetas do Sistema Solar. É também o maior dos quatro planetas telúricos. É por vezes designada como Mundo ou Planeta Azul. Lar de milhões de espécies de seres vivos, incluindo os humanos, a Terra é o único corpo celeste onde é conhecida a existência de vida. O planeta formou-se há 4,56 bilhões de anos, e a vida surgiu na sua superfície um bilhão de anos depois. Desde então, a biosfera terrestre alterou significativamente a atmosfera e outros fatores abióticos do planeta, permitindo a proliferação de organismos aeróbicos, bem como a formação de uma camada de ozônio, a qual, em conjunto com o campo magnético terrestre, bloqueia radiação solar prejudicial, permitindo a vida no planeta. As propriedades físicas do planeta, bem como sua história geológica e órbita, permitiram que a vida persistisse durante este período. Acredita-se que a Terra poderá suportar vida durante pelo menos outros 500 milhões de anos. A sua superfície exterior está dividida em vários segmentos rígidos, chamados placas tectônicas, que migram sobre a superfície terrestre ao longo de milhões de anos. Cerca de 71% da superfície da Terra está coberta por oceanos de água salgada, com o restante consistindo de continentes e ilhas, os quais contêm muitos lagos e outros corpos de água que contribuem para a hidrosfera. Não se conhece a existência de água no estado líquido em equilíbrio, necessária à manutenção da vida como a conhecemos, na superfície de qualquer outro planeta. Os polos geográficos da Terra encontram-se maioritariamente cobertos por mantos de gelo ou por banquisas. O interior da Terra permanece ativo, com um manto espesso e relativamente sólido, um núcleo externo líquido que gera um campo magnético, e um núcleo interno sólido, composto sobretudo por ferro. A Terra interage com outros objetos no espaço, em particular com o Sol e a Lua. No presente, a Terra orbita o Sol uma vez por cada 366,26 rotações sobre o seu próprio eixo, o que equivale a 365,26 dias solares ou um ano sideral. O eixo de rotação da Terra possui uma inclinação de 23,4° em relação à perpendicular ao seu plano orbital, produzindo variações sazonais na superfície do planeta com período igual a um ano tropical (365,24 dias solares). A Lua é o único satélite natural conhecido da Terra, tendo começado a orbitá-la há 4,53 bilhões de anos. É responsável pelas marés, estabiliza a inclinação axial da Terra e abranda gradualmente a rotação do planeta. Entre aproximadamente 4,1 e 3,8 bilhões de anos atrás, durante o intenso bombardeio tardio, impactos de asteroides causaram mudanças significativas na superfície terrestre. Os recursos minerais da Terra em conjunto com os produtos da biosfera, fornecem recursos que são utilizados para suportar uma população humana global. Estes habitantes da Terra estão agrupados em cerca de 200 estados soberanos, que interagem entre si por meio da diplomacia, viagens, comércio e ação militar. As culturas humanas desenvolveram várias crenças sobre o planeta, incluindo a sua personificação em uma deidade, a crença numa Terra plana, ou em que a Terra é o centro do universo, e uma perspectiva moderna do mundo como um ambiente integrado que requer proteção.
O Universo é tudo o que existe fisicamente, a soma do espaço e do tempo e as mais variadas formas de matéria, como planetas, estrelas, galáxias e os componentes do espaço intergaláctico. O termo Universo pode ser usado em sentidos contextuais ligeiramente diferentes, denotando conceitos como o cosmo, o mundo ou a natureza. O universo observável tem de raio cerca de 46 bilhões de anos-luz. A observação científica do Universo levou a inferências de suas fases anteriores. Estas observações sugerem que o Universo é governado pelas mesmas leis físicas e constantes durante a maior parte de sua extensão e história. A teoria do Big Bang é o modelo cosmológico prevalente que descreve como o Universo evoluiu desde os primeiros 10-44 segundos (Tempo de Planck) até hoje. Observações de supernovas têm mostrado que o Universo está se expandindo a uma velocidade acelerada. Os valores anteriores para o número de galáxias no Universo giravam em, aproximadamente, cem bilhões de galáxias. Mas em outubro de 2016 dados reunidos em duas décadas de imagens colhidas pelo Hubble mostraram que o número de galáxias gira em torno de 20 vezes mais, saltando para 2 trilhões de galáxias, aproximadamente. Os espaços vazios do Universo podem estar repletos de matéria escura, de natureza ainda desconhecida. De acordo com o modelo científico vigente, conhecido como Big Bang, o Universo surgiu de um único ponto ou singularidade onde toda a matéria e energia do universo observável encontrava-se concentrada numa fase densa e extremamente quente chamada Era de Planck. A partir dessa era, o Universo vem-se expandindo, possivelmente em curtos períodos (menos que 10 − 32 segundos) de inflação cósmica. Diversas medições experimentais independentes apoiam teoricamente tal expansão e a teoria do Big Bang. Esta expansão tem-se acelerado por ação da energia escura, uma força contrária à gravidade que está agindo mais que esta devido ao fato das dimensões do Universo serem grandes o bastante para dissipar a força gravitacional. Porém, graças ao escasso conhecimento a respeito da energia escura, é ainda pequeno o entendimento do fenômeno e sua influência no destino do Universo. Há alguns anos, a sonda WMAP coletou dados que levaram à determinação da Idade do Universo em 13,73 (± 0,12) bilhões de anos. Entretanto, com base em dados coletados pelo satélite Planck, as interpretações de observações astronômicas indicam que a idade do Universo seria de 13,799 ± 0,021 bilhões de anos, enquanto o diâmetro do universo observável seria de 91 bilhões de anos-luz ou 8,80 × 1026 metros. De acordo com a teoria da relatividade geral, o espaço pode expandir-se a uma velocidade superior à da luz, embora possamos ver somente uma pequena fração da matéria visível do Universo devido à limitação imposta pela velocidade da luz. É incerto se a dimensão do espaço é finita ou infinita. Trezentos mil anos depois do Big Bang, teriam surgido átomos de matéria. As formas de vida teriam aparecido 11,2 bilhões de anos depois.
O declínio contemporâneo da biodiversidade mundial é um fenômeno que envolve a extinção ou significativa redução populacional de inúmeras espécies selvagens, bem como a destruição de ecossistemas em larga escala, e em anos recentes tem sido especialmente dramático na longa história da degradação ambiental causada pelo homem. Os efeitos danosos da ação humana sobre a vida natural são antigos. Começaram a surgir na pré-história, se aprofundaram a partir da consolidação das civilizações e continuam a se fazer notar com impacto crescente, em particular desde fins do séculos XIX, quando a industrialização se intensificou e a população do mundo começou a se expandir exponencialmente. De fato, as causas básicas para o declínio são a explosão demográfica, impondo cada vez maior pressão sobre os ambientes e os recursos naturais, e um modelo de civilização e desenvolvimento caracterizado pela insustentabilidade, explorando agressiva e abusivamente a natureza. Isso é a origem das causas imediatas para as perdas, entre as quais estão a degradação de ecossistemas, a caça e pesca predatórias, a poluição e outras perturbações, que agem interativamente potencializando seus efeitos. A biodiversidade íntegra e sadia é a fonte dos serviços ambientais oferecidos pela natureza, e que são vitais para a sociedade humana em uma infinidade de maneiras — como por exemplo provendo sustento, fertilizando os solos e protegendo-os da erosão, purificando as águas e o ar, regulando as chuvas e o clima em geral, fornecendo energia e uma multiplicidade de outros materiais e substâncias úteis — sem os quais a vida humana seria impossível. Também muito amiúde animais e plantas carregam fortes associações espirituais, folclóricas, artísticas e éticas com o homem, que lhes atribuiu ou encontrou neles poderes, inteligência, beleza, e mesmo sabedoria, e os reproduziu em sua arte, os viu em seus sonhos e falou com eles, aprendendo mistérios, e até os adorou como deuses. Disso deriva que o declínio da biodiversidade deve necessariamente prejudicar, como já prejudica, as pessoas de todo o mundo, seja direta ou indiretamente, em termos econômicos, políticos, culturais e sociais, e representa uma das maiores ameaças ambientais que o mundo hoje enfrenta, comprometendo também o futuro das novas gerações. Muitos programas e acordos internacionais, nacionais e regionais já foram lançados para tentar reverter esse quadro altamente preocupante, com destaque para o estabelecimento da Convenção sobre a Biodiversidade como o principal fórum internacional de debates científicos, técnicos e políticos de alto nível, mas tem sido pouco diante da gravidade do problema, que piora a cada dia, e que apenas em termos econômicos tem gerado prejuízos exorbitantes em escala global, chegando à casa dos trilhões de dólares anuais. A despeito disso, das amplas e sólidas evidências científicas já acumuladas, e mesmo com a considerável publicidade que o tema vem ganhando, a sociedade em geral ainda não percebeu ou não compreendeu devidamente o valor da biodiversidade, não apenas para a natureza em si, mas também para a civilização. Se esta situação não mudar rápido, segundo advertem de modo enfático as maiores autoridades no assunto, em um período de tempo relativamente exíguo o mundo enfrentará uma crise de proporções gigantescas e nunca vistas, cujos primeiros sinais já são auto-evidentes, sem garantia de que poderá ser revertida. No entanto, segundo as avaliações disponíveis, os custos do combate ao problema, mesmo em aparência altos, são pequenos em relação aos seus benefícios no longo prazo, e espera-se que o entendimento desta perspectiva possa estimular as instâncias oficiais e as comunidades em geral para que enfrentem juntas mais decididamente um desafio criado pelo próprio homem e que precisa de uma abordagem objetiva, honesta, vigorosa e imediata.
O ácido desoxirribonucleico (ADN, em português: ácido desoxirribonucleico; ou DNA, em inglês: deoxyribonucleic acid) é um composto orgânico cujas moléculas contêm as instruções genéticas que coordenam o desenvolvimento e funcionamento de todos os seres vivos e alguns vírus, e que transmitem as características hereditárias de cada ser vivo. A sua principal função é armazenar as informações necessárias para a construção das proteínas de ARNs. Os segmentos de ADN que contêm a informação genética são denominados genes. O restante da sequência de ADN tem importância estrutural ou está envolvido na regulação do uso da informação genética. A estrutura da molécula de ADN foi originalmente descoberta por Rosalind Franklin. No entanto, o Prémio Nobel de Fisiologia ou Medicina de 1962 foi entregue ao norte-americano James Watson e ao britânico Francis Crick, que se inspiraram em Franklin e demonstraram o funcionamento e a estrutura em dupla hélice do ADN em 7 de Março de 1953, juntamente com Maurice Wilkins. Do ponto de vista químico, o ADN é um longo polímero de unidades simples (monômeros) de nucleotídeos, cuja cadeia principal é formada por moléculas de açúcares e fosfato intercalados unidos por ligações fosfodiéster. Ligada à molécula de açúcar está uma de quatro bases nitrogenadas mantidas juntas por forças hidrofóbicas. A sequência de bases ao longo da molécula de ADN constitui a informação genética. A leitura destas sequências é feita por intermédio do código genético, que especifica a sequência linear dos aminoácidos das proteínas. A tradução é feita por um ARN mensageiro que copia parte da cadeia de ADN por um processo chamado transcrição e posteriormente a informação contida neste é "traduzida" em proteínas pela tradução. Embora a maioria do ARN produzido seja usado na síntese de proteínas, algum ARN tem função estrutural, como por exemplo o ARN ribossômico, que faz parte da constituição dos ribossomos. Dentro da célula, o ADN pode ser observado numa estrutura chamada cromossoma durante a metáfase. O conjunto de cromossomas de uma célula forma o cariótipo. Antes da divisão celular os cromossomas são duplicados por meio de um processo chamado replicação. Eucariontes como animais, plantas, fungos e protozoários têm o seu ADN dentro do núcleo enquanto que procariontes como as bactérias o têm disperso no citoplasma. Dentro dos cromossomas, proteínas da cromatina como as histonas compactam e organizam o ADN. Estas estruturas compactas guiam as interacções entre o ADN e outras proteínas, ajudando a controlar que partes do ADN são transcritas.
Cannabis (aportuguesado como cânabis ou canábis), também conhecida por vários nomes populares, refere-se a várias drogas psicoativas e medicamentos derivados de plantas do gênero Cannabis. Farmacologicamente, o principal constituinte psicoativo desse tipo de planta é o tetrahidrocanabinol (THC), um dos 400 compostos da planta, incluindo outros canabinoides, como o canabidiol (CBD), canabinol (CBN) e tetrahidrocanabivarin (THCV). A forma herbácea da droga consiste de flores e folhas maduras que subtendem das plantas pistiladas femininas. A forma resinosa, conhecida como haxixe, consiste fundamentalmente de tricomas glandulares coletados do mesmo material vegetal. A cannabis é frequentemente consumida por seus efeitos psicoativos e fisiológicos que podem incluir bom humor, euforia, relaxamento e aumento do apetite. Entre os efeitos colaterais indesejados estão a diminuição da memória de curto prazo, boca seca, dificuldade motora, vermelhidão dos olhos e sentimentos de paranoia ou ansiedade. O consumo humano da cannabis teve início no terceiro milênio a.C. e seu uso atual é voltado para recreação ou como medicamento, além de também ser usada como parte de rituais religiosos ou espirituais. A Organização das Nações Unidas (ONU) estima que cerca de quatro por cento da população mundial (162 milhões de pessoas) usam cannabis pelo menos uma vez ao ano e cerca de 0,6 por cento (22,5 milhões) consomem-na diariamente. A posse, o uso ou a venda da cannabis começou a se tornar ilegal no início do século XX em diversos países ocidentais, principalmente nos Estados Unidos. A proibição do consumo da erva se tornou global após a Convenção Internacional do Ópio, assinada em 1912 na cidade de Haia, quando diversas nações decidiram proibir o comércio mundial do "cânhamo indiano". Desde então, as leis que regulamentam a proibição da planta se intensificaram ao redor do mundo. Na últimas décadas, no entanto, surgiram diversos movimentos pela legalização da cannabis, enquanto alguns países e regiões passaram a permitir o uso do psicoativo sob certas circunstâncias, como foi o caso dos Países Baixos. Em 10 de dezembro de 2013, o Uruguai se tornou o primeiro país do mundo a legalizar o cultivo, a venda e o consumo da cannabis.
O ciclo menstrual é o termo científico para as alterações fisiológicas que ocorrem nas mulheres férteis que têm como finalidade a reprodução sexual e fecundação. Este artigo foca-se apenas no ciclo menstrual humano. O ciclo menstrual, regulado pelo sistema endócrino, é fundamental para a reprodução. É frequentemente dividido em três fases: a fase folicular, a ovulação e a fase luteínica, embora algumas fontes refiram um conjunto diferente de fases: a menstruação, a fase proliferativa e a fase secretora. Os ciclos menstruais contam-se a partir do primeiro dia de hemorragia menstrual. A contracepção hormonal intervém nas alterações hormonais naturais de forma a impedir a reprodução. Estimuladas por quantidades cada vez mais elevadas de estrogénio durante a fase folicular, as hemorragias menstruais abrandam até cessarem por completo, e o endométrio do útero torna-se mais espesso. Inicia-se então o desenvolvimento dos folículos nos ovários, através da influência de um conjunto complexo de hormonas. Após vários dias, um ou ocasionalmente dois dos folículos tornam-se dominantes, e os restantes atrofiam e morrem. Por volta do meio do ciclo, e 24 a 36 horas depois do pico de afluência de hormona luteinizante (LH), o folículo dominante liberta um óvulo durante um estágio designado por ovulação. Depois deste estágio, o óvulo apenas sobrevive durante 24 horas ou menos caso não ocorra fertilização, enquanto que os resquícios do folículo dominante no ovário se tornam corpos lúteos, produzindo grandes quantidades de progesterona. Estimulado pela presença desta hormona, o endométrio altera-se de modo a preparar-se para potenciais nidações de um embrião iniciando-se assim a gravidez. Caso a nidação não ocorra em aproximadamente duas semanas, o corpo lúteo involui, causando quedas abruptas nos níveis de progesterona e de estrogénio. Estas quebras indicam ao útero o momento para eliminar o óvulo e a sua membrana de revestimento, num processo designado por menstruação, terminando assim um ciclo. Durante o ciclo menstrual, ocorrem também alterações nos sistemas fisiológicos, sobretudo no sistema reprodutivo, que podem dar origem a mastodinia ou alterações de ânimo. A primeira menstruação da mulher é designada por menarca e ocorre frequentemente por volta dos 12 ou 13 anos. A idade média da menarca é de cerca de 12,2 anos em Portugal, 12,5 anos nos Estados Unidos, 12,72 no Canadá, 12,9 no Reino Unido, e 13,06 ± 0,1 anos na Islândia. O fim da fase reprodutiva da mulher designa-se menopausa e ocorre normalmente entre os 45 e 55 anos de idade.
Na biologia, Evolução (também conhecida como evolução biológica, genética ou orgânica) é a mudança das características hereditárias de uma população de seres vivos de uma geração para outra. Este processo faz com que as populações de organismos mudem e se diversifiquem ao longo do tempo. O termo "evolução" pode referir-se à evidência observacional que constitui o fato científico intrínseco à teoria da evolução biológica, ou, em acepção completa, à teoria em sua completude. Uma teoria científica é por definição um conjunto indissociável de todas as evidências verificáveis conhecidas e das ideias testáveis e testadas àquelas atreladas. Do ponto de vista genético, a evolução pode ser definida como qualquer alteração no número de genes ou na frequência dos alelos de um ou um conjunto de genes em uma população e ao longo das gerações. Mutações em genes podem produzir características novas ou alterar as que já existiam, resultando no aparecimento de diferenças hereditárias entre organismos. Estas novas características também podem surgir pela transferência de genes entre populações, como resultado de migração, ou entre espécies, resultante de transferência horizontal de genes. A evolução ocorre quando estas diferenças hereditárias tornam-se mais comuns ou raras numa população, quer de maneira não-aleatória, através de seleção natural, ou aleatoriamente, através de deriva genética. A seleção natural é um processo pelo qual características hereditárias que contribuem para a sobrevivência e a reprodução se tornam mais comuns numa população, enquanto características prejudiciais tornam-se mais raras. Isto ocorre porque indivíduos com características vantajosas têm mais sucesso na reprodução, de modo que mais indivíduos na próxima geração herdem tais características. Ao longo de muitas gerações, adaptações ocorrem através de uma combinação de mudanças sucessivas, pequenas e aleatórias nas características, mas significativas em conjunto, em virtude da seleção natural dos variantes mais adequados - adaptados - ao seu ambiente. Em contraste, a deriva genética produz mudanças aleatórias na frequência das características numa população. A deriva genética reflete o papel que o acaso desempenha na probabilidade de um determinado indivíduo sobreviver e reproduzir-se. Na década de 1930, a seleção natural darwiniana foi combinada com a hereditariedade mendeliana em uma síntese moderna, onde foi feita a ligação entre as unidades de evolução - os genes - e o mecanismo central de evolução - fundado na deriva genética e seleção natural. Tal teoria, denominada Síntese Evolutiva Moderna e detentora de um grande poder preditivo e explanatório, por oferecer uma unificadora e inigualável explicação natural para toda a diversidade da vida na Terra, tornou-se o pilar central da biologia moderna. Uma espécie pode ser definida como o agrupamento dos espécimes capazes de compartilhar material genético - usualmente por via sexuada - a fim de reproduzirem-se gerando descendência fértil. No entanto, quando uma espécie é separada em várias populações que por algum motivo não mais se possam cruzar, mecanismos como mutações, deriva genética e a selecção de características novas provocam a acumulação de diferenças ao longo de gerações, diferenças que, acumuladas, podem implicar desde curiosidades biológicas como os denominados anéis de espécies até a emergência de espécies novas e distintas. As semelhanças entre organismos sugere que todas as espécies conhecidas descenderam de um ancestral comum (ou pool genético ancestral) através deste processo de divergência gradual. Estudos do registro fóssil permitem reconstruir de forma satisfatória e precisa o processo de evolução da vida na Terra, desde os primeiros registros de sua presença no planeta - que datam de 3,4 mil milhões de anos atrás - até hoje. Tais fósseis, juntamente com o reconhecimento da fabulosa diversidade de seres vivos atrelada - a grande maioria hoje extinta - já em meados do século dezenove mostravam aos cientistas que as espécies encontram-se cronologicamente relacionadas, e que essas mudam ao longo do tempo. Contudo, os mecanismos que levaram a estas mudanças permaneceram pouco claros até o reconhecimento científico de que o próprio planeta tem uma história geológica muito rica - implicando mudanças ambientais constantes - e até a publicação do livro de Charles Darwin - A Origem das Espécies - detalhando a teoria de evolução por selecção natural. O trabalho de Darwin levou rapidamente à aceitação da evolução pela comunidade científica.
O complexo do exossoma, complexo PM/Scl ou simplesmente exossoma é um complexo proteico multienzimático envolvido em diferentes passos do processamento e degradação de vários tipos de moléculas de ARN (ácido ribunocleico), através da sua atividade exonucleolítica. Os complexos do exossoma podem encontrar-se tanto em células eucariotas como nas arqueobactérias. Nas bactérias equivale a um complexo mais simples, o degradossoma, que desempenha funções similares. O núcleo do complexo possui estrutura anular formada por seis membros, ao qual se acoplam outras proteínas. Nas células eucariotas encontra-se presente tanto no citoplasma como no núcleo celular, especialmente no nucléolo, embora as proteínas que interrelacionam com o complexo em cada um destes compartimentos sejam diferentes, regulando a sua atividade de degradação do ARN segundo a diferente natureza dos substratos presentes em cada compartimento celular. Estes substratos compreendem o ARN mensageiro, ARN ribossomal e muitas espécies de pequenos ARNs bacterianos. O exossoma é constituído por até 11 subunidades que possuem atividade exorribonucleolítica, o que significa que degradam o ARN a partir de uma extremidade (chamada extremidade 3', neste caso) em vez de fragmentar o ARN em pontos específicos da molécula (como uma endonuclease faria). Embora não haja relação causal entre o complexo e nenhuma das doenças conhecidas, algumas das suas proteínas são o antígeno de autoanticorpos responsáveis por algumas doenças autoimunes (especialmente pela síndrome de sobreposição de PM/Scl) e por algumas quimioterapias antimetabólicas que bloqueiam a actividade do complexo. O exossoma foi identificado inicialmente como ribonuclease em 1997 na levedura Saccharomyces cerevisiae a partir da co-purificação de proteínas como Rrp4p e separação por gradiente de glicerol. Não muito tempo depois, em 1999, viria a comprovar-se que o exossoma era, na verdade, o equivalente do complexo já descrito em células humanas, conhecido como complexo PM/Scl, que havia sido identificado nos anos anteriores como um autoantígeno em pacientes com certas doenças autoimunes (ver secção abaixo). A purificação deste complexo humano permitiu a identificação de mais proteínas do exossoma e finalmente a caracterização de todos os seus componentes. Em 2001 a crescente quantidade de dados procedentes dos distintos projetos genoma que se tornaram disponíveis permitiu a identificação de proteínas do exossoma de arqueas, ainda que tivesse transcorrido outros dois anos até que se identificasse o primeiro exossoma de um destes organismos.
Gravidez é o período de cerca de nove meses de gestação nas mulheres, contado a partir da fecundação e implantação de um óvulo no útero até ao nascimento. Durante a gravidez, o organismo materno passa por diversas alterações fisiológicas que sustentam o bebé em crescimento e preparam o parto. A fecundação pode dar-se através de relações sexuais ou ser medicamente assistida. Após a fecundação, o óvulo fecundado desloca-se ao longo de uma das trompas de Falópio e implanta-se na parede do útero, onde forma o embrião e a placenta que o alimentará. O desenvolvimento do embrião tem início com a divisão do óvulo em múltiplas células e é nesta fase que se começam a formar a maior parte dos órgãos, muitos deles funcionais. A partir das oito semanas de idade gestacional, o embrião passa a ser designado feto e apresenta já a forma humana que se desenvolverá continuamente até ao nascimento. O parto ocorre em média cerca de 38 semanas após a fecundação, o que corresponde a aproximadamente 40 semanas após o início do último período menstrual. Uma gravidez múltipla é a gravidez em que existe mais do que um embrião ou feto, como é o caso dos gémeos. Os primeiros sinais que indicam uma possível gravidez são a ausência de menstruação, sensibilidade nas mamas, náuseas, vómitos e aumento da frequência urinária. Uma gravidez pode ser confirmada com um teste de gravidez disponível em farmácias. A gravidez é convencionalmente dividida em três trimestres, de forma a simplificar a referência às diferentes fases do desenvolvimento pré-natal. O primeiro trimestre tem início com a fecundação e termina às doze semanas de idade gestacional, durante o qual existe risco acrescido de aborto espontâneo (morte natural do embrião ou do feto). Durante o segundo trimestre, o risco de aborto espontâneo diminui acentuadamente, a mãe começa a sentir o bebé, são visíveis os primeiros sinais exteriores da gravidez e o seu desenvolvimento é mais facilmente monitorizado. O terceiro trimestre é marcado pelo desenvolvimento completo do feto até ao nascimento. Os cuidados de saúde e os exames pré-natais apresentam uma série de benefícios para a saúde da grávida e do bebé. Entre os cuidados de saúde essenciais estão a suplementação com ácido fólico, a restrição do consumo de tabaco, álcool e drogas, a prática de exercício físico adequado à gravidez, a comparência às consultas de acompanhamento e a realização dos exames médicos e ecografias recomendados. Entre as complicações mais comuns estão a hipertensão, diabetes gestacional, anemia por deficiência de ferro e náuseas e vómitos graves. O termo da gravidez ocorre entre as 37 e as 41 semanas. Os bebés que nascem antes das 37 semanas são considerados pré-termo e depois das 41 semanas pós-termo. Os bebés prematuros apresentam risco acrescido de problemas de saúde. A indução de parto e cesariana não são recomendadas antes das 39 semanas, exceto por motivos médicos. Em 2012 ocorreram 213 milhões de gravidezes, das quais 190 milhões em países em vias de desenvolvimento e 23 milhões em países desenvolvidos. Isto corresponde a 133 gravidezes por cada 1 000 mulheres entre os 15 e 44 anos de idade. Cerca de 10 a 15% das gravidezes diagnosticadas terminam em aborto. Em 2013, as complicações da gravidez causaram a morte a 230.000 pessoas, uma diminuição em relação às 377.000 em 1990. Entre as causas mais comuns estão as hemorragias maternas, complicações de um aborto, hipertensão arterial, infeções, e complicações do parto. Cerca de 40% das gravidezes em todo o mundo não são planeadas, das quais metade resultam em aborto.
Guerra dos Ossos refere-se a um período de intensa especulação e descoberta de fósseis durante a chamada Gilded Age na história dos Estados Unidos, marcada por uma acalorada rivalidade entre Edward Drinker Cope, da Academy of Natural Sciences da Filadélfia, e Othniel Charles Marsh, do Museu Peabody de História Natural na Universidade de Yale. Os dois paleontólogos recorreram a métodos desonestos para se superarem, como roubos, subornos e destruição de ossos. Eles também se atacaram em trabalhos científicos, tentando arruinar a credibilidade do seu rival para deixá-lo sem financiamento para suas pesquisas. As suas buscas por fósseis os levaram para o oeste em direção aos ricos depósitos de ossos no Colorado, Nebraska e Wyoming. Entre 1877 e 1892, ambos usaram a sua riqueza e influência para financiar suas próprias expedições e obter serviços e ossos de dinossauros de caçadores de fósseis. No final da Guerra dos Ossos, ambos esgotaram os seus recursos na busca de supremacia paleontológica. Cope e Marsh foram economicamente e socialmente arruinados por seus esforços em desonrar um ao outro, mas as suas contribuições para a ciência e para a disciplina da paleontologia foram imensas e proveram material substancial para trabalhos futuros – ambos os cientistas deixaram muitas caixas fechadas de fósseis após suas mortes. A disputa entre os dois levou à descoberta e descrição de mais de 136 novas espécies de dinossauros. A Guerra dos Ossos teve como resultado uma maior compreensão da vida pré-histórica e despertou o interesse público nos dinossauros, levando a uma investigação mais aprofundada dos fósseis na América do Norte durante as décadas seguintes. Foi publicada uma considerável quantidade de livros históricos e adaptações de ficção sobre este período de intensa atividade paleontológica. Por um tempo, Cope e Marsh foram amigos. Eles se conheceram em Berlim, em 1864, e passaram alguns dias juntos. Eles até mesmo nomearam espécies em homenagem mútua. No entanto, a relação entre eles piorou com o tempo, devido em parte à natureza temperamental de ambos. Cope era conhecido por ser agressivo e tinha um temperamento explosivo, Marsh era mais lento, metódico e introvertido. Ambos eram briguentos e desconfiados. Suas diferenças também atingiram o nível científico: Cope era um forte defensor do neolamarquismo, enquanto que Marsh apoiava a teoria de Charles Darwin da evolução por seleção natural. Mesmo quando eles eram amigos, eles tinham uma tendência a desprezar-se sutilmente. Como um observador explicou: "O nobre Edward poderia ter considerado que Marsh não era exatamente um cavalheiro. O acadêmico Othniel provavelmente considerou que Cope como não era muito profissional". Cope e Marsh tinham origens muito diferentes. Cope nasceu em uma família quaker rica e influente na Filadélfia. Embora seu pai quisesse que ele trabalhasse como agricultor, Cope distinguiu-se como naturalista. Em 1864, já como membro da Academy of Natural Sciences, tornou-se professor de zoologia na Haverford College e se juntou a Ferdinand Hayden em suas expedições ao oeste. Marsh, filho de uma família de poucos recursos de Lockport, Nova Iorque, teria crescido na pobreza se não fosse a ajuda de seu tio, o filantropo George Peabody. Marsh convenceu seu tio a construir o Museu Peabody de História Natural e obteve o cargo de diretor do museu. Isso, junto com a herança que recebeu de Peabody quando ele morreu em 1869, tornou sua vida financeiramente confortável, mas, devido em parte à inflexibilidade de seu tio sobre o casamento, Marsh permaneceu solteiro por toda a vida. Em uma ocasião, os dois cientistas tinham ido em uma expedição para coletar fósseis nos depósitos de marga, em Nova Jersey, onde William Parker Foulke tinha descoberto o holótipo do Hadrosaurus foulkii, descrito pelo paleontólogo Joseph Leidy (com base nos estudos que Cope realizou sobre anatomia comparada), esta foi uma das primeiras descobertas de dinossauros nos Estados Unidos e os poços ainda eram ricos em fósseis. Embora os dois partilhassem amigavelmente o local, Marsh secretamente subornou os operadores dos poços para trazer-lhe os fósseis descobertos, ao invés de dar a Cope. Os dois começaram a atacar-se em documentos e publicações e suas relações pessoais deterioram-se. Marsh humilhou Cope indicando que a sua reconstrução do Elasmosaurus estava com defeito, com a cabeça posicionada onde a cauda deveria estar (ou assim ele disse, 20 anos depois; foi Leidy, que publicou a correção logo depois). Cope, por sua vez, começou a coletar onde Marsh considerava seu território privativo de coleta no Kansas e Wyoming, o que piorou a relação entre os dois.
A história da biologia traça o estudo do meio vivo desde a Antiguidade até aos tempos modernos. Embora o conceito de biologia enquanto campo científico único e coeso só tenha surgido no século XIX, as ciências biológicas têm origem nas práticas ancestrais de medicina e de história natural que remontam à Ayurveda, à medicina do Antigo Egito e às obras de Aristóteles e Galeno durante a Antiguidade clássica. Esta tradição pioneira continuou a ser aperfeiçoada durante a Idade Média por médicos islâmicos e académicos como Avicena. Durante o Renascimento e no início da Idade Moderna, o raciocínio científico na Europa foi drasticamente alterado com a introdução do empirismo e com a descoberta de inúmeras formas de vida. Entre as figuras de relevo deste movimento destacam-se Andreas Vesalius e William Harvey, introdutores do experimentalismo e da observação científica na fisiologia, e naturalistas como Carolus Linnaeus e Buffon, pioneiros da classificação das espécies e dos registos fósseis, para além de obras no comportamento e desenvolvimento dos seres vivos. A microscopia veio revelar o até então desconhecido mundo dos microorganismos, fornecendo as bases para a teoria celular. A importância crescente da teologia natural, em parte como resposta à ascensão da filosofia mecânica, veio a potenciar o crescimento da história natural, embora assumisse ainda o argumento teleológico do criacionismo. Ao longo dos séculos XVIII e XIX, as ciências biológicas como a botânica e a zoologia tornam-se campos de estudo cada vez mais profissionais. Inúmeros cientistas, como Lavoisier, começam a estabelecer ligações entre o mundo vivo e a matéria inanimada através da física e da química. Exploradores-naturalistas como Alexander von Humboldt investigam a interação entre os seres vivos e o seu meio físico, e a forma como esta relação é afetada pela geografia, estabelecendo as bases para a biogeografia, ecologia e etnologia. Os naturalistas começam a rejeitar o essencialismo e levam em conta a importância da extinção e da mutabilidade das espécies. A teoria celular forneceu uma nova perspetiva sobre os pilares fundamentais da vida. Estes progressos, em conjunto com as conclusões obtidas nos campos da embriologia e paleontologia, foram resumidos na teoria da evolução através da seleção natural de Charles Darwin. O fim do século XIX assistiu ao declínio da teoria da geração espontânea e à ascensão da teoria microbiana das doenças, embora o mecanismo da hereditariedade tivesse permanecido um mistério. No início do século XX, a redescoberta do trabalho de Gregor Mendel levou a progressos imediatos no campo da genética, sobretudo através de Thomas Hunt Morgan e dos seus alunos. Durante a década de 1930, a conjugação dos conceitos patentes na genética populacional e na seleção natural dá origem à síntese neodarwiniana. Estas novas disciplinas científicas desenvolvem-se rapidamente, sobretudo depois de Watson e Crick terem revelado a estrutura do ADN. Após a instituição do "Dogma Central" e da descodificação do código genético, a biologia foi separada entre biologia organismal - que lida com organismos completos e grupos de organismos - e as áreas relacionadas com a biologia celular e molecular. Já no fim do século XX, novas áreas como a genómica e a proteómica vieram inverter esta tendência, já que biólogos "organismais" empregam técnicas moleculares, e biólogos moleculares e celulares investigam também as interações entre os genes e o meio ambiente, assim como a genética das populações naturais de organismos.
O pensamento evolutivo, a concepção de que as espécies mudam ao longo do tempo, tem raízes na antiguidade, nas descobertas científicas de gregos, romanos, chineses e muçulmanos. No entanto, até o século XVIII, o pensamento biológico ocidental era dominado pelo essencialismo, a crença na imutabilidade das formas viventes. Essa concepção começou a se alterar quando, durante o iluminismo, a cosmologia evolutiva e a filosofia mecanicista se espalharam das ciências físicas para a história natural. Naturalistas começaram a centralizar seus estudos na variabilidade das espécies; o surgimento da paleontologia com o conceito de extinção enfraqueceu ainda mais a visão estática da natureza. No início do século XIX, Jean-Baptiste de Lamarck propôs sua teoria da transmutação de espécies, que foi a primeira teoria científica evolutiva totalmente elaborada. Em 1858, Charles Darwin e Alfred Russel Wallace publicaram uma nova teoria evolutiva, que foi explicada em detalhes no livro de Darwin, A origem das espécies, em 1859. Diferente de Lamarck, Darwin propôs o conceito de que os organismos apresentam uma origem comum, e se diferenciam de maneira a formar uma árvore da vida. A teoria fundamentava-se na concepção de seleção natural, e para proporcionar suporte ao seu argumento, Darwin apresentou uma grande quantidade de evidências oriundas de diferentes áreas: pecuária, biogeografia, geologia, morfologia e embriologia. O trabalho de Darwin conduziu a uma rápida aceitação do conceito de evolução, mas o mecanismo proposto, a seleção natural, não foi amplamente aceito até os anos 1940. A maioria dos biólogos argumentava que outros fatores eram responsáveis pela evolução, como a herança de caracteres adquiridos (neolamarquismo), uma tendência inata à mudança (ortogênese), ou grandes mutações repentinas (saltacionismo). A síntese da seleção natural com a genética Mendeliana durante os anos 1920 e 1930 fundou a nova disciplina da genética de populações. Durante os anos 1930 e 1940, a genética de populações foi integrada a outros campos da biologia, resultando numa teoria evolutiva amplamente aplicável, que abarcava a maior parte da biologia — a síntese evolutiva moderna. Em seguida ao estabelecimento da biologia evolutiva, estudos sobre mutação e variabilidade genética em populações naturais, aliados a biogeografia e sistemática, culminaram em sofisticados modelos matemáticos e causais de evolução. Juntas, a paleontologia e a anatomia comparada permitiram reconstruções mais detalhadas da história da vida. Após o surgimento da genética molecular nos anos 1950, o campo de estudo da evolução molecular se desenvolveu, baseado em sequências de proteínas e testes imunológicos, incorporando posteriormente estudos de RNA e DNA. Uma visão da evolução centrada nos genes ganhou proeminência nos anos 1960, seguida pela Teoria neutralista da evolução, gerando grandes debates sobre adaptacionismo, unidades de seleção natural e a importância relativa da deriva genética e da seleção natural. No fim do século XX, o surgimento de técnicas de sequenciamento de DNA permitiram a produção de filogenias moleculares e com isso a reorganização da árvore da vida em três domínios: (Archaea, Eukaria e Eubacteria). Além disso, a transferência horizontal de genes e fatores de simbiogênese recentemente descobertos introduziram ainda mais complexidade à história evolutiva.
Proteínas são macromoléculas biológicas constituídas por uma ou mais cadeias de aminoácidos. As proteínas estão presentes em todos os seres vivos e participam em praticamente todos os processos celulares, desempenhando um vasto conjunto de funções no organismo, como a replicação de ADN, a resposta a estímulos e o transporte de moléculas. Muitas proteínas são enzimas que catalisam reações bioquímicas vitais para o metabolismo. As proteínas têm também funções estruturais ou mecânicas, como é o caso da actina e da miosina nos músculos e das proteínas no citoesqueleto, as quais formam um sistema de andaimes que mantém a forma celular. Outras proteínas são importantes na sinalização celular, resposta imunitária e no ciclo celular. As proteínas diferem entre si fundamentalmente na sua sequência de aminoácidos, que é determinada pela sua sequência genética e que geralmente provoca o seu enovelamento numa estrutura tridimensional específica que determina a sua atividade. Ao contrário das plantas, os animais não conseguem sintetizar todos os aminoácidos de que necessitam para viver. Os aminoácidos que o organismo não é capaz de sintetizar por si próprio são denominados aminoácidos essenciais e devem ser obtidos pelo consumo de alimentos que contenham proteínas, as quais são transformadas em aminoácidos durante a digestão. As proteínas podem ser encontradas numa ampla variedade de alimentos de origem animal e vegetal. A carne, os ovos, o leite e o peixe são fontes de proteínas completas. Entre as principais fontes vegetais ricas em proteína estão as leguminosas, principalmente o feijão, as lentilhas, a soja ou o grão-de-bico. A grande maioria dos aminoácidos está disponível na dieta humana, pelo que uma pessoa saudável com uma dieta equilibrada raramente necessita de suplementos de proteínas. A necessidade é também maior em atletas ou durante a infância, gravidez ou amamentação, ou quando o corpo se encontra em recuperação de um trauma ou de uma operação. Quando o corpo não recebe as quantidades de proteínas necessárias verifica-se insuficiência e desnutrição proteica, a qual pode provocar uma série de doenças, entre as quais atraso no desenvolvimento em crianças ou kwashiorkor. Uma proteína contém pelo menos uma cadeia polímérica linear derivada da condensação de aminoácidos, ou polipeptídeo. Os resíduos individuais de aminoácidos estão unidos entre si através de ligações peptídicas. A sequência dos resíduos de aminoácidos em cada proteína é definida pela sequência de um gene, a qual está codificada no código genético. Durante ou após o processo de síntese, os resíduos de uma proteína são muitas vezes alterados quimicamente através de modificação pós-traducional, a qual modifica as propriedades físicas e químicas das proteínas, o seu enovelamento, estabilidade, atividade e, por fim, a sua função. Nalguns casos as proteínas têm anexados grupos não peptídicos, os quais são denominados cofatores ou grupos prostéticos. As proteínas podem também trabalhar em conjunto para desempenhar determinada função, agrupando-se em complexos proteicos. As proteínas podem ser purificadas a partir de outros componentes celulares recorrendo a diversas técnicas, como a precipitação, ultracentrifugação, eletroforese e cromatografia. Entre os métodos usados para estudar a estrutura e funções das proteínas estão a imuno-histoquímica, mutagénese sítio-dirigida, ressonância magnética nuclear e espectrometria de massa.
Rotavirus (em latim: rota, roda, pela sua forma) é um género de vírus de RNA bicatenário da família Reoviridae. É umas das principais causas de diarreia grave em lactentes e crianças jovens, e é um dos diversos vírus que causam infeções comummente chamadas de gastroenterites. Estima-se que, aos cinco anos de idade, quase todas as crianças do mundo tenham sido infectadas por um rotavírus ao menos uma vez. No entanto, como em cada infecção a imunidade se desenvolve, as infecções subsequentes são menos graves e adultos raramente são afectados. Existem oito espécies deste tipo de vírus, conhecidas como A, B, C, D, E, F, G e H. O Rotavírus A, o mais comum, é o responsável por mais de 90% das infecções em seres humanos. O vírus é transmitido pela rota fecal-oral. Infecta e danifica as células que revestem o intestino delgado (enterócitos) provocando gastroenterite. O rotavírus foi descoberto em 1973 e é responsável por 50% dos internamentos hospitalares devido a diarreias agudas em crianças. Para além do seu impacto na saúde humana, os rotavírus também causam infecções em animais, inclusive no gado. Causam gastroenterites limitadas com diarreia, vómitos, muita dor abdominal e náuseas, após transmissão em comida, objectos ou água infectada com vírus proveniente de fezes. É a causa mais comum de diarreia infecciosa nas crianças da Europa. A infecção de rotavírus é geralmente uma doença fácil de controlar na infância, no entanto, ainda assim, a cada ano em todo o mundo morrem cerca de 450 mil crianças com idades inferiores a cinco anos infectadas, a maioria das quais vive em países em vias de desenvolvimento económico, e quase dois milhões ficam gravemente doentes. O tratamento é feito pela administração de muita água com um pouco de sal e açúcar (soro caseiro), para restabelecer os líquidos e electrólitos perdidos na diarreia. As campanhas de higiene para combater os rotavírus centram-se na terapia de reidratação oral para crianças infectadas e na vacinação contra rotavírus. A incidência e gravidade das infecções do rotavírus baixaram significativamente nos países que incluíram a vacinação contra os rotavírus às suas normas rotineiras de imunização das crianças.
Estomatite aftosa (também chamada de estomatite aftosa recorrente, aftas orais recorrentes ou ulceração aftosa recorrente; em grego: ἄφθα; romaniz.: aphtha , "úlceras na boca") é uma condição comum caracterizada pela formação repetida de úlceras bucais benignas e não-contagiosas em indivíduos saudáveis. O termo informal afta também é predominantemente usado como um nome vulgar. Sua causa não é completamente compreendida, mas envolve uma resposta imune mediada por células T e desencadeada por uma variedade de fatores que variam entre os indivíduos: podem incluir deficiências nutricionais, trauma local, estresse, influências hormonais, alergias e predisposição genética, dentre outros. Essas úlceras ocorrem periodicamente e se curam completamente entre os ataques. Na maioria dos casos, as úlceras individuais duram cerca de 7 a 15 dias, e os episódios de ulceração ocorrem de 3 a 6 vezes por ano. A maioria delas aparece nas superfícies epiteliais não queratinizadas da boca (ou seja, em qualquer lugar exceto a gengiva inserida, o palato duro e o dorso da língua), embora as formas mais graves, que são menos comuns, também possam envolver as superfícies epiteliais queratinizadas. Os sintomas se constituem em pequenos incômodos que podem chegar a interferir no consumo de comidas e bebidas, no entanto as formas graves podem ser debilitantes, causando inclusive perda de peso devido à desnutrição. A condição é muito comum, afetando cerca de 20 por cento da população em geral em algum grau. O início é geralmente na infância ou adolescência e a condição geralmente dura por vários anos antes de desaparecer gradualmente. Não há cura e os tratamentos visam a controlar a dor, promover a cicatrização e reduzir a frequência de episódios de ulceração. Pessoas com estomatite aftosa não apresentam sinais ou sintomas sistêmicos (ou seja, fora da boca) detectáveis. Geralmente, os sintomas podem incluir sensações prodrômicas como ardor, coceira ou ardência, o que pode preceder o aparecimento de qualquer lesão por algumas horas; e dor, que muitas vezes é proporcional à extensão da úlcera e é agravada pelo contato físico, especialmente com certos alimentos e bebidas (por exemplo, ácido). A dor é pior nos dias imediatamente após o início da formação da úlcera e é aliviada com a progressão da cicatrização. Se houver lesões na língua, a fala e a mastigação podem se tornar desconfortáveis, enquanto úlceras no palato mole, orofaringe ou esôfago podem causar odinofagia (dor na garganta). Os sinais são limitados às próprias lesões. Os episódios de ulceração geralmente ocorrem cerca de 3 a 6 vezes por ano. No entanto, os casos graves são caracterizados por ulcerações praticamente constantes (novas lesões se desenvolvem antes das antigas se curarem) e podem causar dor debilitante crônica e interferir em uma alimentação confortável. Em casos graves, isso impede a ingestão adequada de nutrientes, levando à desnutrição e perda de peso. As úlceras aftosas geralmente começam como máculas eritematosas (área plana de mucosa avermelhada) que evoluem para úlceras recobertas por uma membrana fibrinosa amarelo-acinzentada que pode ser raspada. Um halo eritematoso envolve a úlcera. O tamanho, número, localização, tempo de cicatrização e periodicidade entre os casos de formação de úlcera são todos dependentes do subtipo de estomatite aftosa.
Asma é uma doença inflamatória crónica das vias aéreas. Quando as vias aéreas inflamadas são expostas a vários estímulos ou fatores desencadeantes tornam-se hiperreativas e obstruídas, limitando o fluxo de ar através de broncoconstrição, produção de muco e aumento da inflamação. Entre os sintomas mais comuns estão a pieira recorrente, tosse com agravamento noturno, sensação de aperto no peito e dificuldade respiratória recorrente. Pensa-se que a asma tenha origem numa conjugação de fatores genéticos e ambientais. Entre os fatores desencadeantes mais comuns estão os alergénios, como ácaros domésticos, baratas, pólen, pêlo de animais e fungos, e diversos fatores ambientais, como o fumo de tabaco ativo e passivo, a poluição do ar, irritantes químicos, exercício físico e determinados fármacos como a aspirina. A asma pode ser difícil de diagnosticar. Alguns dos sintomas de asma, como a dispneia aguda, o aperto torácico e a pieira, podem ser provocados por outras doenças. O diagnóstico é geralmente realizado com base no padrão dos sintomas, na comprovação da reversibilidade dos sintomas com broncodilatadores e nos resultados de exames de espirometria. A classificação clínica é feita de acordo com a frequência dos sintomas, do volume expiratório máximo no primeiro segundo e do débito expiratório máximo. A asma pode ser classificada como ligeira, moderada ou grave. As exacerbações ou crises agudas têm carácter episódico, embora a inflamação das vias aéreas seja crónica. As crises podem colocar a vida em risco, embora seja possível preveni-las. A gravidade da doença varia entre as pessoas e pode variar ao longo do tempo na mesma pessoa. Embora não exista cura para a asma, é possível controlar a frequência e intensidade dos sintomas. A primeira medida é evitar a exposição aos factores desencadeantes. Se não for suficiente, geralmente recomenda-se o uso de medicação, preferencialmente por via inalatória. Existem dois tipos de medicação para o controlo de asma: os medicamentos para alívio rápido dos sintomas e das crises de asma, como os broncodilatadores de curta duração, e os medicamentos de ação preventiva a longo prazo que previnem o aparecimento de sintomas ou de crises, particularmente os anti-inflamatórios. Encontram-se disponíveis vários dispositivos de inalação, como inaladores pressurizados, inaladores de pó seco e nebulizadores. As câmaras expansoras reduzem os efeitos secundários locais dos corticosteroides inalados e facilitam o uso dos inaladores pressurizados, sobretudo por parte de crianças. Em casos graves podem ser necessários corticosteroides intravenosos, sulfato de magnésio ou hospitalização. A doença requer tratamento a longo prazo e para muitas pessoas implica a utilização de medicamentos preventivos para o resto da vida. A ocorrência de asma tem aumentado significativamente desde a década de 1970. Em 2011, foram diagnosticadas com asma entre 235 e 300 milhões de pessoas e a doença foi responsável pela morte de 250.000 pessoas.
Câncer (português brasileiro) ou cancro (português europeu), também conhecido como neoplasia maligna é um grupo de doenças que envolvem o crescimento celular anormal, com potencial para invadir e espalhar-se para outras partes do corpo, além do local original. Há mais de cem diferentes cânceres conhecidos que afetam os seres humanos, mas nem todos os tumores são cancerosos (malignos); tumores benignos não se espalham pelo corpo. Sinais e sintomas possíveis incluem surgimento de uma massa cancerígena, sangramento anormal, tosse prolongada, perda de peso inexplicável, mudança nas funções intestinais, entre outros. Apesar de estes sintomas poderem indicar câncer, eles também podem ocorrer devido a outras doenças. O uso do tabaco é a causa de cerca de 22% das mortes, evitáveis, por câncer. Outros 10% ocorrem devido à obesidade, uma dieta pobre, falta de atividade física e consumo de bebidas alcoólicas. Entre outros, estão certos tipos de infecções, exposição à radiação ionizante e poluentes ambientais. No mundo em desenvolvimento, cerca de 20% dos cânceres surgem devido a infecções, tais como hepatite B, hepatite C e vírus do papiloma humano (HPV). Estes fatores atuam, pelo menos parcialmente, na alteração dos genes das células. Normalmente muitas dessas mudanças são necessárias para que o câncer se desenvolva. Entre 5% e 10% dos cânceres surgem por conta de defeitos genéticos hereditários. O câncer pode ser detectado através de certos sinais e sintomas ou por meio de testes de rastreio. Em seguida, geralmente é feita a investigação por imagens médicas e a confirmação pela biópsia. [9] Os benefícios do rastreio do câncer de mama ainda são controversos, mas a detecção precoce através de mamografia é útil para o câncer do colo do útero e colorretal. Muitos cânceres podem ser evitados ao: manter um peso ideal, comer muitos vegetais, frutas e grãos integrais, ser vacinado contra certas doenças infecciosas, não comer muita carne vermelha processada, evitar ingestão excessiva de álcool, de fumo e demasiada exposição à luz solar. O câncer é frequentemente tratado através da combinação de radioterapia, cirurgia, quimioterapia e terapia dirigida. A gestão da dor e dos sintomas é uma parte importante do tratamento. Os cuidados paliativos são particularmente importantes para os doentes com cânceres em estágios avançados. A chance de sobrevivência depende do tipo de câncer e da extensão da doença no início do tratamento. Em crianças menores de quinze anos no momento do diagnóstico a taxa de sobrevivência de cinco anos no mundo desenvolvido é, em média, de 81%. Nos Estados Unidos a taxa média de sobrevivência de cinco anos é de 66%. Em 2012, cerca de 14,1 milhões de novos casos de câncer ocorreram globalmente (excluindo casos de câncer de pele que não seja melanoma). A doença causou cerca de 8,2 milhões de mortes, ou 14,6% de todas as mortes humanas, além de um prejuízo anual de 2 trilhões de dólares na economia mundial (dados de 2015). Os tipos mais comuns de câncer nos homens são de pulmão, próstata, colorretal e de estômago. Nas mulheres, os tipos mais comuns são o câncer de mama, colorretal, de pulmão e cervical. Se o câncer de pele que não for melanoma for incluído no total de novos casos anuais, ele representará cerca de 40% dos registros da doença. Em crianças, leucemia linfoide aguda e tumores cerebrais agudos são os mais comuns, exceto na África, onde o linfoma não Hodgkin ocorre com mais frequência. Em 2012, cerca de 165 mil crianças com menos de quinze anos de idade foram diagnosticadas com câncer. O risco de câncer aumenta significativamente com a idade e muitos cânceres ocorrem mais comumente em países desenvolvidos devido à mudança no estilo de vida e à chegada da terceira idade.
O câncer do pulmão (português brasileiro) ou cancro do pulmão (português europeu), também conhecido como neoplasia pulmonar, é uma doença caracterizada pelo crescimento celular descontrolado em tecidos do pulmão. Se não for tratado, esse tumor pode se espalhar para fora do pulmão por um processo chamado de metástase, acometendo órgãos adjacentes e, eventualmente, se disseminando para outras partes do corpo. A maioria dos tumores que começam no pulmão (ou seja, tumores primários de pulmão) são cânceres derivados das células epiteliais (ou seja, carcinomas). Os principais tipos de câncer de pulmão são o adenocarcinoma (AC), o carcinoma de pulmão de células escamosas (CPCE), o carcinoma de pulmão de grandes células (CPGP) e o carcinoma de pulmão pequenas células (CPCP). A causa mais comum do câncer de pulmão é a exposição a longo prazo à fumaça do tabaco. A grande maioria (85%) dos casos de câncer de pulmão são causados pelo tabagismo a longo prazo. Não fumantes compreendem cerca de 10-15% dos casos, e são, frequentemente, atribuídos a fatores genéticos, gás radônio, asbesto ou poluição do ar, incluindo o tabagismo passivo. Os sintomas mais comuns são tosse (também a hemoptise), perda de peso e dificuldades na respiração. O câncer de pulmão pode ser visto na radiografia do tórax e na tomografia computadorizada (TC). O diagnóstico é confirmado por uma biópsia, que geralmente é realizada através de uma broncoscopia ou de uma biópsia guiada por TC. O tratamento e o prognóstico dependem do tipo histológico do tumor, do estágio (grau de extensão da doença) e do bem-estar geral do paciente, medido pelo estado funcional. Os tratamentos mais comuns são a cirurgia, a quimioterapia e a radioterapia. O CNPC pode ser tratado com cirurgia, ao passo que o CPC, normalmente, responde melhor à quimioterapia e à radioterapia. Isso se dá, parcialmente, porque o CPC frequentemente se espalha muito cedo e esses tratamentos são melhores em atingir as células que já se deslocaram para outras partes do corpo. A sobrevida depende do estágio, da saúde geral e de outros fatores, mas, em geral, 15% das pessoas diagnosticadas com câncer de pulmão nos Estados Unidos sobrevivem por, pelo menos, cinco anos após o diagnóstico. Em todo o mundo, o câncer de pulmão é a causa de morte provocada por algum tipo de câncer mais comum, em homens e em mulheres, e, em 2008, foi responsável por 1,37 milhão de mortes. Os sintomas que podem sugerir a possibilidade de um câncer de pulmão incluem: tosse crônica ou mudança no padrão regular da tosse (sintoma mais comum), dispneia (falta de ar), dor torácica, hemoptise (expectoração de sangue), sibilância (chiado no peito), dor abdominal, caquexia (perda de peso), fadiga e perda do apetite, disfonia (rouquidão), baqueteamento digital, disfagia (dificuldade ao engolir).
Cancro da mama (português europeu) ou câncer de mama (português brasileiro) ou carcinoma da mama é o cancro que se desenvolve no tecido mamário. Entre os sinais de cancro da mama estão o aparecimento de um nódulo na mama ou perto da mama na zona da axila; alterações na forma ou na aparência da mama, como retração do mamilo, pele da mama ou aréola, mamilo com aparência escamosa, vermelha, inchada ou com saliências e reentrâncias; ou ainda sensibilidade no mamilo e secreção ou perda de líquido pelo mamilo. Em pessoas com a doença disseminada, pode também verificar-se dor óssea, gânglios linfáticos inchados, falta de ar e icterícia. Entre os fatores de risco para o desenvolvimento de cancro da mama estão o sexo feminino, a idade, obesidade, falta de exercício físico, o consumo de álcool, a terapia de reposição hormonal durante a menopausa, radiação ionizante, idade precoce da primeira menstruação, ter filhos em idade tardia ou não ter tido filhos. Entre 5 e 10% dos casos são causados por genes herdados dos pais da pessoa, entre os quais o BRCA1 e o BRCA2. O cancro da mama geralmente desenvolve-se nas células do revestimento dos canais mamários e dos lóbulos onde é produzido o leite. Os cancros que se desenvolvem nos canais são denominados carcinomas ductais, enquanto que os que se desenvolvem nos lóbulos são denominados carcinomas lobulares. Existem ainda mais 18 sub-tipos de cancro da mama. Alguns cancros desenvolvem-se a partir de condições pré-malignas como o carcinoma ductal in situ. O diagnóstico de cancro da mama é confirmado através de uma biópsia do nódulo em questão. Uma vez realizado o diagnóstico, são realizados mais exames para determinar se o cancro se disseminou para além da mama e a que tratamentos pode responder. A ponderação entre as vantagens e desvantagens do rastreio do cancro da mama ainda é um tópico controverso. Uma revisão de 2013 da Cochrane afirmou que não é claro que o rastreio mamográfico tenha mais benefícios ou malefícios. Outra revisão de 2009 por um grupo de trabalho dos EUA (US Preventive Services Task Force) encontrou evidências de benefícios em pessoas com 40 a 70 anos de idade, tendo recomendado o rastreio a cada dois anos para as mulheres entre os 50 e 74 anos de idade. Como medida de prevenção, em pessoas com risco elevado de desenvolver cancro da mama, podem ser usados os fármacos tamoxifeno e raloxifeno. Outra medida de preventiva em mulheres com risco agravado é a remoção cirúrgica de ambas as mamas. Em pessoas a quem foi diagnosticado cancro, estão disponíveis uma série de tratamentos, incluindo cirurgia, radioterapia, terapia hormonal e terapia direcionada. Dependendo do tipo de intervenção necessário, a cirurgia pode conservar a mama (p.e. quadrantectomia) ou ser realizada uma mastectomia. A reconstrução de mama pode ter lugar durante a cirurgia ou numa data posterior. Nas pessoas em que o cancro já se disseminou para outras partes do corpo, os tratamentos destinam-se sobretudo a prolongar a vida, melhorar o conforto e a qualidade de vida. O prognóstico para o cancro da mama varia em função do tipo de cancro, extensão da doença e idade da pessoa. A taxa de sobrevivência em países desenvolvidos é elevada, sendo mais reduzida nos países em vias de desenvolvimento. Quando se considera todo o mundo, o cancro da mama é a causa mais comum de cancro em mulheres, correspondendo a 25% de todos os casos. Em 2012 foi responsável por 1,68 milhão de casos e 522000 mortes. É mais comum em países desenvolvidos e é mais de 100 vezes mais comum em mulheres do que em homens. O primeiro sintoma perceptível de cancro da mama é geralmente o aparecimento de um nódulo na mama ou na zona dos gânglios linfáticos das axilas, cuja sensação durante a palpação é diferente do tecido envolvente. Mais de 80% dos casos de cancro da mama são descobertos quando a mulher sente um nódulo durante a palpação. Para além dos nódulos, entre os sinais de um possível cancro da mama estão um espessamento diferente do restante tecido mamário; alterações na forma, tamanho ou aparência de uma mama; a retração ou alteração de posição ou forma de um mamilo; pele da mama, aréola ou mamilo com aparência escamosa, vermelha, inchada ou com saliências e reentrâncias; secreção ou perda de líquido pelo mamilo; dor constante em parte da mama ou da axila e inchaço por baixo da axila ou à volta da clavícula. 
Cancro do endométrio (português europeu) ou câncer endometrial (português brasileiro) é o cancro que tem origem no endométrio, o revestimento do útero. A doença é o resultado do crescimento anormal de células com a capacidade de invadir ou de se alastrar para outras partes do corpo. O primeiro sinal é geralmente uma hemorragia vaginal sem estar associada a um período menstrual; os outros sintomas incluem dor ao urinar ou durante o acto sexual ou dores pélvicas. O cancro do endométrio ocorre com maior frequência depois da menopausa. Este cancro é muitas vezes referido como "cancro do útero", embora seja distinto de outras formas de cancros do útero, como o cancro do colo do útero, sarcoma uterino ou doença trofoblástica. Cerca de 40% dos casos estão relacionados com a obesidade. A doença está também associada a uma exposição excessiva ao estrogénio, hipertensão e diabetes. Embora a ingestão de estrogénio isoladamente aumente o risco de cancro do endométrio, a ingestão de uma combinação de estrogénio com progesterona, como acontece na maioria das pílulas contraceptivas combinadas, diminui o risco. Entre 2 e 5% dos casos estão relacionados com genes herdados de um dos progenitores. O tipo histopatológico mais comum de cancro do endométrio é o carcinoma endometrioide, que é responsável por mais de 80% dos casos. O diagnóstico é geralmente realizado mediante uma biópsia endometrial ou pela recolha de uma amostra através de um procedimento denominado dilatação e curetagem. O teste de Papanicolau não costuma ser suficiente para confirmar a doença. Não está recomendado o rastreio regular em pessoas de risco normal. A opção de primeira linha para o tratamento de cancro do endométrio é a histerectomia abdominal, que consiste na cirurgia de remoção integral do útero, a par da remoção das trompas de Falópio e dos ovários de ambos os lados, um procedimento denominado ooforectomia. Em estágios mais avançados pode também ser recomendada a realização de radioterapia, quimioterapia ou terapia hormonal. Quando a doença é diagnosticada nos estágios iniciais, o prognóstico é favorável, com uma taxa de sobrevivência a cinco anos superior a 80%. Em 2012 houve 320 000 casos de cancro do endométrio que foram responsáveis por 76 000 mortes. A doença é a terceira causa mais comum de morte por cancro entre os cancros que afectam apenas as mulheres, atrás do cancro do ovário e do cancro do colo do útero. É mais comum em países desenvolvidos e é o cancro do sistema reprodutor feminino mais comum entre países desenvolvidos. Entre as décadas de 1980 e 2010, a prevalência de cancro do endométrio aumentou em vários países. Acredita-se que isto seja devido ao aumento do número de idosos e ao aumento da obesidade. Em 90% dos casos de cancro do endométrio após a menopausa verifica-se hemorragia vaginal ou metrorragia. As hemorragias são particularmente comuns nos casos de adenocarcinomas, correndo em dois terços de todos os casos. Antes da menopausa, a ocorrência de ciclos menstruais anormais ou extremamente longos e episódios de hemorragia frequentes ou intensos podem ser sinais de cancro do endométrio. Os restantes sintomas para além da hemorragia são poucos comuns. Entre estes sintomas estão secreções vaginais esbranquiçadas ou translúcidas em mulheres após a menopausa. À medida que a doença avança, os sintomas começam a ser mais perceptíveis ou aparecem sinais que podem ser observados num exame físico. O útero pode aumentar de tamanho ou o cancro alastrar-se, o que provoca dores no baixo abdome ou cãibras na pelve. A dor durante o ato sexual ou dor ao urinar são sinais pouco comuns. O útero pode também encher-se de pus. Entre as mulheres que manifestam estes sintomas menos comuns (secreções vaginais, dores pélvicas e pus), só 10-15% dos casos é que revelam ser cancro.
O cancro (português europeu) ou câncer (português brasileiro) do pâncreas surge quando as células do pâncreas, um órgão glandular atrás do estômago, se começam a multiplicar de forma descontrolada e formam um tumor. Estas células cancerígenas têm a capacidade de invadir outras partes do corpo. [10] Existem diversos tipos de cancro do pâncreas. O mais comum, adenocarcinoma pancreático, corresponde a aproximadamente 85% dos casos e em muitas situações o termo "cancro do pâncreas" é usado para designar apenas este tipo. Estes adenocarcinomas começam-se a formar na parte do pâncreas que produz as enzimas digestivas. A partir destas células podem também surgir vários outros tipos de cancro, que no conjunto representam a maioria dos não-adenocarcinomas. Um a dois em cada 100 casos de cancro do pâncreas são tumores neuroendócrinos que se formam a partir das células do pâncreas produtoras de hormonas. Este tipo é geralmente menos agressivo que o adenocarcinoma pancreático. Entre os sintomas mais comuns de cancro pancreático estão a coloração amarelada da pele, dores abdominais e nas costas, perda de peso inexplicável, fezes de coloração clara, urina escura e perda de apetite. Geralmente os sintomas não se manifestam durante as primeiras fases da doença e os sintomas que são suficientemente específicos para suspeitar de cancro pancreático só se manifestam quando a doença já se encontra num estádio avançado. Em muitos casos, no momento do diagnóstico o cancro já se disseminou para outras partes do corpo. O cancro do pâncreas raramente ocorre antes dos 40 anos de idade e mais de metade dos casos de adenocarcinoma pancreático ocorrem em pessoas com mais de 70. Entre os fatores de risco para a doença estão o tabagismo, obesidade, diabetes e determinadas condições genéticas raras. Mais de um quarto dos casos estão relacionados com o fumo do tabaco e 5–10% estão associados a genes hereditários. O cancro pancreático é geralmente diagnosticado com recurso a um conjunto de técnicas de imagiologia médica, como ecografia ou tomografia computadorizada, análises ao sangue e biópsia de amostras de tecido. A doença divide-se em estádios, desde o estádio I até ao estádio IV. Não há evidências de que o rastreio generalizado da população seja eficaz no diagnóstico precoce. O risco de desenvolver cancro do pâncreas é menor entre não fumadores e pessoas que mantêm um peso saudável e limitam o consumo de carne vermelha ou processada. A probabilidade de um fumador desenvolver a doença diminui após deixar de fumar, regressando aos valores da generalidade da população no prazo de 20 anos. O cancro do pâncreas pode ser tratado com cirurgia, radioterapia, quimioterapia, cuidados paliativos ou pela conjugação de vários destes métodos. As opções de tratamento dependem em parte do estádio do cancro. A cirurgia é o único tratamento que cura a doença, embora também possa ser realizada com o objetivo de melhorar a qualidade de vida da pessoa quando não exista possibilidade de cura. [1] Por vezes é necessária medicação e medidas para gestão da dor. Os cuidados paliativos são recomendados até para pessoas cujo tratamento se destine à cura. Em 2012, os cancros pancreáticos de todos os tipos foram a sétima causa mais comum de morte por cancro, correspondendo a 330 000 mortes à escala global. A prevalência da doença é maior em países desenvolvidos, os quais foram a origem de 70% dos novos casos em 2012. O adenocarcinoma pancreático apresenta geralmente um prognóstico muito reservado: após o diagnóstico, apenas 25% das pessoas sobrevive mais de um ano e apenas 5% sobrevive por mais de cinco anos. Nos casos em que o cancro é diagnosticado durante a fase inicial, a taxa de sobrevivência após cinco anos aumenta para aproximadamente 20%. Os cancros neuroendócrinos apresentam um diagnóstico mais positivo; após cinco anos, a taxa de sobrevivência é de 65%, embora dependa consideravelmente do tipo de tumor. Os diversos tipos de cancro do pâncreas podem ser divididos em dois grupos genéricos. A grande maioria dos casos (99%) ocorre na parte do pâncreas que produz enzimas digestivas, denominada de componente exócrino. Embora existam vários subtipos de cancros pancreáticos exócrinos, o diagnóstico e tratamento é em grande parte idêntico. A pequena minoria de cancros que surge no tecido produtor de hormonas (endócrino) apresenta características clínicas diferentes. Ambos os grupos ocorrem principalmente (mas não de forma exclusiva) em pessoas com mais de 40 anos e são ligeiramente mais comuns em homens. No entanto, alguns subtipos mais raros ocorrem principalmente em mulheres ou crianças.
Colangiocarcinoma, também conhecido como câncer de vias biliares, é uma forma de câncer que se forma nos ductos biliares. Os sintomas da doença podem incluir dor abdominal, pele amarelada, perda de peso, prurido generalizado e febre. Também podem ocorrer fezes esbranquiçadas ou urina escura. Outros cânceres do trato biliar incluem o de vesícula biliar e o de ampola de Vater. Os fatores de risco incluem colangite esclerosante primária (uma doença inflamatória dos ductos biliares), colite ulcerosa, cirrose, hepatite C, hepatite B, infecção por certos parasitas de fígado e algumas malformações congênitas do órgão. No entanto, a maioria das pessoas não possui fatores de risco identificáveis. Suspeita-se do diagnóstico com base em uma combinação de exames de sangue, imagiologia médica, endoscopia e, às vezes, exploração cirúrgica. A doença é confirmada pelo exame de células do tumor em microscópio. É tipicamente um adenocarcinoma (um câncer que forma glândulas ou secreta mucina). O diagnóstico é tipicamente incurável. Nesses casos, os tratamentos paliativos podem incluir ressecção cirúrgica, quimioterapia, radioterapia e procedimentos de stent. Em cerca de um terço dos casos que envolvem o ducto biliar comum e menos comumente em outros locais, o tumor pode ser completamente removido por cirurgia, oferecendo a chance de cura. Mesmo quando a remoção cirúrgica é bem sucedida, quimioterapia e radioterapia são geralmente recomendadas. Em certos casos, a cirurgia pode incluir um transplante de fígado. Mesmo quando a cirurgia é bem sucedida, a sobrevida em cinco anos é tipicamente menor que 50%. O colangiocarcinoma é raro no mundo ocidental, com estimativas ocorrendo em 0,5 a 2 pessoas por 100 000 ao ano. As taxas são mais altas no sudeste da Ásia, onde parasitas de fígado são comuns. As taxas em partes da Tailândia são de 60 por 100 000 ao ano. Geralmente ocorre em pessoas na faixa dos 70 anos; no entanto, naqueles com colangite esclerosante primária, ocorre frequentemente nos anos 40. As taxas de colangiocarcinoma no fígado no mundo ocidental têm aumentado. [8] As indicações físicas mais comuns de colangiocarcinoma são testes de função hepática anormais (amarelecimento dos olhos e da pele que ocorre quando os ductos biliares são bloqueados pelo tumor), icterícia, dor abdominal (30%–50%), prurido generalizado (66%), perda de peso (30%–50%), febre (até 20%) e alterações na cor das fezes ou da urina. Até certo ponto, os sintomas dependem da localização do tumor: pacientes com colangiocarcinoma nos ductos biliares extra-hepáticos (fora do fígado) são mais propensos a ter icterícia, enquanto aqueles com tumores dos ductos biliares dentro do fígado mais frequentemente têm dor sem icterícia. Exames de sangue da função hepática em pacientes com a doença frequentemente revelam o chamado "quadro obstrutivo", com níveis elevados de bilirrubina, fosfatase alcalina e gamaglutamiltranspeptidase, e níveis relativamente normais de transaminases. Tais achados laboratoriais sugerem obstrução dos ductos biliares, em vez de inflamação ou infecção do parênquima hepático, como a principal causa da icterícia.
Dengue é uma doença tropical infecciosa causada pelo vírus da dengue, um arbovírus da família Flaviviridae, gênero Flavivírus e que inclui quatro tipos imunológicos: DEN-1, DEN-2, DEN-3 e DEN-4. Os sintomas incluem febre, dor de cabeça, dores musculares e articulares e uma erupção cutânea característica que é semelhante à causada pelo sarampo. Em uma pequena proporção de casos, a doença pode evoluir para a dengue hemorrágica com risco de morte, resultando em sangramento, baixos níveis de plaquetas sanguíneas, extravasamento de plasma no sangue ou até diminuição da pressão arterial a níveis perigosamente baixos. A dengue é transmitida por várias espécies de mosquito do gênero Aedes, principalmente o Aedes aegypti. O vírus tem quatro tipos diferentes e a infecção por um deles dá proteção permanente para o mesmo sorotipo e imunidade parcial e temporária contra os outros três. Um contágio subsequente por algum tipo diferente do vírus aumenta o risco de complicações graves no paciente. Como não há vacina disponível no mercado, a melhor forma de evitar a epidemia é a prevenção, através da redução ou destruição do habitat e da população de mosquitos transmissores e da limitação da exposição a picadas. A dengue tem como hospedeiros vertebrados o ser humano e outros primatas, mas somente o primeiro apresenta manifestação clínica da infecção e período de viremia de aproximadamente sete dias. Nos demais primatas, a viremia é baixa e de curta duração. Atualmente, a dengue é a arbovirose mais comum que atinge a humanidade, sendo responsável por cerca de 100 milhões de casos/ano em uma população de risco de 2,5 a 3 bilhões de seres humanos. O tratamento da dengue é de apoio, com reidratação oral ou intravenosa para os casos leves ou moderados e fluidos intravenosos e transfusão de sangue para os casos mais graves. O número de casos da doença tem aumentado dramaticamente desde os anos 1960, com cerca de 50 a 390 milhões de pessoas infectadas todos os anos. A dengue é endêmica do sudeste asiático e as primeiras descrições da doença datam de 1779, sendo que sua causa viral e seu modo de transmissão foram descobertos no início do século XX. A dengue tornou-se um problema global desde a Segunda Guerra Mundial e é endêmica em mais de 110 países diferentes, principalmente em regiões tropicais de Oceania, África Oriental, Caribe e América. Além de eliminar os mosquitos, pesquisas para o desenvolvimento de uma vacina e medicação diretamente orientada para esse tipo de vírus são formas de controlar a doenças. [4] O vírus da dengue, provavelmente, se originou de vírus que circulavam em primatas não humanos nas proximidades da península da Malásia. O crescimento populacional aproximou as habitações da região à selva e, assim, mosquitos transmitiram vírus ancestrais de primatas a humanos que, após mutações, originaram os quatro diferentes tipos de vírus da dengue atuais. O primeiro registro de um provável caso de dengue foi publicado numa enciclopédia médica chinesa da época da dinastia Jin (265-420). Os chineses se referiam à doença como "veneno da água" e sabiam que havia alguma associação com insetos voadores. O principal vetor, o mosquito Aedes aegypti, se espalhou para fora da África durante os séculos XV a XIX, em parte devido ao aumento do comércio de escravos. [9] Houve relatos de epidemias no século XVII, mas os primeiros registros mais plausíveis de dengue datam de 1779 e 1780, quando uma epidemia varreu a Ásia, África e América do Norte. Dessa época até 1940, epidemias de dengue se tornaram frequentes. [8] Em 1906, a transmissão por mosquitos do gênero Aedes foi confirmada. No ano seguinte, em 1907, foi demonstrado que a dengue é causada por um vírus, tornando-a a segunda doença na história, depois da febre amarela, de etiologia viral confirmada. Posteriormente, pesquisas de John Burton Cleland e Joseph Franklin Siler completaram a compreensão básica da transmissão da dengue. A acentuada propagação da dengue durante e após a Segunda Guerra Mundial tem sido atribuída a perturbações ecológicas. As mesmas tendências também levaram à disseminação de diferentes sorotipos da doença para novas áreas e ao surgimento da dengue causadora da febre hemorrágica. Esta forma grave da doença foi relatada pela primeira vez em 1953, nas Filipinas. Na década de 1970, a forma grave da doença tornou-se uma das principais causas de mortalidade infantil e apareceu também na região do Pacífico e na América. A dengue hemorrágica e a síndrome do choque da dengue foram observadas pela primeira vez na América do Sul e Central em 1981, a DENV-2 foi contraída por pessoas que haviam sido previamente infectadas com o DENV-1 vários anos antes.
Doença celíaca é uma doença autoimune crónica do intestino delgado causada por uma reação ao glúten em pessoas com predisposição genética. [10] Os sintomas clássicos incluem problemas gastrointestinais como diarreia crónica, distensão abdominal, má-absorção intestinal e perda de apetite. Em crianças, pode ocorrer atraso no crescimento que geralmente se inicia entre os seis meses e dois anos de idade. Os sintomas não clássicos são mais comuns, especialmente em pessoas com mais de dois anos. A pessoa pode não manifestar sintomas gastrointestinais ou manifestar apenas sintomas gastrointestinais ligeiros, podem ocorrer sintomas em qualquer parte do corpo, ou não haver sintomas visíveis de todo. Embora a doença celíaca seja geralmente diagnosticada durante a infância, pode aparecer em qualquer idade. A doença está associada a outras doenças autoimunes, como diabetes mellitus tipo 1 e tiroidite, entre outras. A doença celíaca é causada por uma reação do organismo ao glúten, um conjunto de proteínas presentes no trigo e em outros cereais como a cevada ou centeio. A aveia em quantidades moderadas é geralmente tolerada, dependendo da variedade e desde que não esteja contaminada com outros cereais com glúten. Ao ser exposto ao glúten, o sistema imunitário desencadeia uma resposta anormal, produzindo uma série de anticorpos que podem afetar diversos órgãos. No intestino delgado, esta resposta imunitária provoca uma reação inflamatória que faz diminuir as vilosidades intestinais. Isto afeta a absorção de nutrientes pelo intestino, o que em muitos casos causa anemia. O diagnóstico é geralmente feito com análises aos anticorpos do sangue e uma biópsia do intestino, podendo ser apoiado por exames genéticos. No entanto, o diagnóstico nem sempre é conclusivo. [23] Em muitos celíacos, o exame aos anticorpos é negativo e muitos indivíduos apresentam apenas ligeiras alterações intestinais com vilosidades normais. Em alguns casos, o diagnóstico pode levar anos para ser confirmado, mesmo que as pessoas sejam examinadas e manifestem sintomas pronunciados. Com o avanço das técnicas de rastreio, é cada vez mais comum o diagnóstico de doença celíaca entre pessoas assintomáticas. No entanto, não há ainda evidências conclusivas que apoiem a eficácia do rastreio sistemático. Embora a doença seja causada por uma intolerância permanente às proteínas do trigo, não se trata de uma forma de alergia ao trigo. O único tratamento eficaz conhecido é seguir uma dieta sem glúten durante toda a vida. A ausência de glúten permite a reparação das mucosas do intestino, alivia os sintomas e diminui o risco de complicações na maioria das pessoas. Quando a doença não é tratada, pode levar ao aparecimento de cancros como o linfoma intestinal e a um ligeiro aumento do risco de morte prematura. A frequência da doença varia entre as diferentes regiões do mundo, desde 1 em cada 300 pessoas até 1 em cada 40 pessoas. A média é de entre 1 em cada 100-170 pessoas. Nos países desenvolvidos, estima-se que 80% dos casos não cheguem a ser diagnosticados, geralmente devido à ausência de sintomas gastrointestinais e ao pouco conhecimento da doença entre a população. A doença celíaca é ligeiramente mais comum entre mulheres do que em homens. O termo "celíaco" tem origem no grego κοιλιακός (koiliakós, "abdominal"), tendo sido introduzido na linguagem contemporânea durante o século XIX numa tradução do que é geralmente considerada a primeira descrição da doença por Areteu da Capadócia.
Doença de Alzheimer é uma doença neurodegenerativa crónica e a forma mais comum de demência. A doença manifesta-se lentamente e vai-se agravando ao longo do tempo. O sintoma inicial mais comum é a perda de memória a curto prazo, com dificuldades em recordar eventos recentes. Os primeiros sintomas são geralmente confundidos com o processo normal de envelhecimento ou manifestações de stresse. À medida que a doença evolui, o quadro de sintomas inclui dificuldades na linguagem, desorientação, perder-se com facilidade, alterações de humor, perda de motivação, desinteresse por cuidar de si próprio, desinteresse por tarefas quotidianas e comportamento agressivo. Em grande parte dos casos, a pessoa com Alzheimer afasta-se progressivamente da família e da sociedade. Gradualmente, o corpo vai perdendo o controle das funções corporais, o que acaba por levar à morte. Embora a velocidade de progressão possa variar, geralmente a esperança de vida após o diagnóstico é de três a nove anos. A doença de Alzheimer é a causa de 60–70% dos casos de demência. As causas de Alzheimer ainda não são totalmente compreendidas. Pensa-se que 70% do risco seja de origem genética com vários genes implicados. Entre outros fatores de risco estão antecedentes de lesões na cabeça, depressão e hipertensão arterial. O mecanismo da doença está associado às placas senis e aos novelos neurofibrilares no cérebro. Quando se suspeita de Alzheimer com base no historial clínico, o diagnóstico é geralmente confirmado com exames que avaliam o comportamento e a capacidade de raciocínio da pessoa, podendo ser realizados exames imagiológicos e análises ao sangue para descartar outras causas. No entanto, só é possível determinar um diagnóstico definitivo através de um exame ao tecido cerebral. O risco de Alzheimer pode ser diminuído com exercícios mentais, exercício físico e controlo da obesidade. No entanto, estas recomendações não são apoiadas por evidências fortes. Não existem medicamentos ou suplementos que tenham demonstrado diminuir o risco. Não existem atualmente tratamentos para parar ou reverter a progressão de Alzheimer, embora alguns possam melhorar temporariamente os sintomas. À medida que a doença avança, a pessoa torna-se progressivamente dependente da assistência de um cuidador. Em muitos casos, é o cônjuge ou um familiar próximo quem assume o papel de principal cuidador. Os programas de exercício podem ter alguns benefícios para a realização de tarefas quotidianas, melhorando o prognóstico. Os distúrbios comportamentais e a psicose associados à demência são muitas vezes tratados com antipsicóticos, embora não sejam recomendados, uma vez que existem poucos benefícios e um aumento do risco de morte prematura. Em 2015 havia aproximadamente 29,8 milhões de pessoas em todo o mundo com Alzheimer. A doença geralmente tem início em pessoas com idade superior a 65 anos, embora 4 a 5% dos casos sejam de início precoce. A doença afeta cerca de 6% das pessoas com 65 ou mais anos de idade. Em 2015, a demência foi a causa de 1,9 milhão de mortes. A doença de Alzheimer foi descrita pela primeira vez em 1906 pelo psiquiatra e patologista alemão Alois Alzheimer. Em países desenvolvidos, Alzheimer é uma das doenças com maiores custos sociais e económicos. A doença tem um custo significativo para os cuidadores, a nível social, psicológico, físico e económico.
Doença de Chagas ou Tripanossomíase americana é uma doença tropical parasitária causada pelo protozoário Trypanosoma cruzi e transmitida principalmente por insetos da subfamília Triatominae. Os sintomas mudam ao longo do curso da infecção. Na fase inicial, eles podem não estar presentes ou podem ser: febre, gânglios linfáticos aumentados, dor de cabeça e inchaço no local da mordida. Após 8-12 semanas, os indivíduos entram na fase crônica da doença e em 60-70% nunca desenvolvem outros sintomas. Os 30 a 40% restantes apresentam sintomas adicionais de 10 a 30 anos após a infecção inicial. Isto inclui o alargamento dos ventrículos do coração em 20 a 30% levando a insuficiência cardíaca. A dilatação do esôfago ou o alargamento do cólon também podem ocorrer em 10% das pessoas. T. cruzi é transmitido para humanos e outros mamíferos principalmente pela via vetorial, geralmente através do contagio com as fezes de insetos hematófagos da subfamília Triatominae, popularmente denominados de "barbeiros" (p. ex.: Triatoma infestans). A doença pode também ser transmitida através de transfusão de sangue, transplante de órgãos, ingestão de alimentos contaminados com o parasita e da mãe para o feto. O diagnóstico precoce da doença é feito pela detecção do parasita no sangue, utilizando um microscópio. A forma crônica é diagnosticada pela presença de anticorpos para T. cruzi no sangue. A prevenção ocorre principalmente pela eliminação dos barbeiros e em evitar suas picadas. Outros esforços de prevenção incluem a triagem do sangue usado para transfusões. Infecções precoces são tratáveis ​​com a medicação benznidazol ou nifurtimox. Eles quase sempre resultam em cura se forem dados no início, no entanto, tornam-se menos eficazes quanto mais tempo se passa após contrair a doença. Quando utilizados na forma crônica podem retardar ou prevenir o desenvolvimento de sintomas em fase terminal. Benznidazol e nifurtimox causam efeitos colaterais temporários em até 40% das pessoas, incluindo doenças de pele, toxicidade cerebral e irritação do sistema digestório. Estima-se que 7 000 000-8 000 000 pessoas, sobretudo no México, América Central e América do Sul, têm a doença de Chagas. Isso resulta em cerca de 12 500 mortes por ano desde 2006. A maioria das pessoas com a doença são indivíduos de baixa renda e grande parte das pessoas com a doença não percebem que estão infectadas. Movimentos populacionais em grande escala têm expandido as áreas onde os casos da doença de Chagas são encontrados e estes passaram a incluir muitos países da Europa e nos Estados Unidos. Essas áreas também têm visto um aumento nos casos até 2014. A doença foi descrita pela primeira vez em 1909 por Carlos Chagas, do qual recebeu o nome. Ela afeta mais de 150 outras espécie de animais. Atualmente, o médico e pesquisador hispano-brasileiro Pedro Albajar Viñas é o responsável pelo programa da OMS para o combate da doença de Chagas. Nas áreas endêmicas, o principal mecanismo de transmissão é o vetorial, ou seja, através de um inseto vetor da subfamília Triatominae, principalmente dos gêneros Triatoma, Rhodnius e Panstrongylus. O inseto infecta-se com Trypanosoma cruzi ao alimentar-se de sangue de animais ou humanos contaminados. De hábitos noturnos, ele esconde-se durante o dia em fendas nas paredes e telhados, e emerge à noite, quando os habitantes estão dormindo. Por causa da tendência de picar a face das pessoas, o inseto é popularmente conhecido como "barbeiro" ou "chupão". Após a picada e a ingestão de sangue, ele defeca próximo ao local. O prurido intenso no local da picada ajuda as formas infectantes de T. cruzi (denominadas de tripomastigotas), nas fezes, a penetrarem na ferida da picadura, mas elas também podem penetrar por mucosas intactas, como a conjuntival. Uma vez dentro do hospedeiro, o tripomastigota invade as células próximas à inoculação, onde diferenciam-se em amastigotas intracelulares. Os amastigotas multiplicam-se por divisão binária e diferenciam-se em tripomastigotas, que são liberados na corrente sanguínea, infectando células de tecidos variados, e este ciclo então é repetido diversas vezes. As manifestações clínicas são resultado deste ciclo infeccioso. Os tripomastigotas circulantes não se replicam (diferente da tripanossomíase africana). E a replicação só recomeça quando os parasitas entram em outras células ou são ingeridos por outro vetor. [3] Como doença caracteristicamente rural, tradicionalmente acomete pessoas de origem interiorana que habitam ou habitaram casas de baixa qualidade, onde o vetor facilmente se aloja e coloniza. Vegetação densa (como a das florestas tropicais) e habitats urbanos não são ideais para o estabelecimento do ciclo de transmissão humano. No entanto, em regiões onde o habitat silvestre e sua fauna são reduzidos pela exploração econômica e ocupação humana, como em áreas recém-desmatadas, áreas de cultura da Leopoldinia piassaba e algumas partes da região amazônica, um ciclo de transmissão humano pode desenvolver-se quando os insetos procuram por novas fontes de alimentação. [6] T. cruzi pode também ser transmitido por transfusões de sangue. Com a exceção dos derivados sanguíneos (como anticorpos fracionados), todos os componentes do sangue são infecciosos. O parasita permanece viável a 4 °C por até no mínimo 18 dias ou mais de 250 dias quando mantido em temperatura ambiente. Não está claro se o T. cruzi pode ser transmitido por meio de componentes do sangue congelados e descongelados. [7]
A doença por vírus Ébola (DVE), também denominada no Brasil por doença por vírus ebola, é uma doença infeciosa causada pelo vírus Ébola que afeta seres humanos e outros mamíferos. Os sintomas têm início de duas a três semanas após contrair o vírus, manifestando-se inicialmente por febre, garganta inflamada, dores musculares e dores de cabeça. Estes sintomas são seguidos por vómitos, diarreia e exantema, a par de insuficiência hepática e renal. Nesta fase, a pessoa infetada pode apresentar hemorragias internas e externas. Em caso de morte, esta geralmente ocorre entre 6 a 16 dias após o início dos sintomas e na maior parte dos casos deve-se à diminuição da pressão arterial resultante da perda de sangue. [2] O vírus pode ser adquirido através de contacto com o sangue ou outros fluidos biológicos de um ser humano ou animal infetado. A transmissão por via aérea ainda não foi documentada em ambiente natural. Acredita-se que o reservatório natural seja o morcego-da-fruta, o qual é capaz de propagar o vírus sem ser afetado. Os humanos são infetados pelo contacto direto com os morcegos ou com animais que foram infetados pelos morcegos. Uma vez estabelecida a infeção humana, a doença pode-se também disseminar entre determinada população. Os sobreviventes do sexo masculino continuam a ser capazes de transmitir a doença através do sémen durante cerca de dois meses. Para o diagnóstico de DVE, são primeiro excluídas doenças com sintomas semelhantes, como a malária, cólera ou outras febres hemorrágicas virais. Para confirmar o diagnóstico são examinadas amostras de sangue para a presença de anticorpos virais, ARN viral ou do próprio vírus. O controle de um surto exige a coordenação entre vários serviços médicos, a par de um determinado nível de envolvimento da comunidade. Entre os serviços médicos necessários estão a rápida deteção e diagnóstico de pessoas de risco, rápido acesso a serviços de laboratório adequados, tratamento adequado dos infetados e gestão adequada dos mortos através de cremação ou enterro. A prevenção passa por diminuir o risco de propagação da doença entre animais infetados e seres humanos. Isto pode ser feito através do uso de vestuário de proteção ao manusear carcaças de animais suspeitas ou garantindo que toda a carne é plenamente cozinhada antes de ser consumida. Durante o contacto com pessoas com a doença, deve também ser usado vestuário de proteção adequado e as mãos devem ser frequentemente lavadas. As amostras de tecidos e fluidos corporais de pessoas infetadas devem ser manuseadas com especial precaução. Não está ainda disponível qualquer tratamento específico para a doença. Os cuidados de apoio envolvem a terapia de reidratação oral (administração de água ligeiramente doce e salgada) ou terapia intravenosa, sendo capazes de melhorar o prognóstico da doença. A doença apresenta elevado risco de morte, matando entre 25% e 90% das pessoas infetadas, com média de 50%. A DVE foi identificada pela primeira vez numa região do Sudão e no Zaire, atual República Democrática do Congo. A doença geralmente ocorre em surtos em regiões tropicais da África subsariana. Entre 1976, ano em que foi pela primeira vez identificada, e 2013, a Organização Mundial de Saúde reportou um total de 1716 casos. O maior surto verificado até hoje foi o surto de ébola na África Ocidental de 2014, que atualmente afeta a Guiné-Conacri, a Serra Leoa e a Libéria. Estão em curso medidas para desenvolver uma vacina, embora ainda não exista nenhuma. Em 2017, cientistas da Universidade de Queensland, na Austrália, desenvolveram um tratamento eficaz, rápido e económico para o vírus Ébola usando anticorpos de cavalos. Os sinais e sintomas do ébola geralmente têm início de forma súbita ao longo de um estágio inicial semelhante à gripe e caracterizado por fadiga, febre, dor de cabeça e dores nas articulações, musculares e abdominais. Vómitos, diarreia e anorexia são também sintomas comuns. Entre os sintomas menos comuns estão a inflamação da garganta, dores no peito, soluços, falta de ar e dificuldade em engolir. Em cerca de metade dos casos os pacientes apresentam exantema maculopapular. O tempo médio entre o momento em que se contrai a infeção e a primeira manifestação de sintomas é de entre 8 a 10 dias, mas pode ocorrer entre 2 e 21 dias. Os primeiros sintomas de DVE podem ser semelhantes aos de malária, dengue ou outras doenças tropicais, antes de a doença progredir para a fase hemorrágica. Todas as pessoas infetadas mostram sintomas do envolvimento do sistema circulatório, como coagulopatia. [16] Durante a fase hemorrágica, as primeiras hemorragias internas ou subcutâneas podem se manifestar através de olhos avermelhados ou pela presença de sangue no vómito. Em cerca de 40-50% dos casos verificam-se relatos de hemorragias nas pregas da pele e das mucosas; por exemplo, no sistema digestivo, nariz, vagina e gengivas. [17] Entre os tipos de hemorragias associados à doença estão a presença de sangue no vómito, na tosse e nas fezes. As hemorragias intensas são raras e geralmente restritas ao sistema digestivo. Geralmente, a evolução para sintomas hemorrágicos é um indicador do agravamento do prognóstico e a perda de sangue pode provocar a morte.
A doença pulmonar obstrutiva crónica (português europeu) ou crônica (português brasileiro) (DPOC) é um tipo de doença pulmonar obstrutiva caracterizada por diminuição prolongada do calibre das vias aéreas respiratórias e destruição do tecido pulmonar. Entre os principais sintomas estão falta de ar e tosse com produção de expectoração. A DPOC é uma doença progressiva, o que significa que geralmente se agrava com o decorrer do tempo. A partir de determinado momento começam-se a verificar dificuldades em realizar atividades do dia-a-dia, como subir escadas. Bronquite crónica e enfisema são termos antigos usados para denominar diferentes tipos de DPOC. O termo "bronquite crónica" é ainda hoje usado para definir uma tosse produtiva que se manifeste durante pelo menos três meses por ano e ao longo de dois anos. A inalação de fumo de tabaco é a causa mais comum de DPOC. Outros fatores de risco, como a poluição do ar e a genética, são menos significativos. Em países desenvolvidos, uma das fontes mais comuns de poluição do ar é a má ventilação de fontes de aquecimento e de queima de combustíveis. A exposição prolongada a estes irritantes causa uma resposta inflamatória nos pulmões, o que provoca um estreitamento das vias aéreas e a destruição do tecido pulmonar. O diagnóstico tem por base a medição do fluxo respiratório através de testes que avaliam a função respiratória. Ao contrário da asma, o estreitamento das vias aéreas na DPOC não melhora significativamente após a administração de um broncodilatador. A maior parte dos casos de DPOC pode ser evitada diminuindo a exposição aos fatores de risco. Entre as principais medidas estão a diminuição da exposição ao fumo do tabaco e a melhoria da qualidade do ar. Embora o tratamento possa atrasar o agravamento, a doença não tem cura. O tratamento inclui deixar de fumar, a vacinação, a reabilitação respiratória e, em muitos casos, a inalação de broncodilatadores e corticosteroides. Em algumas pessoas pode ser benéfica a oxigenoterapia de longo prazo ou um transplante de pulmão. Em pessoas que manifestam períodos de agravamento agudo, podem ser necessárias a hospitalização e uma maior quantidade de medicamentos. Em 2013 a DPOC afetava cerca de 329 milhões de pessoas, o que corresponde a cerca de 5% da população mundial. Geralmente ocorre em pessoas com mais de 40 anos de idade e afeta em proporção idêntica homens e mulheres. Em 2013, a doença foi responsável pela morte de 2,9 milhões de pessoas, um aumento em relação às 2,4 milhões em 1990. Mais de 90% destas mortes ocorrem em países em vias de desenvolvimento. Estima-se que o número de mortes venha a aumentar devido ao aumento de número de fumadores nos países em desenvolvimento e ao aumento da população idosa em muitos países. Em 2010, a doença implicou um custo económico estimado de 2,1 bilhões de dólares. Os sintomas mais comuns da doença pulmonar obstrutiva crónica são produção de expectoração, falta de ar e tosse produtiva.  Estes sintomas manifestam-se durante um longo período de tempo e geralmente vão-se agravando ao longo do tempo. Não é claro se existem ou não diferentes tipos de DPOC. Embora antigamente a doença se dividisse em enfisema e bronquite crónica, hoje considera-se que enfisema é apenas uma descrição das alterações no pulmão, e não uma doença em si, e que a bronquite crónica descreve apenas os sintomas que podem ou não ocorrer no contexto de DPOC. Em muitos casos, o primeiro sintoma a manifestar-se é a tosse crónica. Por definição, está-se perante bronquite crónica quando essa tosse persiste por mais de três meses por ano e durante pelo menos dois anos, quando é acompanhada pela produção de expectoração e quando não é possível determinar outra explicação. Esta condição pode anteceder o desenvolvimento completo de DPOC. A quantidade de expectoração produzida pode variar no prazo de horas ou dias. No entanto, em alguns casos da doença é possível que não se verifique tosse ou que se verifique apenas tosse ocasional e sem ser produtiva. Algumas pessoas com DPOC atribuem os sintomas ao "catarro do fumador". Dependendo do contexto social e cultural, a expectoração pode ser engolida ou cuspida. A tosse forte pode causar fracturas nas costelas e perda de consciência. As pessoas com DPOC muitas vezes apresentam um histórico daquilo que aparentam ser constipações de duração prolongada. A falta de ar é geralmente o sintoma que mais incomoda as pessoas com DPOC. O sintoma é geralmente descrito em termos leigos como "custar a respirar", "sentir-se sem fôlego" ou "não conseguir inspirar ar suficiente". [20] Dependendo do contexto cultural, podem ser usados diversos termos. Geralmente a falta de ar é mais intensa ao realizar esforços de longa duração e vai-se agravando ao longo do tempo. Em estágios avançados da doença, a falta de ar manifesta-se até mesmo em repouso e pode-se manifestar continuamente. Em pessoas com a doença, a falta de ar é uma fonte de ansiedade e diminuição da qualidade de vida. Muitas pessoas com DPOC avançada recorrem a respiração freno-labial para aliviar a falta de ar. Esta técnica respiratória consiste em inspirar o ar pelo nariz com a boca fechada e expirar pela boca com os lábios cerrados o máximo possível.
Esclerose múltipla (EM) é uma doença desmielinizante de etiologia ainda desconhecida, caracterizada por uma reacção inflamatória na qual são danificadas as bainhas de mielina que envolvem os axónios dos neurónios cerebrais e medulares, levando à sua desmielinização e ao aparecimento de um vasto quadro de sinais e sintomas. A doença manifesta-se geralmente em jovens adultos e é mais frequente em mulheres numa razão de cerca de 3:1. A sua prevalência varia consoante a situação geográfica estudada, entre 2 e mais de 150 casos em cada 100 000 indivíduos, nos países tropicais e nos países nórdicos respetivamente. A EM foi descrita pela primeira vez em 1868 por Jean-Martin Charcot. Devido a esta desmielinização, a EM afecta a capacidade das células nervosas do cérebro e da medula espinhal comunicarem entre si de forma eficaz. As células nervosas comunicam entre si através da transmissão de impulsos eléctricos, designados por potenciais de acção, ao longo dos seus filamentos extensos designados por axónios, os quais estão envolvidos por uma substância isolante chamada mielina. Na EM, o próprio sistema imunitário do corpo ataca e destrói a mielina. Uma vez destruída, os axónios deixam de poder transmitir o potencial de acção de um neurónio ao neurónio seguinte ficando assim a condução do estímulo nervoso interrompida. O termo "esclerose múltipla" é uma referência às lesões, ou escleroses, que ocorrem sobretudo na substância branca do cérebro, cerebelo e medula espinal, que é constituída principalmente por fibras nervosas revestidas de mielina. Embora sejam plenamente conhecidos os mecanismos envolvidos no desenvolvimento da doença, a causa é ainda desconhecida. As teorias plausíveis inclinam-se para uma causa genética, infecciosa ou muito provavelmente imunológica. Também foram identificados alguns factores de risco ambientais. A doença pode manifestar-se através de praticamente qualquer sintoma neurológico, dependente da localização da placa de desmielinização, e frequentemente evolui com a perda de capacidades físicas e cognitivas. A EM pode assumir várias formas, e cada novo sintoma pode ocorrer em ataques discretos e isolados (forma recrudescente) ou os sintomas podem-se ir acumulando ao longo do tempo (forma progressiva). Entre cada ataque, a sintomatologia pode desaparecer por completo, embora normalmente se verifiquem sequelas neurológicas permanentes, sobretudo à medida que a doença progride. Não se conhece uma cura eficaz contra a esclerose múltipla. O tratamento baseia-se na tentativa de melhoria das funções fisiológicas comprometidas depois de um ataque, na prevenção de novos episódios e na prevenção da degenerescência. A medicação para a EM pode apresentar vários efeitos colaterais e mesmo ser mal tolerada pelo organismo, o que leva a que muitas pessoas procurem tratamentos alternativos, apesar da falta de evidências científicas que os confirmem. É difícil obter um prognóstico preciso; depende do sub-tipo da doença, da característica individual da doença, dos sintomas iniciais e do grau de degenerescência que o indivíduo apresenta à medida que a doença progride. [15] A esperança de vida em doentes de EM é de cerca de cinco a dez anos inferior à da restante população. O indivíduo com EM pode experimentar praticamente qualquer sinal ou sintoma neurológico, incluindo alterações sensoriais como a perda de sensibilidade táctil ou formigueiro, parestesia, fadiga muscular, clónus, espasmos musculares ou dificuldades locomotoras; dificuldades na coordenação e de equilíbrio (ataxia); dificuldades na fala (disartria) ou na deglutição (disfagia); problemas visuais tais como fosfeno, diplopia, nistagmo, na sequência de uma neurite óptica. Fadiga, dor aguda ou crónica e dificuldades miccionais e do peristaltismo intestinal com obstipação secundária. São também comuns vários graus de degradação da capacidade cognitiva, bem como sintomas de depressão nervosa e humor instável, alternando entre episódios de choro e de alegria eufórica. São também característicos da EM, embora não exclusivos, fenómeno de Uhthoff, um agravamento dos sintomas em função da exposição a temperaturas superiores ao normal, e o sinal de Lhermitte, uma sensação de corrente eléctrica que irradia pela coluna vertebral ao dobrar o pescoço. Os sintomas da EM pioram normalmente durante episódios agudos de agravamento, designados por recaídas ou agudizações, ou devido a uma degenerescência progressiva e contínua das funções neurológicas, ou ainda uma conjugação de ambos. As recaídas da EM são quase sempre imprevisíveis, ocorrem sem aviso prévio e sem causa aparente, a um ritmo raramente superior a um episódio e meio por ano. Alguns ataques, no entanto, são antecedidos por estímulos comuns. As recaídas ocorrem com maior frequência durante a primavera e o verão. As infecções virais, como a rinofaringite, gripe ou gastroenterite, aumentam o risco de novos episódios. O stresse pode também desencadear um ataque. A gravidez afecta a susceptibilidade às agudizações, baixando o ritmo de ocorrência a cada trimestre de gestação. Durante os primeiros meses após o parto, no entanto, é maior o risco de ocorrer um ataque. De uma maneira geral, a gravidez não parece ter influência na degenerescência a longo prazo. Muitos dos potenciais estímulos foram já examinados e demonstrou-se que não têm influência no ritmo de ocorrência dos episódios. Não há qualquer evidência que a vacinação, a amamentação, o trauma físico, ou que o fenómeno de Ohthoff sejam estímulos de ataques.
Gripe é uma doença infecciosa causada por diversos vírus ARN da família Orthomyxoviridae e que afeta aves e mamíferos. Os sintomas mais comuns são calafrios, febre, rinorreia, dores de garganta, dores musculares, dores de cabeça, tosse, fadiga e sensação geral de desconforto. Em crianças pode ainda provocar diarreia e dores abdominais. Embora seja frequentemente confundida com a constipação, a gripe é uma doença mais grave provocada por um tipo de vírus diferente. A gripe é geralmente transmitida por via aérea através de tosse ou de espirros, os quais propagam partículas que contêm o vírus. A gripe pode também ser transmitida por contacto direto com excrementos ou secreções nasais de aves infetadas, ou através de contacto com superfícies contaminadas. Os vírus da gripe podem ser neutralizados pela luz solar, desinfetantes e detergentes. Uma vez que o vírus pode ser neutralizado com sabonete, lavar frequentemente as mãos reduz o risco de infeção. A gripe pode ocasionalmente levar ao aparecimento de pneumonia, tanto viral como bacteriana, mesmo em pessoas bastante saudáveis. Os países desenvolvidos têm geralmente à disposição vacinas contra a gripe. As aves de criação são frequentemente vacinadas para evitar que sejam dizimadas por um eventual surto. A vacina humana mais comum é a vacina trivalente, que contém antígenos purificados e neutralizados de três estirpes virais. Esta vacina geralmente inclui material de dois subtipos de Influenzavirus A e uma estirpe de Influenzavirus B. A vacina trivalente não apresenta qualquer risco de transmissão da doença. No entanto, uma vacina produzida para um determinado ano pode não ser eficaz no ano seguinte, uma vez que o vírus da gripe evolui rapidamente, substituindo as estirpes antigas por novas. No tratamento da gripe são também usados alguns antivirais, como o oseltamivir. A gripe propaga-se globalmente em ciclos sazonais de epidemias, as quais provocam anualmente entre três e cinco milhões de casos graves da doença e entre 250 000 e 500 000 mortes, número que pode ascender a milhões em anos de pandemia. Ao longo do século XX ocorreram três pandemias de gripe, cada uma delas provocada pelo aparecimento de uma nova estirpe do vírus em seres humanos, e responsáveis pela morte de dezenas de milhões de pessoas. Em muitos casos, as novas cepas de gripe aparecem quando um vírus já existente se propaga para o ser humano a partir de outra espécie animal, ou quando uma estirpe humana recolhe novos genes de um vírus que só infeta aves ou suínos. Uma estirpe aviária denominada H5N1 levantou algumas preocupações em relação a uma nova pandemia de gripe em finais da década de 1990, mas não chegou a evoluir para uma forma de fácil contágio entre os seres humanos. Em abril de 2009 ocorreu uma pandemia de uma nova estirpe que combinava genes da gripe humana, aviária e suina, denominada H1N1 ou gripe suína. Os sintomas de gripe podem ter início de forma súbita um ou dois dias após a infeção. Geralmente, os primeiros sintomas são calafrios ou uma sensação de frio, embora a febre seja também comum nesta fase, com temperaturas entre os 38 e os 39 °C. Muitas pessoas sentem-se de tal forma doentes que se sentem compelidas a ficar de cama por vários dias, com dores ao longo de todo o corpo que se agravam nas costas e pernas. Os sintomas da gripe podem incluir: Febre e sensação extrema de frio (calafrios e tremores), tosse, congestão nasal, rinorreia, dores musculares, principalmente nas articulações e na garganta, fadiga, dores de cabeça, olhos irritados e lacrimejantes, vermelhidão nos olhos, boca, garganta e nariz, petéquias, em crianças, sintomas gastrointestinais como diarreia e dores abdominais, que podem ser graves nos casos de gripe B, ou ainda provocar náuseas e vómitos. [16] Nos primeiros estágios de infeção, pode ser difícil distinguir uma gripe de uma constipação. A gripe pode ser identificada pelo aparecimento súbito de febre elevada e fadiga acentuada. A gripe é uma mistura de sintomas de constipação e de pneumonia, dores musculares, dores de cabeça e fadiga. Em adultos, a diarreia normalmente não é um sintoma de gripe, embora tenha sido observada em alguns casos de gripe aviária e possa ser um sintoma em crianças. Os sintomas de maior fiabilidade para determinar um diagnóstico de gripe encontram-se na tabela à direita. Uma vez que os antivirais só são eficazes no tratamento da gripe quando administrados na fase inicial, pode ser importante identificar os casos o mais cedo possível. Dos sintomas enumerados acima, as combinações de febre com tosse, dor de garganta e congestão nasal podem melhorar a precisão do diagnóstico. Dois estudos sugerem que durante surtos locais de gripe, a prevalência será superior a 70%, pelo que os pacientes com qualquer destas combinações de sintomas podem ser tratados com inibidores de neuraminidase, mesmo sem a realização de exames. Mesmo fora do contexto de um surto local, pode ser pertinente o tratamento de idosos durante a época de gripe quando a prevalência seja superior a 15%. Estão disponíveis testes de diagnóstico rápido para a gripe, com uma sensibilidade de 70-75% e especificidade de 90-95%, quando comparados com culturas virais. Estes testes podem ser particularmente úteis durante a época de gripe (prevalência=25%). A gripe pode ocasionalmente causar pneumonia viral ou bacteriana, mesmo em pessoas saudáveis. O principal sintoma deste tipo de complicações é a dificuldade respiratória. Outro sinal de alarme para a pneumonia bacteriana verifica-se quando uma criança, ou até um adulto, aparenta estar a melhorar e subitamente tem uma recidiva com febre elevada.
Hipertensão arterial é uma doença crónica em que a pressão sanguínea nas artérias se encontra constantemente elevada. A doença geralmente não causa sintomas. No entanto, a longo prazo é um dos principais fatores de risco para uma série de doenças graves como a doença arterial coronária, acidente vascular cerebral, insuficiência cardíaca, doença arterial periférica, incapacidade visual, doença renal crónica e demência. A hipertensão arterial pode ser classificada como primária ou secundária. Cerca de 90–95% dos casos são primários, tendo origem em fatores não específicos genéticos e de estilo de vida. Entre os fatores relacionados com o estilo de vida que aumentam o risco de hipertensão estão o excesso de sal na dieta, excesso de peso, tabagismo e consumo de álcool. Os restantes 5–10% dos casos são secundários, uma vez que têm origem em causas identificáveis, como doença renal crónica, estenose da artéria renal, doenças endócrinas ou uso de pílula contraceptiva. A pressão arterial é expressa em duas medidas: a pressão sistólica e pressão diastólica. A pressão sistólica é a pressão máxima, enquanto a diastólica é a pressão mínima.  Na maior parte dos adultos, a pressão arterial normal em repouso sistólica é de 120 a 140 milímetros de mercúrio (mmHg) e a diastólica de 75 a 85 mmHg.  Para a maior parte dos adultos, considera-se que a pessoa tem hipertensão arterial quando a pressão arterial em repouso é consistentemente superior a 140/90 mmHg. Em crianças e idosos, os valores de referência são diferentes. A monitorização em ambulatório ao longo de 24 horas oferece uma medição mais rigorosa do que os medidores portáteis.  As alterações no estilo de vida e a medicação permitem diminuir a pressão arterial e o risco de complicações. Entre as alterações no estilo de vida estão perder peso, diminuir o consumo de sal, praticar exercício físico e manter uma dieta saudável. Quando as alterações no estilo de vida não são suficientes podem ser administrados medicamentos anti-hipertensivos. Existem três clamedicamentos que permitem controlar a pressão arterial em 90% das pessoas. O tratamento de pressão arterial de grau II (≥160/100 mmHg) com medicação está associado a um aumento da esperança de vida. O tratamento da pressão arterial entre 145/90 e 160/100 mmHg é menos claro, dado que algumas revisões da literatura observam benefícios enquanto outras não observam benefícios claros. A hipertensão arterial afeta entre 16 e 37% de toda a população mundial. Estima-se que em 2010 a hipertensão tenha sido um fator em 18% de todas as mortes (9,4 milhões em todo o mundo). A hipertensão foi definida como a pressão sanguínea de valor igual ou superior a 140/90 mmHg para um adulto jovem. Esta definição surgiu após 12 anos de experiência em 350.000 indivíduos de idades compreendidas entre os 18 e os 74 anos corroborados posteriormente pelo estudo JNC7. Levantou-se uma polémica acerca deste valor em virtude de a maioria dos médicos, cardiologistas ou não, considerar normal o valor de 140 mmHg. Após um longo consenso, a OMS (Organização Mundial de Saúde) juntamente com a Sociedade International de Hipertensão (ISH), tendo em conta a relação benefício/riscos do tratamento, fixou os limites em 140/90 mmHg sendo considerados normotensos [nota 1] todos os indivíduos adultos com uma pressão arterial de 140/90 mmHg. No adulto com mais de 74 anos, (faixa etária não englobada no estudo JNC7) pode-se aceitar um limite de 150/90 mmHg, tendo em conta a rigidez fisiológica da parede arterial. [22] A pseudo-hipertensão entre os idosos é também um factor a considerar. Esta situação deve-se à calcificação das artérias, o que resulta em níveis de leitura anormalmente elevados no esfigmomanómetro enquanto que as medições intra-arteriais são normais. O processo de endurecimento das paredes arteriais com o envelhecimento é progressivo e o aumento de pressão arterial sistólica com a idade também será progressivo sem que isto signifique hipertensão arterial.
Malária é uma doença infecciosa transmitida por mosquitos e causada por protozoários parasitários do género Plasmodium. Os sintomas mais comuns são febre, fadiga, vómitos e dores de cabeça. Em casos graves pode causar icterícia, convulsões, coma ou morte. Os sintomas começam-se a manifestar entre 10 e 15 dias após a picada. Quando não é tratada, a doença pode recorrer meses mais tarde. Uma nova infeção geralmente causa sintomas mais ligeiros. No entanto, esta imunidade parcial pode desaparecer no prazo de meses a anos se a pessoa não for continuamente exposta à doença. A doença é geralmente transmitida pela picada de uma fêmea infectada do mosquito Anopheles. A picada introduz no sistema circulatório do hospedeiro os parasitas presentes na sua saliva. Os parasitas depositam-se no fígado, onde se desenvolvem e reproduzem. Existem cinco espécies de Plasmodium que podem infetar os seres humanos. A maior parte das mortes são causadas pelo P. falciparum. As espécies P. vivax, P. ovale e P. malariae geralmente causam formas menos graves de malária que raramente são fatais. A espécie P. knowlesi raramente causa a doença em seres humanos. O diagnóstico de malária tem por base análises microscópicas ao sangue que confirmem a presença do parasita ou através testes de diagnóstico rápido que detectam a presença de antigénios no sangue. Existem também técnicas de diagnóstico que usam a reação em cadeia da polimerase para detectar o ADN do parasita, embora o seu uso nas regiões onde a doença é endémica seja pouco comum devido ao seu elevado custo e complexidade. A transmissão da doença pode ser combatida através da prevenção das picadas de mosquito. As medidas de prevenção mais comuns são o uso de redes mosquiteiras ou repelente de insetos e medidas de erradicação, como o uso de inseticidas ou o escoamento de águas estagnadas. Estão disponíveis diversos medicamentos para prevenção da malária em viajantes que se desloquem a países onde a doença seja endémica. Em regiões de elevada incidência, está recomendada a administração de sulfadoxina/pirimetamina nas crianças mais novas e em grávidas após o primeiro trimestre. Não existe vacina eficaz contra a malária, apesar de haver esforços no sentido de desenvolver uma. O tratamento recomendado para a malária é uma artemisinina associada a um segundo antimalárico. Os antimaláricos são geralmente mefloquina, lumefantrina ou sulfadoxina/pirimetamina. Quando não está disponível artemisinina pode ser usada quinina com doxiciclina. Em áreas onde a doença é comum, recomenda-se que se confirme a presença da doença antes de iniciar o tratamento devido à preocupação com a crescente resistência farmacológica. Vários parasitas desenvolveram resistência a uma série de antimaláricos, como é o caso do P. falciparum resistente ao quinino e a resistência a artemisinina em várias partes do Sudeste Asiático. A malária é endémica em regiões tropicais e subtropicais devido à chuva abundante, temperatura quente e grande quantidade de água estagnada, o que proporciona habitats ideais para as larvas do mosquito. A doença encontra-se disseminada pelas regiões tropicais e subtropicais do planeta ao longo de uma larga faixa em redor do equador, que inclui grande parte da África subsaariana, Ásia e América Latina. Em 2016, ocorreram em todo o mundo 216 milhões de casos de malária, que se estima terem sido a causa de 731.000 mortes. Cerca de 90% dos casos e das mortes ocorreram em África. Entre 2000 e 2015 a incidência da doença diminuiu 37%. A malária está geralmente associada à pobreza e tem um grande impacto negativo no desenvolvimento económico. Em África, estima-se que a doença resulte em perdas de 12 bilhões de dólares por ano devido aos custos com a prestação de cuidados de saúde, baixas de trabalho e impacto no turismo. Os sinais e sintomas da malária manifestam-se geralmente entre 8 a 25 dias após a infecção. No entanto, os sintomas podem-se manifestar mais tarde em indivíduos que tenham tomado medicação antimalárica de prevenção. As manifestações iniciais da doença, iguais em todas as espécies de malária, são semelhantes aos sintomas da gripe,[13] podendo ainda ser semelhantes aos de outras doenças virais e condições clínicas como a sepse ou gastroenterite. Entre os sinais incluem-se dores de cabeça, febre, calafrios, dores nas articulações, vómitos, anemia hemolítica, icterícia, hemoglobina na urina, lesões na retina e convulsões. O sintoma clássico da malária são ataques paroxísticos, a ocorrência cíclica de uma sensação súbita de frio intenso seguida por calafrios e posteriormente por febre e sudação. Estes sintomas ocorrem a cada dois dias em infecções por P. vivax e P. ovale e a cada três dias em infecções por P. malariae. A infecção por P. falciparum pode provocar febre recorrente a cada 36-48 horas ou febre menos aguda, mas contínua.[15] Os casos mais graves de malária são geralmente provocados por P. falciparum, variante que é muitas vezes denominada "malária falciparum". Os sintomas desta variante manifestam-se entre 9 a 30 dias após a infecção. Os indivíduos com "malária cerebral" apresentam muitas vezes sintomas neurológicos, entre os quais postura anormal, nistagmo, paralisia do olhar conjugado (incapacidade de mover em conjunto os olhos na mesma direção), opistótono, convulsões ou coma.
Meningite é uma inflamação aguda das membranas protetoras que revestem o cérebro e a medula espinal, denominadas coletivamente por meninges. Os sintomas mais comuns são febre súbita e elevada, dor de cabeça intensa e rigidez no pescoço. Entre outros possíveis sintomas estão confusão mental ou alteração do estado de consciência, vómitos e intolerância à luz ou a barulho. As crianças mais novas geralmente manifestam apenas sintomas inespecíficos, como irritabilidade, sonolência ou recusa em comer. A meningite causada por bactérias meningocócicas apresenta manchas características na pele. A inflamação das meninges é geralmente causada por uma infeção por vírus, bactérias ou outros microorganismos. Ainda que de forma pouco comum, pode também ser causada por alguns medicamentos. A meningite pode provocar a morte devido à proximidade da inflamação com o cérebro e medula espinal, o que faz com que a condição seja classificada como emergência médica. Um diagnóstico de meningite pode ser confirmado ou excluído com uma punção lombar. Este procedimento consiste na inserção de uma agulha no canal medular para recolher uma amostra do líquido cefalorraquidiano que envolve o cérebro e medula espinal, a qual é posteriormente analisada em laboratório. Algumas formas de meningite podem ser prevenidas mediante vacinação com a vacina meningocócica, vacina contra a papeira, vacina antipneumocócica e vacina Hib. Pode também ser útil administrar antibióticos em pessoas com exposição significativa a determinados tipos de meningite. O tratamento inicial da meningite aguda é a administração imediata de antibióticos e, em alguns casos, de antivirais. Podem também ser administrados corticosteroides para prevenir complicações resultantes de uma inflamação excessiva. A meningite pode causar complicações graves a longo prazo, incluindo perda auditiva, epilepsia, hidrocefalia ou défice cognitivo, sobretudo quando não é tratada rapidamente. Quando não é tratada, a meningite bacteriana é quase sempre fatal. Pelo contrário, a meningite viral tende a resolver-se espontaneamente e raramente é fatal. Em 2015 houve 8,7 milhões de casos de meningite em todo o mundo. No mesmo ano, a doença foi responsável por 379.000 mortes, uma diminuição em relação às 464.000 em 1990. Com tratamento adequado e atempado, o risco de morte por meningite bacteriana é inferior a 15%. Entre dezembro e junho ocorrem frequentemente surtos da doença numa faixa da região da África subsariana entre a Gâmbia e a Eritreia. Podem também ocorrer pequenos surtos em outras regiões do mundo. O termo meningite tem origem no grego μῆνιγξ meninx, que significa "membrana", e no sufixo médico -ite, ou "inflamação". A tríade clássica de sinais de diagnóstico consiste em rigidez da nuca, febre súbita e elevada e dor de cabeça intensa com alteração do estado mental. Esta tríade denomina-se meningismo. No entanto, apenas cerca de 45% dos casos de meningite bacteriana é que apresentam em simultâneo todos estes três sinais. Quando não está presente nenhum dos três sinais, é extremamente improvável que se esteja na presença de meningite. Entre outros sinais geralmente associados à meningite estão a intolerância a luzes brilhantes e intolerância a barulhos. Em adultos, o sinal mais comum é uma dor de cabeça intensa, presente em cerca de 90% dos casos de meningite bacteriana, seguido por rigidez da nuca, presente em 70% dos casos. A rigidez na nuca é a incapacidade de mover passivamente o pescoço para a frente devido ao aumento do tónus muscular. No entanto, em muitos casos as crianças mais novas não apresentam os sintomas anteriormente mencionados, podendo apenas mostrar-se irritadas ou aparentar estarem doentes. Em bebés até seis meses de idade, é possível que as fontanelas se possam apresentar volumosas. Entre outras características que permitem distinguir meningite de outras doenças menos graves em crianças estão dores nas pernas, extremidades frias e cor da pele anormal. Entre outros possíveis sinais estão a presença do sinal de Kernig ou sinal de Brudziński. O sinal de Kernig pode ser avaliado deitando a pessoa de costas, com a anca e joelho fletidos 90°. Numa pessoa com sinal de Kernig positivo, a dor limita a extensão excessiva do joelho. Um sinal de Brudziński positivo ocorre quando a flexão do pescoço causa a flexão involuntária do joelho e anca. Embora estes sinais sejam geralmente usados no diagnóstico de meningite, são de sensibilidade limitada; ou seja, nem sempre ocorrem. No entanto, são muito específicos para a meningite, o que significa que raramente ocorrem em outras doenças. Outro exame, denominado "manobra de acentuação", ajuda a determinar a presença de meningite em pessoas com febre e dor de cabeça. Pede-se à pessoa que rode rapidamente e horizontalmente a cabeça. Se esta manobra não agravar a dor, é pouco provável que se trate de meningite. A meningite meningocócica pode ser diferenciada de outras meningites pela presença de uma mancha característica na pele, denominada púrpura. Esta mancha pode anteceder os outros sintomas. É constituída por numerosos pontos pequenos, irregulares e vermelhos ou azuis, no tronco, pernas, membranas mucosas, conjuntiva e, ocasionalmente, mas palmas das mãos ou pés. Ao ser pressionada com um dedo, a cor vermelha não desaparece. Embora esta mancha não esteja presente em todos os casos de meningite meningocócica, é relativamente específica da doença, embora em alguns casos possa ocorrer em meningite causada por outras bactérias. Entre outros possíveis sinais de meningite na pele estão sinais de febre aftosa e herpes genital, ambas associadas a várias formas de meningite viral.
A poliomielite, também chamada de pólio ou paralisia infantil, é uma doença infecciosa viral aguda transmitida de pessoa a pessoa, principalmente pela via fecal-oral. O termo deriva do grego poliós (πολιός), que significa "cinza", myelós (µυελός "medula"), referindo-se à substância cinzenta da medula espinhal, e o sufixo -itis, que denota inflamação, ou seja, inflamação da substância cinzenta da medula espinhal. Contudo, algumas infecções mais graves podem se estender até o tronco encefálico e ainda para estruturas superiores, resultando em polioencefalite, que provoca apneia, a qual requer ventilação mecânica com o uso de um respirador artificial. Embora aproximadamente 90% das infecções por pólio não causem sintomas (são assintomáticas), os indivíduos afetados podem exibir uma variedade de sintomas se o vírus atingir a corrente sanguínea. Em cerca de 1% dos casos, o vírus alcança o sistema nervoso central, preferencialmente infectando e destruindo neurônios motores, levando à fraqueza muscular e à paralisia flácida aguda. Diferentes tipos de paralisia podem ocorrer, dependendo dos nervos envolvidos. A pólio espinhal é a forma mais comum, caracterizada por paralisia assimétrica que, com frequência, afeta as pernas. A pólio bulbar cursa com fraqueza dos músculos inervados pelos nervos cranianos. A pólio bulboespinhal é uma combinação das paralisias bulbar e espinhal. A poliomielite foi reconhecida pela primeira vez como uma condição distinta por Jakob Heine, em 1840. Seu agente causador, o poliovírus, foi identificado em 1908 por Karl Landsteiner. Embora grandes epidemias de pólio sejam desconhecidas até o final do século XIX, esta foi uma das doenças infantis mais temidas do século XX. As epidemias de pólio causaram deficiências físicas em milhares de pessoas, principalmente crianças. A pólio existiu por milhares de anos silenciosamente, como um patógeno endêmico até os anos 1880, quando grandes epidemias começaram a ocorrer na Europa; pouco depois, as epidemias espalharam-se nos Estados Unidos. Por volta de 1910, grande parte do mundo experimentou um aumento dramático dos casos de poliomielite e as epidemias tornaram-se eventos comuns, principalmente nas cidades durante os meses de verão. Essas epidemias — que deixaram milhares de crianças e adultos paralíticos — incentivaram a "Grande Corrida" em busca do desenvolvimento de uma vacina. Desenvolvida na década de 1950, a vacina contra a pólio reduziu o número global de casos da doença por ano de centenas de milhares para menos de mil. Os esforços pela vacinação, apoiados pela GAVI Alliance, Rotary International, Organização Mundial da Saúde (OMS) e UNICEF, devem resultar na erradicação global desta doença. Em agosto de 2020 a OMS anunciou que apenas dois países ainda tinham casos de transmissão e que "o mundo está mais perto de alcançar a erradicação global da pólio". O termo "poliomielite" é usado para identificar a doença causada por qualquer um dos três sorotipos do poliovírus. Dois padrões básicos de infecção por pólio são descritos: a doença menor, que não envolve o sistema nervoso central (SNC), algumas vezes chamada de poliomielite abortada, e a forma maior envolvendo o SNC, que pode ser paralítica ou não paralítica. Na maioria das pessoas com sistema imunitário normal, uma infecção por poliovírus é assintomática. Raramente a infecção produz sintomas com alguma importância; esses podem incluir infecção das vias aéreas superiores (infecção da garganta e febre), distúrbios gastrointestinais (náuseas, vômitos, dores abdominais, constipação ou, raramente, diarreia) e sintomas de gripe. O vírus atinge o sistema nervoso central em cerca de 3% dos pacientes. A maioria dos indivíduos com envolvimento do SNC desenvolve meningite asséptica não paralítica, com sintomas de dor de cabeça, no pescoço, nas costas, no abdome e nas extremidades, febre, vômitos, letargia e irritabilidade. Aproximadamente de um a cinco casos em mil progridem para a doença paralítica, na qual os músculos se tornam fracos, moles e de difícil controle, e, finalmente, completamente paralisados; essa condição é conhecida como paralisia flácida aguda. Dependendo do local da paralisia, a poliomielite paralítica é classificada como espinhal, bulbar ou bulboespinhal. A encefalite, infecção do próprio tecido cerebral, pode ocorrer em casos raros e geralmente só atinge crianças muito jovens. É caracterizada por confusão, alterações do estado mental, cefaleias, febre e, mais raramente, convulsões e paralisia espástica. 
Síndrome da imunodeficiência adquirida (SIDA; em inglês: acquired immunodeficiency syndrome, AIDS) é uma doença do sistema imunológico humano causada pelo vírus da imunodeficiência humana (VIH; em inglês: human immunodeficiency virus, HIV). Durante a infecção inicial, uma pessoa pode passar por um breve período doente, com sintomas semelhantes aos da gripe. Normalmente isto é seguido por um período prolongado sem qualquer outro sintoma. À medida que a doença progride, ela interfere mais e mais no sistema imunológico, tornando a pessoa muito mais propensa a ter outros tipos de doenças, como infecções oportunistas e câncer, que geralmente não afetam as pessoas com um sistema imunológico saudável. O HIV é transmitido principalmente através de relações sexuais sem o uso de preservativo (incluindo sexo anal e, até mesmo, oral), transfusões de sangue contaminado, agulhas hipodérmicas e de mãe para filho, durante a gravidez, o parto ou amamentação. Outros fluidos corporais, como saliva e lágrimas, não transmitem o vírus. A prevenção da contaminação pelo HIV, principalmente através de programas de sexo seguro e de troca de agulhas, é uma estratégia fundamental para controlar a propagação da doença. Apesar de ainda não existir uma cura definitiva ou uma vacina, o tratamento antirretroviral pode retardar o desenvolvimento da doença e elevar a expectativa de vida do portador do vírus. Enquanto o tratamento antirretroviral reduz o risco de morte e de complicações da doença, estes medicamentos são de alto custo e estão associados a diversos efeitos colaterais. No entanto, há três casos confirmados de pacientes que eliminaram o vírus do corpo após realizarem um transplante de medula óssea, em que tiverem células-tronco implantadas em seu organismo que vieram de um doador com uma mutação genética forneceu resistência ao HIV. Tal procedimento, todavia, ainda é arriscado e complexo para ser aplicado a todos os soropositivos. A pesquisa genética indica que o HIV surgiu no centro-oeste da África durante o início do século XX. A AIDS foi reconhecida pela primeira vez em 1981, pelo Centro de Controle e Prevenção de Doenças (CDC) dos Estados Unidos, e a sua causa — o HIV — foi identificada na primeira metade da década. Desde a sua descoberta, a AIDS causou a morte de aproximadamente 30 milhões de pessoas (até 2009). Em 2010, cerca de 34 milhões de pessoas eram portadoras do vírus no mundo. A AIDS é considerada uma pandemia, um surto de doença que está presente em uma grande área e que está se espalhando ativamente. HIV/AIDS têm tido um grande impacto na sociedade contemporânea, tanto como uma doença quanto como uma fonte de discriminação. A doença também tem impactos econômicos significativos. Há muitos equívocos sobre o HIV/AIDS, tais como a crença de que ela pode ser transmitida pelo contato casual não sexual. A doença também se tornou sujeita a muitas controvérsias envolvendo as religiões, além de ter atraído a atenção médica e política internacional (e um financiamento de larga escala) desde que foi identificada em 1980. Mais de 75 milhões de pessoas foram infectadas em todo o mundo até o final de 2019. Aproximadamente 34,7 milhões de pessoas até 2020 morreram. História. Descoberta. A AIDS foi observada clinicamente pela primeira vez em 1981, nos Estados Unidos. Os casos iniciais ocorreram em um grupo de usuários de drogas injetáveis ​​e de homens homossexuais que estavam com a imunidade comprometida sem motivo aparente. Eles apresentavam sintomas de pneumonia pelo fungo Pneumocystis carinii (PCP), uma infecção oportunista incomum até então, conhecida por ocorrer em pessoas com o sistema imunológico muito debilitado. Pouco depois, um número inesperado de homens gays desenvolveu um tipo de câncer de pele raro chamado sarcoma de Kaposi. Muitos mais casos de PCP e de sarcoma de Kaposi surgiram, quando um alerta foi dado ao Centro de Controle e Prevenção de Doenças (CDC), que enviou uma força-tarefa para acompanhar o surto. Robert Gallo, codescobridor do HIV no início dos anos 1980 entre (da esquerda para a direita) Sandra Eva, Sandra Colombini e Ersell Richardson. Nos primeiros dias o CDC não tinha um nome oficial para a doença e referia-se a ela por meio das condições clínicas associadas como, por exemplo, a linfadenopatia, chamando-a de "linfadenopatia generalizada persistente". Eles também usavam "Sarcoma de Kaposi e infecções oportunistas", nome pelo qual uma força-tarefa foi criada em 1981. Em determinado momento, o CDC cunhou a frase "a doença dos 4 H's", uma vez que a síndrome parecia afetar haitianos, homossexuais, hemofílicos e usuários de heroína. Na imprensa geral, o termo "GRID", de gay-related immune deficiency (em tradução livre, "deficiência imunológica relacionada aos gays"), tinha sido inventado. No entanto, depois de determinar que a AIDS não estava restrita à comunidade homossexual, percebeu-se que o termo GRID estava errado e a sigla AIDS, de acquired immunodeficiency syndrome (em português: síndrome da imunodeficiência adquirida, SIDA), foi introduzida em uma reunião em julho de 1982. Em setembro daquele mesmo ano, o CDC começou a se referir à doença como AIDS. Em 1983, dois grupos de pesquisa independentes liderados por Robert Gallo e Luc Montagnier declararam que um novo retrovírus poderia ter infectado os pacientes com AIDS e publicaram suas descobertas na mesma edição da revista Science. Gallo afirmou que o vírus que seu grupo de pesquisa isolou de um paciente com AIDS tinha uma forma muito semelhante a de outros vírus T-linfotrópicos, que sua equipe tinha sido a primeira a isolar. O grupo de Gallo chamou o vírus recém isolado de HTLV-III. Ao mesmo tempo, o grupo de Montagnier isolou um vírus a partir de um paciente que apresentava inchaço dos nódulos linfáticos do pescoço e fraqueza física, dois sintomas característicos da AIDS. Contradizendo o relatório do grupo de Gallo, Montagnier e seus colegas mostraram que as proteínas do núcleo do vírus eram imunologicamente diferentes das do HTLV-I. O grupo de Montagnier chamou o vírus que isolaram de lymphadenopathy-associated virus, LAV (em português: "vírus associado à linfadenopatia"). Quando, em 1986, descobriu-se que estes dois vírus eram o mesmo, LAV e HTLV-III foram renomeados para HIV, sigla em inglês de vírus da imunodeficiência humana.
Metrô de São Paulo ou Metropolitano de São Paulo, conhecido popularmente como Metrô, é um sistema de transporte metroviário que serve a cidade de São Paulo, no Brasil. O Metrô de São Paulo é operado pela Companhia do Metropolitano de São Paulo, sociedade de economia mista do estado de São Paulo. Fundada em 24 de abril de 1968, a empresa é responsável pelo planejamento, projeto, construção e operação do sistema de transporte metroviário na Região Metropolitana de São Paulo. Tendo a maior parte de seu controle acionário associada ao governo do estado, é subordinada à Secretaria dos Transportes Metropolitanos do Estado de São Paulo. Integra também a Rede Metropolitana de Transporte de São Paulo. O Grupo CCR (por meio das concessionárias ViaQuatro e ViaMobilidade) opera, respectivamente, as linhas 4–Amarela e 5–Lilás do sistema. O metrô paulista está em operação desde 14 de setembro de 1974. É o maior e mais movimentado sistema de transporte metroviário do Brasil, com uma extensão de 104,4 quilômetros de linhas ferroviárias distribuídas em seis linhas, que possuem um total de 91 estações (63 operadas totalmente pelo Metrô, 11 operadas totalmente pela ViaQuatro, 17 operadas totalmente pela ViaMobilidade, duas estações operadas pelo Metrô e pela ViaQuatro: Luz e República e duas estações operadas pelo Metrô e pela ViaMobilidade: Santa Cruz e Chácara Klabin). Compõem o sistema as linhas: 1–Azul (Jabaquara↔Tucuruvi), 2–Verde (Vila Madalena↔Vila Prudente), 3–Vermelha (Corinthians • Itaquera↔Palmeiras • Barra Funda), 4–Amarela (Luz↔Vila Sônia), 5–Lilás (Capão Redondo↔Chácara Klabin) e 15–Prata (Vila Prudente↔Jardim Colonial). Possui interligação com o sistema de trens urbanos, através de integração com linhas da Companhia Paulista de Trens Metropolitanos (CPTM) nas estações Brás, Tatuapé, Palmeiras–Barra Funda, Luz, Corinthians–Itaquera, Santo Amaro, Tamanduateí e Pinheiros, e em outros terminais de transporte intermodal na cidade de São Paulo. Diariamente, o sistema de São Paulo transporta cerca de 5,3 milhões de passageiros. Em 2010, o Metrô de São Paulo foi considerado o melhor sistema de transporte sobre trilhos da América Latina pelo The Metro Awards, sendo o primeiro da região em ter uma estação equipada com portas de plataforma (2010), sistema CBTC para sinalização e controle de trens (2010) e trens com tecnologia 100% automática e sem condutores (2010). Em agosto de 2015, foi eleito um dos melhores sistemas de metrô do mundo pela revista americana Business Insider, sendo o único sistema latino-americano a pertencer a essa lista. No entanto, engenheiros especializados em transportes urbanos afirmam que o metrô está saturado e que a solução deste problema só poderá ocorrer a longo prazo. Apesar dos investimentos em expansão e modernização, sua extensão é considerada insuficiente para as dimensões da região metropolitana que serve. Um estudo realizado em 2010 revelou que o metrô paulistano era, naquele ano, o mais lotado do mundo, com 11,5 milhões de passageiros transportados a cada quilômetro de linha. A demanda de passageiros naquele ano exigia, segundo o estudo, duzentos quilômetros de linhas e havia então 70,6 quilômetros. O recorde de metrô mais lotado do mundo já havia sido atingido em 2008, quando foram transportados dez milhões de passageiros por quilômetro. A superlotação do sistema, aliada a falhas constantes na operação, refletiu na percepção dos usuários. Em setembro de 2011, o metrô obteve a pior avaliação de sua história, segundo pesquisa realizada pelo Datafolha junto aos usuários do sistema. Em maio de 2015, havia 78,3 quilômetros de linhas, uma expansão de menos de oito quilômetros em cinco anos. Em 2011, a meta anunciada pelo governo do Estado era chegar a cem quilômetros de linhas até o fim de 2014, incluindo o monotrilho, o que só ocorreria no final de 2019.
Pedro II (Rio de Janeiro, 2 de dezembro de 1825 – Paris, 5 de dezembro de 1891), cognominado "o Magnânimo", foi o segundo e último monarca do Império do Brasil, tendo imperado no país durante um período de 58 anos, foi o filho mais novo do imperador Pedro I do Brasil e da imperatriz consorte Maria Leopoldina da Áustria e, portanto, membro do ramo brasileiro da Casa de Bragança. Nascido no Palácio imperial de São Cristóvão, no Rio de Janeiro. A abrupta abdicação do pai e sua partida para Portugal, tornaram Pedro imperador com apenas cinco anos. Obrigado a passar a maior parte do seu tempo estudando em preparação para reinar, conheceu poucos momentos de alegria e amigos de sua idade. Suas experiências com intrigas palacianas e disputas políticas durante este período tiveram grande impacto na formação de seu caráter. O imperador D. Pedro II tornou-se um homem com forte senso de dever e devoção ao seu país e seu povo. Por outro lado, ressentiu-se cada vez mais de seu papel como monarca. Teve a maioridade decretada para assumir o governo e evitar a desintegração do Império, tendo deixado ao sucessor republicano um país caracterizado como potência emergente na arena internacional. A nação distinguiu-se de seus vizinhos hispano-americanos devido à sua estabilidade política e especialmente por sua forma de governo: uma funcional monarquia parlamentar constitucional. O Brasil também foi vitorioso em três conflitos internacionais (a Guerra do Prata, a Guerra do Uruguai e a Guerra do Paraguai) sob seu império, assim como prevaleceu em outras disputas internacionais e tensões domésticas. Um erudito, o imperador estabeleceu uma reputação como um vigoroso patrocinador do conhecimento, da cultura e das ciências. Ele ganhou o respeito e admiração de estudiosos como Graham Bell, Charles Darwin, Victor Hugo e Friedrich Nietzsche, e foi amigo de Richard Wagner, Louis Pasteur e Henry Wadsworth Longfellow, dentre outros. D. Pedro II não permitiu nenhuma medida contra sua remoção e não apoiou qualquer tentativa de restauração da monarquia. O imperador deposto passou os seus últimos dois anos de vida no exílio na Europa, vivendo só. Algumas décadas após sua morte, sua reputação foi restaurada e seus restos mortais foram trazidos de volta ao Brasil em meio a amplas celebrações. Pedro nasceu às 02h30 da manhã do dia 2 de dezembro de 1825 no Palácio de São Cristóvão, na cidade do Rio de Janeiro, Brasil. Batizado em homenagem a São Pedro de Alcântara, seu nome completo era Pedro de Alcântara João Carlos Leopoldo Salvador Bibiano Francisco Xavier de Paula Leocádio Miguel Gabriel Rafael Gonzaga. Seu pai, o imperador D.Pedro I, foi o fundador do ramo brasileiro da Casa de Bragança e seu nome era precedido pelo honorífico "Dom" ("Senhor" ou "Lorde") desde o nascimento. Era neto do rei português João VI e sobrinho de Miguel I. Sua mãe era a arquiduquesa Maria Leopoldina da Áustria, filha de Francisco II, último monarca do Sacro Império Romano-Germânico. Por sua mãe, Pedro era sobrinho de Napoleão Bonaparte e primo dos imperadores Francisco José I da Áustria e Maximiliano do México. Único filho legítimo do sexo masculino de D.Pedro I a sobreviver à infância, foi oficialmente reconhecido como herdeiro do trono brasileiro com o título de Príncipe Imperial a 6 de agosto de 1826. A imperatriz consorte D.Leopoldina morreu a 11 de dezembro de 1826, poucos dias após dar à luz um menino natimorto, quando Pedro tinha um ano de idade. Pedro não guardou recordações de sua mãe, a não ser pelo que depois lhe foi contado. A influência e lembrança de seu pai também apagou-se com o tempo, e não guardou fortes imagens de Pedro I, mas apenas poucas e vagas lembranças. Dois anos e meio após a morte de Leopoldina, o imperador casou-se com Amélia de Leuchtenberg. O príncipe Pedro passou pouco tempo com sua madrasta; no entanto, criaram um relacionamento afetuoso e mantiveram contato até a morte dela em 1873. O imperador D.Pedro I abdicou em 7 de abril de 1831, após um longo conflito com a facção liberal (que por sua vez iria mais tarde dividir-se nos dois partidos dominantes na monarquia, o Conservador e o Liberal) dominante no parlamento. Ele e D.Amélia partiram imediatamente para a Europa, onde o agora novamente príncipe D.Pedro iria lutar para restaurar sua filha Maria II, cujo trono em Portugal fora usurpado por seu irmão Miguel I. Deixado para trás, o príncipe imperial. Pedro tornou-se pelas leis sucessórias, imediatamente "Dom Pedro II. Por graça de Deus e unânime aclamação dos povos, Imperador Constitucional e Defensor Perpétuo do Brasil". Seguindo assim a velha tradição portuguesa de "Rei morto, Rei posto."
Pedro I do Brasil ou Pedro IV de Portugal (Queluz, 12 de outubro de 1798 – Queluz, 24 de setembro de 1834), apelidado de "o Libertador" e "o Rei Soldado", foi o primeiro Imperador do Brasil como Pedro I de 1822 até sua abdicação em 1831, e também Rei de Portugal e Algarves como Pedro IV entre março e maio de 1826. Era o quarto filho do rei João VI de Portugal e de sua esposa, a rainha Carlota Joaquina da Espanha, sendo assim um membro da Casa de Bragança. Pedro viveu seus primeiros anos de vida em Portugal até que as tropas francesas invadiram o país em 1807, forçando a transferência da família real para o Brasil. A eclosão da Revolução liberal do Porto em 1820, forçou a volta de João VI para Portugal em abril do ano seguinte, ficando Pedro no Brasil como seu regente. Ele precisou lidar com ameaças de tropas portuguesas revolucionárias e insubordinadas, com todas no final sendo subjugadas. Desde a chegada da família real portuguesa em 1808, o Brasil tinha gozado de grande autonomia política, porém a ameaça do governo português de revogar essas liberdades criou grande descontentamento na colônia. Pedro ficou do lado dos brasileiros e declarou a Independência do Brasil em 7 de setembro de 1822. Foi aclamado como seu imperador no dia 12 de outubro e derrotou todas as forças fiéis a Portugal até março de 1824. Alguns meses depois, esmagou a Confederação do Equador, uma revolta separatista que havia eclodido na província de Pernambuco e se alastrado para outras províncias do nordeste brasileiro. Uma nova rebelião se iniciou na província Cisplatina no começo de 1825, com a subsequente tentativa por parte das Províncias Unidas do Rio da Prata de anexá-la levando o Brasil a entrar na Guerra da Cisplatina. Nesse meio tempo Pedro também se proclamou como rei de Portugal, rapidamente abdicando do trono em favor de sua filha mais velha Maria II. A situação piorou em 1828 quando a guerra no sul fez o Brasil perder a Cisplatina, que se tornou um país independente, denominado Uruguai. No mesmo ano D. Miguel, irmão mais novo de Pedro, era aclamado como Rei de Portugal pelas cortes. Seus casos sexuais extraconjugais criaram grandes escândalos e também mancharam sua reputação. Mais dificuldades surgiram no parlamento brasileiro, onde os debates políticos passaram a ser dominados a partir de 1826 com a discussão sobre se o governo deveria ser escolhido pelo imperador ou pela legislatura. Pedro foi incapaz de lidar com os problemas simultâneos do Brasil e Portugal, por fim abdicando do trono brasileiro em 7 de abril de 1831 em favor de seu filho mais novo Pedro II e partindo para a Europa. Pedro invadiu Portugal em julho de 1832 no comando de um exército maioritariamente composto por mercenários estrangeiros. Inicialmente seu envolvimento parecia ser em uma guerra civil portuguesa, porém logo o conflito ficou maior e englobou toda a Península Ibérica em uma disputa entre defensores do liberalismo e aqueles que queriam a volta do absolutismo. Pedro acabou morrendo de tuberculose em 24 de setembro de 1834, poucos meses depois de ele e os liberais terem saído vitoriosos. Ele foi considerado por contemporâneos e pela posteridade como uma figura importante que auxiliou na propagação dos ideais liberais que haviam permitido que o Brasil e Portugal deixassem os regimes absolutistas para formas mais representativas de governo.
A imigração italiana no Brasil teve como ápice o período entre 1880 e 1930. Os ítalo-brasileiros estão espalhados principalmente pelos estados do Sul e do Sudeste do Brasil. Os ítalo-brasileiros são descendentes da enorme massa de imigrantes italianos que chegaram ao Brasil entre 1870 e 1960. Não existem dados concretos sobre o número de descendentes de italianos no Brasil, visto que o censo nacional do Instituto Brasileiro de Geografia e Estatística (IBGE) não questiona a ancestralidade do povo brasileiro há várias décadas. No último censo a questionar a ancestralidade, o de 1940, 1.260.931 brasileiros disseram ser filhos de pai italiano, enquanto que 1 069 862 disseram ser filhos de mãe italiana. Os italianos natos eram 285 mil e os naturalizados brasileiros, 40 mil. Portanto, italianos e filhos eram pouco mais de 3,8% da população do Brasil em 1940. Em 1925, o governo da Itália havia estimado que italianos e descendentes eram 6% da população brasileira e 15% da população branca. Uma pesquisa de 1999, do sociólogo, ex-presidente do IBGE, Simon Schwartzman, indicou que cerca de 10% dos brasileiros entrevistados afirmaram ter ancestralidade italiana, percentual que, numa população de cerca de 200 milhões de brasileiros, representaria em torno de 20 milhões de descendentes. Uma fonte italiana, de 1996, cita o número de 22 753 000 descendentes. Segundo pesquisa de 2016 publicada pelo IPEA, em um universo de 46 801 772 nomes de brasileiros analisados, 3 594 043 ou 7,7% deles tinham o último ou o único sobrenome de origem italiana. A embaixada italiana no Brasil, em 2013, divulgou o número de 30 milhões de descendentes de imigrantes italianos (cerca de 15% da população brasileira), metade no estado de São Paulo. Segundo pesquisa do demógrafo Giorgio Mortara, complementada na década de 1980 por Judicael Clevelário, apenas entre 16 e 18% da população brasileira descendia de imigrantes entrados no Brasil após 1840, incluindo italianos e todas as outras nacionalidades. A maioria dos estudos sobre o impacto da imigração tem seguido as conclusões de Giorgio Mortara das décadas de 1940 e 1950. Mortara concluiu que apenas cerca de 15% do crescimento demográfico do Brasil, de 1840 e 1940, deveu-se à imigração, e que a população de origem imigrante (imigrantes e descendentes) era de 16% da população total do Brasil. Os ítalo-brasileiros são considerados a maior população de oriundi (descendentes de italianos) fora da Itália. Eles mantêm os costumes tradicionais italianos, assim como parte da população brasileira, que acabou por absorvê-los por causa do impacto da imigração italiana no Brasil. A contribuição dos italianos é notável em todos os setores da sociedade brasileira, principalmente na mudança socioeconômica que os italianos produziram no campo e nas cidades. Podemos citar desde o modo de vida que mudou profundamente influenciado pelo catolicismo, bem como nas artes, música, arquitetura, alimentação e no empreender italiano na abertura de empresas, e também como trabalhadores especializados. No campo, podemos citar a introdução de novas técnicas agrícolas, e principalmente na mudança do latifúndio para pequenas propriedades agrícolas e na introdução da policultura de produtos. A grande maioria dos ítalo-brasileiros está no sul e no sudeste do Brasil, mas há ítalo-brasileiros também em outras regiões do Brasil. Muitos ítalo-brasileiros já residentes no Brasil, em especial no sul, migrariam para estados do Centro-Oeste – em especial para o Mato Grosso do Sul. No Rio Grande do Sul, Santa Catarina, Paraná e Espírito Santo, alguns ítalo-brasileiros ainda falam italiano e outros dialetos regionais da Itália, mas ítalo-brasileiros mais jovens costumam falar apenas português.
Museu de Arte de São Paulo Assis Chateaubriand (mais conhecido pelo acrônimo MASP) é um centro cultural e museu brasileiro concebido em 1947 idealizado pelo jornalista paraibano Assis Chateaubriand com o crítico de arte italiano Pietro Bardi. Situado desde 7 de novembro de 1968, na Avenida Paulista, cidade de São Paulo (estado homônimo), em um edifício projetado pela arquiteta ítalo-brasileira Lina Bo Bardi para ser sua sede, é considerado uma das mais importantes instituições culturais brasileiras. É conhecido pelo vão de mais de 70 metros que se estende sob quatro enormes pilares, concebido pelo engenheiro José Carlos de Figueiredo Ferraz, o edifício é considerado um importante exemplar da arquitetura brutalista brasileira, sendo tombado pelas três instâncias de proteção ao patrimônio: IPHAN, Condephaat e Conpresp. Instituição particular sem fins lucrativos, o museu foi fundado em 1947, ao longo de sua história, notabilizou-se por uma série de iniciativas importantes no campo da museologia e da formação artística, bem como por sua forte atuação didática. Foi também um dos primeiros espaços museológicos do continente a atuar com perfil de centro cultural, bem como o primeiro museu do país a acolher as tendências artísticas surgidas após a Segunda Guerra Mundial. O MASP possui a mais importante e abrangente coleção de arte ocidental da América Latina e do hemisfério sul, em que se notabilizam sobretudo os consistentes conjuntos referentes às escolas italiana e francesa. Possui também extensa seção de arte brasileira e pequenos conjuntos de arte africana e asiática, artes decorativas, peças arqueológicas etc., totalizando aproximadamente 8 mil peças. O acervo é tombado pelo Instituto do Patrimônio Histórico e Artístico Nacional (IPHAN). O museu também abriga uma das maiores bibliotecas especializadas em arte do país. A década de 1940 caracterizou-se no Brasil como um período de grande efervescência no plano econômico e político. A ascensão de Getúlio Vargas ao poder, em 1930, havia marcado o fim do liberalismo e uma maior interferência do Estado na vida econômica do país, mas fatores de ordem internacional, como a Segunda Guerra Mundial e a crise de 1929, favoreceram um surto de desenvolvimento industrial, em substituição ao ciclo do café, tendo como consequência direta a criação das condições necessárias ao crescimento urbano e à instalação de uma "estrutura cultural" no país. Em São Paulo, particularmente, o período se notabilizou pela consolidação de um vigoroso parque industrial. O estado, a essa altura, já havia suplantado o Rio de Janeiro como principal produtor de bens de consumo do país. A capital paulista prosseguia em sua trajetória de extraordinário crescimento populacional. Atraindo muitas indústrias e concentrando uma expressiva e poderosa elite, abandonava progressivamente o aspecto de cidade provinciana. No plano cultural, sem embargo, São Paulo ainda distava muito da então capital federal, onde o debate estético encontrava-se muito mais adiantado e o poder público já assimilava as manifestações modernas internacionais (sendo o edifício do Ministério da Educação e Cultura o exemplo maior de tal contexto). Sua referência mais notável continuava a ser a Semana de Arte Moderna de 1922. Se por um lado esse evento havia permitido alguma abertura aos artistas modernos nos salões oficiais, influenciado a criação de grupos e associações como a Sociedade Pró-Arte Moderna e a Família Artística Paulista e garantido alguma substância ao debate estético, por outro, seus propósitos não chegaram a atingir o grande público nem a definir um circuito artístico local. A cidade contava com uma casa de ópera de prestígio e com uma grande quantia de cine-teatros, de programação bastante diversificada, mas havia um único museu voltado à arte, a Pinacoteca do Estado, dedicada quase exclusivamente à arte acadêmica. A Escola de Belas Artes seguia a mesma orientação e eram poucas as galerias comerciais abertas às tendências modernas. O paraibano Assis Chateaubriand, fundador e proprietário dos Diários Associados - à época o maior conglomerado de veículos de comunicação do Brasil – foi uma das figuras mais emblemáticas desse período. Comandava um verdadeiro império midiático, composto por 34 jornais, 36 emissoras de rádio, uma agência de notícias, uma editora (responsável pela publicação da revista O Cruzeiro, a mais lida do país entre 1930 e 1960) e se preparava para ser o pioneiro da televisão na América Latina - e futuro proprietário de 18 estações. Dono de um espírito empreendedor, Chateaubriand manteve uma postura ativa no processo de modernização do Brasil e utilizava-se da influência de seu conglomerado para pressionar a elite do país a auxiliá-lo em suas iniciativas, quer fossem políticas, econômicas ou culturais. Em meados dos anos quarenta, criou a "campanha da aviação", que consistia em enérgicos pedidos de contribuições para a aquisição de aeronaves de treinamento a serem doados ao aeroclubes do país. Como fruto da iniciativa, cerca de mil aviões foram comprados e doados às escolas para formação de pilotos. Terminada a campanha, Chateaubriand iniciaria uma nova e ousada empreitada: a aquisição de obras de arte para formar um museu de nível internacional no Brasil.
A Pinacoteca do Estado de São Paulo é um dos mais importantes museus de arte do Brasil. Ocupa um edifício construído em 1900, no Jardim da Luz, centro de São Paulo, projetado por Ramos de Azevedo e Domiziano Rossi para ser a sede do Liceu de Artes e Ofícios. É o mais antigo museu de arte de São Paulo, fundado em 1905 e regulamentado como museu público estadual desde 1911. Após a reforma conduzida por Paulo Mendes da Rocha na década de 1990, tornou-se uma das mais dinâmicas instituições culturais do país, integrando-se ao circuito internacional de exposições, promovendo eventos culturais diversos e mantendo uma ativa produção bibliográfica. A Pina, como também é conhecida, também administra o espaço denominado Estação Pinacoteca, ou Pina Estação, instalado no antigo edifício do DOPS, no Bom Retiro, onde mantém exposições temporárias de arte contemporânea, a Biblioteca Walter Wey e o Centro de Documentação e Memória da instituição. A Pinacoteca abriga um dos maiores e mais representativos acervos de arte brasileira, com mais dez mil peças abrangendo majoritariamente a história da pintura brasileira dos séculos XIX e XX. Destacam-se também a Coleção Brasiliana, integrada por trabalhos de artistas estrangeiros atuantes no Brasil ou inspirados pela iconografia do país, a Coleção Nemirovsky, com um expressivo conjunto de obras-primas do modernismo brasileiro e, mais recentemente, a Coleção Roger Wright, recebida em comodato no mês de janeiro de 2015. Logotipo e designação. Desde janeiro de 2016, a Pinacoteca usa um logo apresentando somente o "apelido" Pina. Segundo o então diretor de relações institucionais do museu, Paulo Vicelli, a mudança oficializa o nome que os frequentadores do museu já utilizavam. A Estação Pinacoteca passou a ser designada por “Pina Estação”, e o museu do Jardim da Luz por “Pina Luz”. A nova identidade visual foi criada pela agência de publicidade F/Nazca Saatchi & Saatchi. Os elementos marcantes da arquitetura do museu, como as colunas, os tijolos e a escadaria, fazem parte do sistema de identidade visual. História. Antecedentes. As origens da Pinacoteca do Estado remetem à criação do Liceu de Artes e Ofícios de São Paulo. Este, por sua vez, é fruto de um contexto de profundas modificações sociais, políticas e econômicas que operavam em São Paulo na segunda metade do século XIX. A então província, que se mantivera de pouca expressão até a década 1870, metamorfoseava-se, alavancada pela expansão cafeeira e pela consolidação da rede ferroviária. Recebia um grande fluxo de imigrantes (intensificado após a abolição da escravatura), que conferiam transformações significativas, abarcando desde a cultura material e hábitos alimentares até as novas formas de sociabilização. Os núcleos urbanos adensavam-se e modernizavam-se. Na cidade de São Paulo, os capitais acumulados pelos cafeicultores eram reinvestidos na incipiente indústria, realimentando o ciclo de prosperidade. Novos edifícios eram construídos e a técnica da taipa cedia espaço à alvenaria de tijolos. Bairros nobres eram construídos para abrigar os solares e palacetes dos barões do café, sempre seguindo os padrões arquitetônicos europeus, marcados pelo ecletismo. Ainda mais numerosos eram os bairros operários que surgiam, expandindo de forma acelerada o núcleo urbano.
A Bandeira do Brasil constitui a bandeira nacional da República Federativa do Brasil. É composta por uma base verde em forma de retângulo, sobreposta por um losango amarelo e um círculo azul, no meio do qual está atravessada uma faixa branca com o lema "Ordem e Progresso", em letras maiúsculas verdes. O Brasil adotou oficialmente este projeto para sua bandeira nacional em 19 de novembro de 1889, substituindo a bandeira do Império do Brasil. O conceito foi criado por Raimundo Teixeira Mendes, com a colaboração de Miguel Lemos, Manuel Pereira Reis e Décio Villares. É um dos símbolos nacionais brasileiros, ao lado do Laço Nacional, do Selo Nacional, do Brasão de Armas e do Hino Nacional. O lema "Ordem e Progresso" é inspirado pelo lema do positivismo de Auguste Comte: O Amor por princípio e a Ordem por base; o Progresso por fim, versão traduzida do francês. O campo verde e o losango dourado da bandeira imperial anterior foram preservados — o verde representava a Casa de Bragança de Pedro I, o primeiro imperador do Brasil, enquanto o ouro representava a Casa de Habsburgo de sua esposa, a imperatriz Maria Leopoldina. O círculo azul com 27 estrelas brancas de cinco pontas substituiu o brasão de armas do Império. As estrelas, cuja posição na bandeira refletem o céu visto na capital Rio de Janeiro em 15 de novembro de 1889, representam as unidades federativas — cada estrela representa um estado específico, além do Distrito Federal. Apenas depois da independência que foi criada a primeira bandeira oficial do Brasil. Entre setembro e dezembro de 1822, o pavilhão pessoal do antigo príncipe real do Reino Unido — um losango amarelo em campo verde, tendo ao meio o brasão de armas do príncipe criado por Jean-Baptiste Debret a pedido de D. Pedro I —, passou a ser usado para representar o novo país. Com a consagração de D. Pedro I como imperador do Brasil, a coroa real que ornava o brasão foi substituída pela coroa imperial. O decreto que originalmente instituiu a bandeira e o brasão nacionais do Brasil, assinado aos 18 de setembro de 1822, nada oficializa sobre os possíveis significados das formas e cores adotadas. Outro decreto, que institui o laço nacional do Brasil e que também é datado de 18 de setembro de 1822, assim determina as cores escolhidas: "(…) será composto das cores emblemáticas — verde de primavera e amarelo d'ouro". Em 29 de setembro de 1823, um agente diplomático do Brasil junto à corte de Viena teria descrito a nova bandeira a Metternich, explicando ser a cor verde em referência à casa de Bragança, da qual fazia parte imperador D. Pedro I, ao passo que a amarela simbolizaria a casa de Habsburgo-Lorena, da qual fazia parte a imperatriz D. Leopoldina. Possivelmente, o verde teria sido escolhido para representar os Braganças em decorrência de ser essa a cor do dragão, figura heráldica associada a essa casa. O dragão, como divisa dinástica, seria ainda lembrado no cetro imperial, na guarda de honra e como ornamento de diferentes edifícios e objetos da família imperial. O verde também fora usado para representar os Braganças no estandarte pessoal de D. Pedro II de Portugal. Para alguns autores, Debret inspirou-se em estandartes regimentais do Primeiro Império Francês para criar os elementos pouco usuais da bandeira brasileira — um losango sobre o campo. À época, a França era referência cultural e política. Deve-se lembrar, ainda, que Debret e toda sua geração de artistas neoclássicos eram favorecidos por Napoleão Bonaparte e muitos preferiram sair do país após a queda do mesmo. A escolha do desenho e das cores, contudo, antecede a Independência do Brasil, pois já estavam presentes na bandeira projetada por Debret em 1820 a pedido de D. João VI. No desenho, o dragão aparece em lugar do laço nacional, unindo os ramos que suportam o brasão.
Dom Casmurro é um romance escrito por Machado de Assis, publicado em 1899 pela Livraria Garnier. Escrito para publicação em livro, o que ocorreu em 1900 – embora com data do ano anterior, ao contrário de Memórias Póstumas de Brás Cubas (1881) e Quincas Borba (1891), escritos antes em folhetins –, é considerado pela crítica o terceiro romance da "Trilogia Realista" de Machado de Assis, ao lado desses outros dois, embora o próprio autor não tenha formulado esta categoria. Seu protagonista é Bento Santiago, o narrador da história que, contada em primeira pessoa, pretende "atar as duas pontas da vida", ou seja, unir relatos desde sua mocidade até os dias em que está escrevendo o livro. Entre esses dois momentos, Bento escreve sobre suas reminiscências da juventude, sua vida no seminário, seu caso com Capitu e o ciúme que advém desse relacionamento, que se torna o enredo central da trama. Ambientado no Rio de Janeiro do Segundo Império, se inicia com um episódio que seria recente, no qual o narrador recebe a alcunha de "Dom Casmurro", daí o título do romance. Machado de Assis o escreveu utilizando ferramentas literárias como a ironia e a intertextualidade, fazendo referências a Schopenhauer e, sobretudo, à peça Otelo, o Mouro de Veneza de Shakespeare. Ao longo dos anos, Dom Casmurro, com seus temas do ciúme, a ambiguidade de Capitu, o retrato moral da época e o caráter do narrador, recebeu inúmeros estudos, adaptações para outras mídias e interpretações no mundo inteiro: desde psicológicas e psicanalíticas na crítica literária dos anos 30 e dos anos 40, passando pela crítica literária feminista na década de 1970, até sociológicas da década de 1980, e adiante. Creditado como um precursor do Modernismo e de ideias posteriormente escritas pelo criador da psicanálise Sigmund Freud, o livro influenciou os escritores John Barth, Graciliano Ramos e Dalton Trevisan e é considerado por alguns, disputando com Memórias Póstumas de Brás Cubas, como a obra-prima de Machado. Dom Casmurro foi traduzido para diversas línguas, continua a ser um de seus livros mais famosos e é considerado uma das obras mais fundamentais de toda a literatura brasileira.
Macunaíma, o herói sem nenhum caráter é um livro publicado em 1928 pelo polímata brasileiro Mário de Andrade, considerado a sua obra-prima. Escrito em pouco tempo mas fruto de pesquisas anteriores que o autor fazia sobre as origens e as especificidades da cultura e do povo brasileiro, narra a história do herói índio Macunaíma desde seu nascimento na selva até sua morte e transfiguração, uma trajetória movimentada e aventuresca em que é ajudado por seus irmãos e outros personagens, em busca de uma pedra mágica, o muiraquitã, que havia recebido de seu grande amor, Ci, a Mãe do Mato, mas que fora perdida e acabara em posse de Piaimã, um gigante comedor de gente que vivia como abastado burguês em São Paulo. A obra é de difícil classificação no sistema dos gêneros literários, sua estrutura tem elementos de muitos gêneros combinados, mas é muito elogiada como um experimento linguístico e literário extraordinariamente original e bem-sucedido, que esconde sua erudição na aparente facilidade com que integra modos de falar e elementos de crônicas, lendas, ditados e contos folclóricos de todo o Brasil em uma narrativa coerente, vigorosa, ágil e cativante. Também é admirada como uma penetrante reflexão sobre a cultura e sociedade brasileira, sua história, seu presente e seu destino. O mítico e o mágico fluem livremente ao lado da realidade concreta e muitas vezes a transfiguram, impregnando-a de novos significados. O protagonista é um personagem tão complexo, imprevisível e pouco definível quanto o formato da narrativa, alterna entre momentos de aguda perspicácia e estupidez, entre mansidão e brutalidade, entre grandeza e vilania; é, com efeito, um ser sobre-humano, dotado de poderes mágicos, e vive operando prodígios. Seu caráter e moralidade fora dos padrões, em particular, bem como sua sexualidade exuberante e irrefreada, têm sido objeto de intensa exploração crítica e debate. O livro hoje é largamente conhecido no Brasil, e seu protagonista saiu das suas páginas para ir viver no imaginário coletivo da nação, tornando-se um ícone popular. Na mídia e na cultura popular formou-se uma persistente imagem de Macunaíma como um retrato do "brasileiro médio" e seu modo de viver e entender o mundo, geralmente enfatizando traços negativos de preguiça, inconstância, libertinagem, covardia e pouca confiabilidade, mas para a maioria dos críticos recentes essa visão é um estereótipo pouco fiel à realidade. É verdade que a acidentada e excitante carreira do herói acaba em uma grande derrota: ele perde tudo ao que dava valor e tudo que dava sabor à sua existência, perde seus amores, sua família, seu império, e toda sua tribo se extingue; no final, solitário e desiludido, deixa o mundo e vai para o céu, transformando-se numa constelação. Porém, reivindicando a diversidade e miscigenação que marcaram a formação do país, normalizando o imprevisto, e incorporando a liberdade, a imaginação, a poesia e o maravilhoso ao cotidiano, ele tem sido entendido pelos pesquisadores muitas vezes como um "anti-herói", um símbolo da resistência ao colonialismo, à massificação, à homogeneização e à higienização étnica e cultural, aos preconceitos e discursos hegemônicos; um contraponto ao racionalismo frio e desumanizante, ao mundo das convenções, das regras fixas, dos horários rígidos e dos valores supostamente eternos e universais, cuja mensagem permanece viva e pertinente para o presente. A controvérsia, de fato, cercou a obra desde seu lançamento em 1928, surgindo em um momento em que os intelectuais modernistas procuravam tanto descobrir como redesenhar a "verdadeira" identidade nacional, trabalhando num contexto conservador, enfrentando a herança ainda muito viva do passado monárquico e colonial, e lutando para abrir um caminho legítimo e novo para um futuro que não discerniam com clareza. A volumosa bibliografia crítica que Macunaíma produziu e produz — sendo um dos livros brasileiros mais estudados de todos os tempos — é repleta de polêmicas sobre o seu significado, seus aspectos estéticos e suas implicações morais, culturais, políticas, históricas e sociais. Porém, a crítica o reconhece consensualmente como uma obra-prima e como um dos maiores marcos do Modernismo literário brasileiro, se não o maior. É obra muito estudada também por pesquisadores estrangeiros e foi traduzida para várias línguas. Tendo se tornado uma das obras canônicas da literatura brasileira, já foi adaptada para o cinema, os quadrinhos e o teatro, e serve como frequente referência em vários domínios artísticos e culturais.
Memórias Póstumas de Brás Cubas é um romance escrito por Machado de Assis, desenvolvido em princípio como folhetim, de março a dezembro de 1880, na Revista Brasileira, para, no ano seguinte, ser publicado como livro, pela então Tipografia Nacional como Memorias Posthumas de Braz Cubas. O livro tem como marcas um tom cáustico e novo estilo na obra de Machado de Assis, bem como audácia e inovação temática no cenário literário nacional, que o fez receber, à época, resenhas estranhadas. Confessando adotar a "forma livre" de Laurence Sterne em seu Tristram Shandy (1759–67), ou de Xavier de Maistre, em Memórias Póstumas rompe com a narração linear e objetivista de autores proeminentes da época, como Flaubert e Zola, para retratar o Rio de Janeiro e sua época em geral com pessimismo, ironia e indiferença — um dos fatores que fizeram com que fosse amplamente considerada a obra que iniciou o Realismo no Brasil, ainda que com elementos livres e próprios a Machado de Assis. Memórias Póstumas de Brás Cubas retrata a escravidão, as classes sociais, o cientificismo e o positivismo da época, chegando a criar, inclusive, uma nova filosofia — mais bem desenvolvida posteriormente em Quincas Borba (1891) — o Humanitismo, sátira à lei do mais forte do "darwinismo social". Críticos e escritores também atestam que, com esse romance, Machado de Assis antecipou e precedeu uma série de elementos de vanguarda que só seriam ampliados e assimilados no século seguinte, sejam do Modernismo e da Semana de 22, da Poesia Concreta, da literatura fantástica e do realismo mágico de autores como Jorge Luis Borges, Julio Cortázar e brasileiros, e, de fato, alguns chamam-na "primeira narrativa fantástica do Brasil", ainda que o fantástico seja apenas um traço de humor para retrarar a realidade sem pudores. O livro influencia e influenciou gerações de escritores como John Barth, Donald Barthelme e Ciro dos Anjos, e é notado como uma das obras mais revolucionárias e inovadoras da literatura brasileira. Mesmo depois de mais de um século de sua publicação original, ainda tem recebido inúmeros estudos e interpretações, adaptações para diversas mídias e traduções para outras línguas. Em 2020, por exemplo, uma nova tradução para o inglês esgotou em um dia após seu lançamento nos EUA.
A História de Brasília, a capital do Brasil, localizada no Distrito Federal, iniciou com as primeiras ideias de uma capital brasileira no centro do território nacional. A necessidade de interiorizar a capital do país parece ter sido sugerida pela primeira vez em meados do século XVIII, ou pelo Marquês de Pombal, ou pelo cartógrafo italiano a seu serviço Francesco Tosi Colombina. A ideia foi retomada pelos Inconfidentes, e foi reforçada logo após a chegada da corte portuguesa ao Rio de Janeiro em 1808, quando esta cidade era a capital do Brasil. A primeira menção ao nome de Brasília para a futura cidade apareceu em um folheto anônimo publicado em 1822, e desde então sucessivos projetos apareceram propondo a interiorização. A primeira Constituição da República, de 1891, fixou legalmente a região onde deveria ser instalada a futura capital, mas foi somente em 1956, com a eleição de Juscelino Kubitschek, que teve início a efetiva construção da cidade, inaugurada ainda incompleta em 21 de abril de 1960 após um apertado cronograma de trabalho, seguindo um plano urbanístico de Lúcio Costa e uma orientação arquitetural de Oscar Niemeyer. Os edifícios mais complexos, como os palácios e a catedral, seguiram os projetos estruturais elaborados pelo engenheiro Joaquim Cardozo. A partir desta data iniciou-se a transferência dos principais órgãos da administração federal para a nova capital, e na abertura da década de 1970 estava em pleno funcionamento. No desenrolar de sua curta história, Brasília, como capital nacional, testemunhou uma série de eventos importantes e foi palco de grandes manifestações populares. Planejada para receber 500 mil habitantes em 2000, segundo dados do IBGE ela nesta data possuía 2,05 milhões, sendo 1,96 milhões na área urbana e cerca de 90 mil na área rural. Este é apenas um dos paradoxos que colorem a história de Brasília. Concebida como um exemplo de ordem e eficiência urbana, como uma proposta de vida moderna e otimista, que deveria ser um modelo de convivência harmoniosa e integrada entre todas as classes, Brasília sofreu na prática importantes distorções e adaptações em sua proposta idealista primitiva, permitindo um crescimento desordenado e explosivo, segregando as classes baixas para a periferia e consagrando o Plano Piloto para o uso e habitação das elites, além de sua organização urbana não ter-se revelado tão convidativa para um convívio social espontâneo e familiar como imaginaram seus idealizadores, pelo menos para os primeiros de seus habitantes, que estavam habituados a tradições diferentes. Controversa desde o início, custou aos cofres públicos uma fortuna, jamais calculada exatamente, o que esteve provavelmente entre as causas das crises financeiras nacionais dos anos seguintes à sua construção. O projeto foi combatido como uma insensatez por muitos, e por muitos aplaudido como uma resposta visionária e grandiosa ao desafio da modernização brasileira. A construção de Brasília teve um impacto importante na integração do Centro-Oeste à vida econômica e social do Brasil, mas enfrentou e, como todas as grandes cidades, ainda enfrenta atualmente sérios problemas de habitação, emprego, saneamento, segurança e outros mais. Por outro lado, a despeito das polêmicas em seu redor, consolidou definitivamente sua função como capital e tornou-se o centro verdadeiro da vida na nação, e tornou-se também um ícone internacional a partir de sua consagração como Patrimônio da Humanidade em 1987, sendo reconhecida por muitos autores como um dos mais importantes projetos urbanístico-arquitetônicos da história.
Curitiba é um município brasileiro, capital do estado do Paraná, localizado a 934 metros de altitude no Primeiro Planalto Paranaense, a mais de 110 quilômetros do Oceano Atlântico, distante 1386 km a sul de Brasília, capital federal. Com 1.963.726 habitantes, é o município mais populoso do Paraná e da região Sul, além de ser o 8.º do país, segundo estimativa populacional calculada pelo IBGE para 2021. Fundado em 1693, a partir de um pequeno povoado bandeirante, Curitiba tornou-se uma importante parada comercial com a abertura da estrada tropeira entre Sorocaba e Viamão, vindo, em 1853, a ser a capital da recém-emancipada Província do Paraná. Desde então, a cidade, conhecida pelas suas ruas largas, manteve um ritmo de crescimento urbano fortalecido pela chegada de diversos imigrantes europeus ao longo do século XIX, na maioria, alemães, poloneses, ucranianos e italianos, que contribuíram para a atual diversidade cultural. Curitiba experimentou diversos planos urbanísticos e legislações que visavam controlar seu crescimento, que a levaram a ficar famosa internacionalmente pelas suas inovações urbanísticas e cuidado com o meio ambiente. A maior delas foi no transporte público, cujo sistema inspirou o TransMilenio, implantado em Bogotá, na Colômbia. Também conta com elevada posição nos indicadores de educação, a menor taxa de analfabetismo e a melhor qualidade no ensino básico entre as capitais. O Índice Mastercard de Mercados Emergentes 2008, criado com a intenção de avaliar e comparar o desempenho das cidades em diferentes funções que interligam os negócios e o comércio no mundo inteiro, posicionou-a como a 49ª com maior influência global. Curitiba foi classificada pelo Índice Verde de Cidades de 2015, realizado pela Siemens com a Economist Intelligence Unit, como a mais ambientalmente sustentável da América Latina. Também foi considerada pela Organização das Nações Unidas para a Educação, a Ciência e a Cultura (Unesco) uma das “cidades criativas” do Brasil em 2014, ao lado de Florianópolis. Em uma recente pesquisa publicada pela revista Forbes, Curitiba foi citada como a terceira cidade mais sagaz do mundo, que considera esperta a urbe que se preocupa, de forma conjunta, em ser ecologicamente sustentável, com qualidade de vida, boa infraestrutura e dinamismo econômico. Curitiba recebeu a classificação de cidade global gama- por parte do Globalization and World Cities Study Group & Network (GaWC). Entretanto, alguns problemas socioeconômicos persistem, em 2016 a capital paranaense foi classificada no 44.º lugar entre as 50 cidades com as maiores taxas de homicídio do mundo.
Piracicaba é um município brasileiro no interior do estado de São Paulo, Região Sudeste do país. É a principal cidade da Região Metropolitana de Piracicaba (RMP) e está situada a cerca de 150 km a noroeste da capital paulista. Ocupa uma área de pouco mais de 1378 km², sendo aproximadamente 169 km² em área urbana, e sua população em 2020 era de 407.252 habitantes, sendo assim o 16º município mais populoso do estado. Foi fundada em 1767, às margens do rio Piracicaba, manancial que foi vital para a região. No decorrer do século XIX, a agricultura se desenvolveu no município, com destaque para o cultivo da cana-de-açúcar e do café. Contudo, ainda na primeira metade do século XX, a cidade entrou em decadência. Com o fim do ciclo do café e a queda constante de preços do açúcar, a economia piracicabana se estagnou, o que viria a ser revertido somente a partir do início de sua industrialização. A cidade se tornou uma das primeiras a se industrializar no país, com a abertura de plantas fabris ligadas ao setor metal-mecânico e de equipamentos destinados à produção de açúcar. Esta atividade expandiu-se a partir da década de 1970 para o setor sucroalcooleiro, com a criação do programa Pró-álcool, voltado para a produção de álcool hidratado para uso automotivo, devido à crise mundial do petróleo em 1973. Isto contribuiu significativamente para o crescimento industrial de Piracicaba ao longo das décadas seguintes, chegando a ser o 52º maior PIB brasileiro em 2012, sendo sede de um dos principais centros industriais da região, além de contar com diversas universidades. Além da importância econômica, Piracicaba ainda é um importante centro cultural de sua região. O Horto Florestal de Tupi e o Balneário de Ártemis configuram-se como grandes áreas de preservação ambiental, enquanto que o Parque Professor Phillipe Westin e os parques situados às margens do rio Piracicaba são relevantes pontos de visitação localizados na zona urbana. O Salão Internacional de Humor de Piracicaba é considerado um dos mais importantes eventos sobre humor gráfico, realizado anualmente no Engenho Central. Este antigo engenho canavieiro foi tombado como patrimônio histórico e cultural, servindo hoje como espaço cultural, artístico e recreativo. A área do município é de 1378,069 km², sendo que desse total 168,824 km² estão em área urbana. De acordo com a divisão regional vigente desde 2017, instituída pelo IBGE, o município pertence às Regiões Geográficas Intermediária de Campinas e Imediata de Piracicaba. Situa-se a 22º43′30” de latitude sul e 47º38′56” de longitude oeste e está a uma distância de 152 quilômetros a noroeste da capital paulista. Limita-se com: Saltinho, Laranjal Paulista, Rio das Pedras e Tietê (a sul); Santa Bárbara d'Oeste (sudeste); Limeira e Iracemápolis (leste); Rio Claro (nordeste); São Pedro, Charqueada e Ipeúna (norte); Santa Maria da Serra (noroeste); Anhembi (oeste); e Conchas (sudoeste).
O rio Doce é um curso de água da Região Sudeste do Brasil, que banha os estados de Minas Gerais e Espírito Santo. Origina-se na confluência dos rios Piranga e do Carmo entre os municípios de Ponte Nova, Rio Doce e Santa Cruz do Escalvado, em Minas Gerais. Contudo, seu curso principal se inicia com a nascente do rio Xopotó, afluente do rio Piranga, em Desterro do Melo. A partir do encontro dos rios Piranga e do Carmo, o manancial percorre 853 km até a foz no oceano Atlântico, banhando 38 municípios. O encontro com o mar forma um estuário que está localizado na altura do povoado de Regência Augusta, integrante do município de Linhares, no litoral do Espírito Santo. A bacia do rio Doce abrange 86 mil km² de área de drenagem e quase 230 municípios, com uma população de mais de 3,6 milhões de habitantes, correspondendo à bacia mais importante totalmente inserida na Região Sudeste do Brasil. Foi descrito pela primeira vez pelos colonizadores europeus por André Gonçalves em 1501. Contudo, núcleos urbanos se consolidaram nas margens do curso hídrico somente no século XX, com a construção da Estrada de Ferro Vitória a Minas. A locação da ferrovia, que acompanha o rio Doce de Colatina ao Vale do Aço, deu início ao crescimento populacional ao permitir o desenvolvimento econômico. Trata-se de uma região com notável presença da agropecuária e da atividade industrial, sobretudo da mineração e da siderurgia. O potencial hidrelétrico do rio e seus afluentes é aproveitado pelas indústrias e pelo Sistema Interligado Nacional por meio de usinas hidrelétricas. Apesar de sua importância geográfica, social e econômica, o manancial sofre gravemente com o assoreamento, desmatamento, baixa cobertura por matas ciliares e recebimento de esgoto sem tratamento. Essa situação foi agravada pelo rompimento de barragem em Mariana em 2015, quando a lama de uma barragem de rejeitos pertencente à Samarco chegou ao rio Doce através do rio do Carmo. Dessa forma, todo o leito foi contaminado por metais usados na mineração. Esse foi o maior impacto ambiental da história brasileira, mas antes mesmo desse acidente o rio já se posicionava entre os dez mais poluídos do país.
Organização das Nações Unidas (ONU), ou simplesmente Nações Unidas, é uma organização intergovernamental criada para promover a cooperação internacional. Uma substituição à Liga das Nações, a organização foi estabelecida em 24 de outubro de 1945, após o término da Segunda Guerra Mundial, com a intenção de impedir outro conflito como aquele. Na altura de sua fundação, a ONU tinha 51 Estados-membros; desde 2011, são 193. A sua sede está localizada em Manhattan, Nova York, e possui extraterritorialidade. Outros escritórios situam-se em Genebra, Nairóbi e Viena. A organização é financiada com contribuições avaliadas e voluntárias dos países-membros. Os seus objetivos incluem manter a segurança e a paz mundial, promover os direitos humanos, auxiliar no desenvolvimento econômico e no progresso social, proteger o meio ambiente e prover ajuda humanitária em casos de fome, desastres naturais e conflitos armados. Durante a Segunda Guerra, o presidente estadunidense, Franklin D. Roosevelt, começou a discutir a criação de uma agência que sucederia a Liga das Nações, e a Carta das Nações Unidas foi elaborada em uma conferência em abril–junho de 1945; a carta entrou em vigor a 24 de outubro de 1945, e a ONU começou a operar. A sua missão de promover a paz foi complicada nas suas primeiras décadas de existência, por culpa da Guerra Fria, entre Estados Unidos, União Soviética e seus respectivos aliados. Teve participação em ações importantes na Coreia e no Congo-Léopoldville, além de ter aprovado a criação do estado de Israel em 1947. O número de integrantes cresceu bastante após o grande processo de descolonização na década de 1960, ocorrido principalmente na África, na Ásia e na Oceania, e, na década seguinte, seu orçamento para programas de desenvolvimento social e econômico ultrapassou em muito seus gastos com a manutenção da paz. Após o término da Guerra Fria, a ONU assumiu as principais missões militares e de paz ao redor do globo, com diferentes níveis de sucesso. A organização foi laureada com o Nobel da Paz em 2001, e alguns de seus oficiais e agências também ganharam o prêmio. Outras avaliações da eficácia da ONU são mistas. Alguns analistas afirmam que as Nações Unidas são uma força importante no que tange manter a paz e o estimular o desenvolvimento humano, enquanto outros adjetivam-na de ineficiente, corrupta ou tendenciosa. Seis órgãos principais compõem as Nações Unidas: a Assembleia Geral (assembleia deliberativa principal); o Conselho de Segurança (para decidir determinadas resoluções de paz e segurança); o Conselho Econômico e Social (para auxiliar na promoção da cooperação econômica e social internacional e desenvolvimento); o Conselho de Direitos Humanos (para promover e fiscalizar a proteção dos direitos humanos e propor tratados internacionais sobre esse tema); o Secretariado (para fornecimento de estudos, informações e facilidades necessárias para a ONU) e o Tribunal Internacional de Justiça (o órgão judicial principal). Além desses, há órgãos complementares de todas as outras agências do Sistema das Nações Unidas, como a Organização Mundial de Saúde (OMS), o Programa Alimentar Mundial (PAM) e o Fundo das Nações Unidas para a Infância (UNICEF). O cargo mais alto na ONU é o de secretário-geral, ocupado por António Guterres desde 2017.
A União Europeia (UE) é uma união económica e política de 27 Estados-membros independentes situados principalmente na Europa. A UE tem as suas origens na Comunidade Europeia do Carvão e do Aço (CECA) e na Comunidade Económica Europeia (CEE), formadas por seis países em 1957. Nos anos que se seguiram, o território da UE foi aumentando de dimensão através da adesão de novos Estados-membros, ao mesmo tempo que aumentava a sua esfera de influência através da inclusão de novas competências políticas. O Tratado de Maastricht instituiu a União Europeia com o nome atual em 1993. A última revisão significativa aos princípios constitucionais da UE, o Tratado de Lisboa, entrou em vigor em 2009. Bruxelas é a capital de facto da União Europeia. A UE atua através de um sistema de instituições supranacionais independentes e de decisões intergovernamentais negociadas entre os Estados-membros. As instituições da UE são a Comissão Europeia, o Parlamento Europeu, o Conselho da União Europeia, o Conselho Europeu, o Tribunal de Justiça da União Europeia, o Tribunal de Contas Europeu e o Banco Central Europeu. O Parlamento Europeu é o único órgão directamente eleito, a cada cinco anos, pelos cidadãos da UE. A UE instituiu um mercado comum através de um sistema harmonizado de leis aplicáveis a todos os Estados-membros. No Espaço Schengen (que inclui 22 Estados-membros e 4 estados não membros da UE) foram abolidos os controlos de passaporte. As políticas da UE têm por objetivo assegurar a livre circulação de pessoas, bens, serviços e capitais, legislar assuntos comuns na justiça e manter políticas comuns de comércio, agricultura, pesca e desenvolvimento regional. A Zona Euro, a união monetária, foi criada em 1999 e é atualmente composta por 19 Estados-membros. Através da Política Externa e de Segurança Comum, a UE exerce um papel nas relações externas e de defesa. A UE tem em todo o mundo missões diplomáticas permanentes, estando representada nas Nações Unidas, na Organização Mundial do Comércio (OMC), no G7 e no G-20. Com uma população total de aproximadamente 448 milhões de pessoas, o que representa 5,7% da população mundial, a UE gerou um produto interno bruto (PIB) de 12,2 mil milhões de euros em 2010, o que representa cerca de 20% do PIB global, medido em termos de paridade do poder de compra. Em 2012, a União Europeia foi laureada com o Nobel da Paz, entregue pelo Comité Nobel "por ter contribuído ao longo de mais de seis décadas para o avanço da paz e da reconciliação, democracia e direitos humanos na Europa". No anúncio do prémio, o Comité referiu que "o terrível sofrimento durante a Segunda Guerra Mundial provou a necessidade de uma nova Europa. (...) Hoje, uma guerra entre a França e a Alemanha é impensável. Isto mostra que, através da boa vontade e construção de confiança mútua, inimigos históricos podem transformar-se em aliados."
O Paraná é uma das 27 unidades federativas do Brasil. Está localizado ao norte da região Sul, da qual é o único estado a fazer divisa com outras regiões. Delimitado a leste pelo Oceano Atlântico, é dividido em 399 municípios e se limita com os estados de Mato Grosso do Sul (a NO), de São Paulo (ao N e a L) e de Santa Catarina (ao S), além da província argentina de Misiones (a SO) e os departamentos paraguaios de Canindeyú e Alto Paraná (a O). Sua área é de 199307,922 km², um pouco menor que a Romênia, país com formato semelhante. Com uma população de 10,4 milhões de habitantes, é o quinto estado mais populoso do Brasil. Sua capital é Curitiba. Outros municípios importantes são: Londrina, Maringá, Ponta Grossa, Guarapuava, Cascavel, São José dos Pinhais e Foz do Iguaçu. É o quarto estado mais rico do Brasil pelo PIB, atrás de SP, RJ e MG. Seu território, que abrange toda a extensão da antiga República do Guairá à época do Império Espanhol, era a província mais nova do Brasil imperial, desmembrada de São Paulo em 1853, sendo seu primeiro presidente Zacarias de Góis. Foi criada por motivos diversos, podendo ser citados um acordo pelo apoio oferecido pelos paranaenses à Revolução Farroupilha e o cultivo lucrativo da erva-mate. É também o mais novo estado da região Sul do país, logo depois do Rio Grande do Sul (1807) e Santa Catarina (1738). É historicamente conhecido por sua abundância de pinheirais espalhados pela porção do planalto sul, onde o clima é subtropical úmido temperado, semelhante aos estados de Santa Catarina e do Rio Grande do Sul, enquanto o restante do Brasil conta com o clima tropical. A espécie predominante na vegetação é a Araucaria angustifolia. Os ramos dessa árvore aparecem na bandeira e no brasão, símbolos adotados em 1947. Atualmente, esse ecossistema encontra-se muito destruído devido à ocupação humana. Já o relevo do Paraná é dos mais altos do Brasil: 52% do território se encontram além de seiscentos metros e somente 3% aquém de trezentos metros. Os rios mais importantes do Paraná são o Paraná, o Iguaçu, o Ivaí, o Tibagi, Paranapanema, o Itararé e o Piquiri.
Santa Catarina é uma das 27 unidades federativas do Brasil. Está situada no centro da região Sul do país. Possui como limites: ao norte, com o Paraná, ao sul, com o Rio Grande do Sul. A leste, com o Oceano Atlântico e a oeste, com a província argentina de Misiones. Compreende uma superfície de 95 736,165 km². Com uma população de 6,2 milhões de habitantes, é o décimo estado mais populoso do Brasil. Sua capital é Florianópolis, segunda cidade mais populosa do estado, após Joinville. Além do Espírito Santo, Santa Catarina constitui um dos dois estados cuja capital não é o município com maior número de habitantes. Conta com 295 municípios. Suas maiores cidades constituem Joinville, Florianópolis, Blumenau, Criciúma e Chapecó. Constitui um dos estados brasileiros com relevo mais montanhoso. 52% do território se encontram além de 600 metros. Uruguai, Canoas, Pelotas, Negro, do Peixe, Itajaí, Iguaçu, Chapecó e Tubarão constituem os rios mais importantes. Tem um clima subtropical úmido. Sua economia tem como bases os setores industrial (agroindustrial, têxtil, cerâmica, de máquinas e equipamentos), extrativista (mineral) e pecuarista. O território foi concedido a Pero Lopes de Sousa em 1534. Em 1675, Nossa Senhora do Desterro, hoje Florianópolis, na ilha de Santa Catarina, começou a ser criada por Francisco Dias Velho. Em 1739, a capitania foi emancipada de São Paulo e, dez anos depois, vieram os primeiros açorianos trazidos pelo governador Silva Pais, que concedeu enorme estímulo à região. Em 1777, foi invadida pelos espanhóis, no entanto, acabou sendo entregue no mesmo ano, como resultado do Tratado de Santo Ildefonso. Em 1829, foi criada a mais antiga colônia de imigrantes alemães. No período imperial, Santa Catarina foi palco de vários conflitos, principalmente da Revolução Farroupilha, que atingiu a província. Ao término do século XIX, o descobrimento do carvão mineral em suas terras concedeu enorme estímulo ao progresso no sul do estado. Para cá vieram novas multidões de imigrantes alemães e também de italianos, que se ocuparam das pequenas plantações e da vitivinicultura. Os índices sociais do estado estão entre os mais altos do Brasil. Tem o mais elevado índice de expectativa de vida do país (empatando com o Distrito Federal). Possui a menor taxa de mortalidade infantil e também é a unidade federativa com a mais baixa desigualdade econômica e analfabetismo do Brasil. Santa Catarina possui o sexto maior PIB do país, com uma economia variada e com fortes afinidades à industrialização. Importante polo de exportação e de consumo, é um dos estados que mais expandem na economia brasileira e que responde por 4% do produto interno bruto do país.
Brasil, oficialmente República Federativa do Brasil, é o maior país da América do Sul e da região da América Latina, sendo o quinto maior do mundo em área territorial (equivalente a 47,3% do território sul-americano), com 8 510 345,538 km², e o sexto em população (com mais de 213 milhões de habitantes). É o único país na América onde se fala majoritariamente a língua portuguesa e o maior país lusófono do planeta, além de ser uma das nações mais multiculturais e etnicamente diversas, em decorrência da forte imigração oriunda de variados locais do mundo. Sua atual Constituição, promulgada em 1988, concebe o Brasil como uma república federativa presidencialista, formada pela união dos 26 estados, do Distrito Federal e dos 5 570 municípios. Banhado pelo Oceano Atlântico, o Brasil tem um litoral de 7491 km e faz fronteira com todos os outros países sul-americanos, exceto Chile e Equador, sendo limitado a norte pela Venezuela, Guiana, Suriname e pelo departamento ultramarino francês da Guiana Francesa; a noroeste pela Colômbia; a oeste pela Bolívia e Peru; a sudoeste pela Argentina e Paraguai e ao sul pelo Uruguai. Vários arquipélagos formam parte do território brasileiro, como o Atol das Rocas, o Arquipélago de São Pedro e São Paulo, Fernando de Noronha (o único destes habitado por civis) e Trindade e Martim Vaz. O Brasil também é o lar de uma diversidade de animais selvagens, ecossistemas e de vastos recursos naturais em uma grande variedade de habitats protegidos. O território que atualmente forma o Brasil foi oficialmente descoberto pelos portugueses em 22 de abril de 1500, em expedição liderada por Pedro Álvares Cabral. Segundo alguns historiadores como Antonio de Herrera e Pietro d'Anghiera, o encontro do território teria sido três meses antes, em 26 de janeiro, pelo navegador espanhol Vicente Yáñez Pinzón, durante uma expedição sob seu comando. A região, então habitada por indígenas ameríndios divididos entre milhares de grupos étnicos e linguísticos diferentes, cabia a Portugal pelo Tratado de Tordesilhas, e tornou-se uma colônia do Império Português. O vínculo colonial foi rompido, de fato, quando em 1808 a capital do reino foi transferida de Lisboa para a cidade do Rio de Janeiro, depois de tropas francesas comandadas por Napoleão Bonaparte invadirem o território português. Em 1815, o Brasil se torna parte de um reino unido com Portugal. Dom Pedro I, o primeiro imperador, proclamou a independência política do país em 1822. Inicialmente independente como um império, período no qual foi uma monarquia constitucional parlamentarista, o Brasil tornou-se uma república em 1889, em razão de um golpe militar chefiado pelo marechal Deodoro da Fonseca (o primeiro presidente), embora uma legislatura bicameral, agora chamada de Congresso Nacional, já existisse desde a ratificação da primeira Constituição, em 1824. Desde o início do período republicano, a governança democrática foi interrompida por longos períodos de regimes autoritários, até um governo civil e eleito democraticamente assumir o poder em 1985, com o fim da ditadura militar. Como potência regional e média, a nação tem reconhecimento e influência internacional, sendo que também é classificada como uma potência global emergente e como uma potencial superpotência por vários analistas. O PIB nominal brasileiro foi o décimo segundo maior do mundo e o oitavo por paridade do poder de compra (PPC) em 2020. O país é um dos principais celeiros do planeta, sendo o maior produtor de café dos últimos 150 anos, além de ser classificado como uma economia de renda média-alta pelo Banco Mundial e como um país recentemente industrializado, que detém a maior parcela de riqueza global da América do Sul. No entanto, o país ainda mantém níveis notáveis de corrupção, criminalidade e desigualdade social. É membro fundador da Organização das Nações Unidas (ONU), G20, BRICS, Comunidade dos Países de Língua Portuguesa (CPLP), União Latina, Organização dos Estados Americanos (OEA), Organização dos Estados Ibero-americanos (OEI) e Mercado Comum do Sul (Mercosul).
A República Popular da China (RPC; chinês simplificado: 中华人民共和国; chinês tradicional: 中華人民共和國), também conhecida simplesmente como China, é o maior país da Ásia Oriental e o mais populoso do mundo, com mais de 1,38 bilhão de habitantes, quase um quinto da população da Terra. É uma república popular socialista unipartidária. Na constituição, descreve-se como um sistema multipartidário de cooperação e consulta política sob a liderança do Partido Comunista da China e como uma "ditadura democrática popular liderada pela classe trabalhadora e baseada na aliança de trabalhadores e camponeses". Tem jurisdição sobre vinte e duas províncias, cinco regiões autônomas (Xinjiang, Mongólia Interior, Tibete, Ningxia e Quancim), quatro municípios (Pequim, Tianjin, Xangai e Xunquim) e duas Regiões Administrativas Especiais com relativa autonomia (Hong Kong e Macau). A capital da RPC é Pequim. Com aproximadamente 9,6 milhões de quilômetros quadrados, a República Popular da China é o terceiro (ou quarto) maior país do mundo em área total e o terceiro maior em área terrestre. Sua paisagem é variada, com florestas de estepes e desertos (como os de Gobi e de Taclamacã) no norte seco e frio, próximo da Mongólia e da Sibéria (Rússia), e florestas subtropicais no sul úmido e quente, próximo ao Vietnã, Laos e Mianmar. O terreno do país, a oeste, é de alta altitude, com o Himalaia e as montanhas Tian Shan formando fronteiras naturais entre a China, a Índia e a Ásia Central. Em contraste, o litoral leste da China continental é de baixa altitude e tem uma longa faixa costeira de 14 500 quilômetros, delimitada a sudeste pelo Mar da China Meridional e a leste pelo Mar da China Oriental, além dos quais estão Taiwan, Coreia (Norte e Sul) e Japão. A nação tem uma longa história, composta por diversos períodos distintos. A civilização chinesa clássica — uma das mais antigas do mundo — floresceu na bacia fértil do rio Amarelo, na planície norte do país. O sistema político chinês era baseado em monarquias hereditárias, conhecidas como dinastias, que tiveram seu início com a semimitológica Xia (aproximadamente 2 000 a.C.) e terminaram com a queda dos Qing, em 1911. Desde 221 a.C., quando a dinastia Qin começou a conquistar vários reinos para formar um império único, o país expandiu-se, fraturou-se e reformulou-se várias vezes. A República da China, fundada em 1911 após a queda da dinastia Qing, governou o continente chinês até 1949. Em 1945, a república chinesa adquiriu Taiwan do Império do Japão, após o fim da Segunda Guerra Mundial. Na fase de 1946–1949 da Guerra Civil Chinesa, o Partido Comunista derrotou o nacionalista Kuomintang no continente e estabeleceu a República Popular da China, em Pequim, em 1 de outubro de 1949, enquanto o Partido Nacionalista mudou a sede do seu governo para Taipei. Desde então, a jurisdição da República da China está limitada à Taiwan e algumas ilhas periféricas (incluindo Penghu, Quemói e Matsu) e o país recebe reconhecimento diplomático limitado ao redor do mundo. Desde a introdução de reformas econômicas em 1978, a China tornou-se em uma das economias de mais rápido crescimento no mundo, sendo o maior exportador e o terceiro maior importador de mercadorias do planeta. A industrialização reduziu a sua taxa de pobreza de 53% (em 1981) para 8% (em 2001). O país tem sido considerado uma superpotência emergente por vários acadêmicos, analistas econômicos e militares. A importância da China como uma grande potência é refletida através de seu papel como segunda maior economia do mundo (ou segunda maior em poder de compra) e da sua posição como membro permanente do Conselho de Segurança da Organização das Nações Unidas e de várias outras organizações multilaterais, incluindo a Organização Mundial do Comércio, Cooperação Econômica Ásia–Pacífico, Grupo dos Vinte, BRICS e da Organização para Cooperação de Xangai. Além disso, o país é reconhecido como uma potência nuclear, além de possuir o maior exército do mundo em número de soldados e o segundo maior orçamento de defesa.
Leonel de Moura Brizola (nascido Leonel Itagiba de Moura Brizola; Carazinho, 22 de janeiro de 1922 – Rio de Janeiro, 21 de junho de 2004) foi um engenheiro civil e político brasileiro. Considerado um líder trabalhista, foi governador do Rio de Janeiro e do Rio Grande do Sul, sendo o único político eleito pelo povo para governar dois estados diferentes em toda a história do Brasil. Brizola nasceu em Carazinho, no distrito rural de São Bento, no Noroeste do Rio Grande do Sul. Viveu seus primeiros anos no interior do estado, mudando-se para Porto Alegre em 1936. Lá, deu prosseguimento aos seus estudos e trabalhou como engraxate, graxeiro, ascensorista e servidor público. Ingressou no curso de engenharia civil na Universidade Federal do Rio Grande do Sul em 1945, graduando-se em 1949. Enquanto ainda estudava na UFRGS, ingressou na política, ficando responsável por organizar a ala jovem do Partido Trabalhista Brasileiro. Em um evento político, conheceu Neusa Goulart, irmã do também político João Goulart, com a qual se casou em 1950 e teve três filhos. Em 1947, Brizola foi eleito deputado estadual pelo PTB. Tornou-se um político em ascensão no estado: em 1954, foi eleito deputado federal, com uma votação recorde; dois anos depois, elegeu-se prefeito de Porto Alegre; e, em 1958, governador do Rio Grande do Sul. Como governador, tornou-se proeminente por suas políticas sociais e por promover a Campanha da Legalidade, em defesa da democracia e da posse de Goulart como presidente. Em 1962, transferiu seu domicílio eleitoral para a Guanabara, estado pelo qual elegeu-se deputado federal. Durante o governo de Goulart, este e Brizola mantiveram uma relação tumultuada, mas uniram-se novamente antes do golpe militar de 1964. Depois que suas propostas de resistência não foram bem-sucedidas, Brizola exilou-se no Uruguai. Voltou ao Brasil em 1979, depois de um exílio de quinze anos no Uruguai, nos Estados Unidos e em Portugal. No mesmo ano, fundou e presidiu o Partido Democrático Trabalhista, um partido social-democrata e populista. Em 1982, foi eleito governador do Rio de Janeiro, iniciando um programa de construção dos Centros Integrados de Educação Pública. Na eleição presidencial de 1989, por pouco não foi para o segundo turno. Um ano depois, voltou a governar o Rio de Janeiro, sendo eleito no primeiro turno. A partir daí, não logrou êxito em nenhuma das quatro eleições que disputou, de vice-presidente de Lula da Silva em 1998 a senador pelo Rio de Janeiro em 2002. Faleceu, em 2004, vítima de um infarto agudo do miocárdio.
A partida entre Brasil e Alemanha pela semifinal da Copa do Mundo FIFA de 2014 foi disputada em 8 de julho de 2014 no Estádio Mineirão na cidade brasileira de Belo Horizonte. As duas seleções chegaram às semifinais com campanhas invictas na competição e esperava-se uma partida equilibrada, já que ambas as equipes eram forças tradicionais da Copa do Mundo FIFA, dividindo oito torneios conquistados e tendo se enfrentado anteriormente na final da edição de 2002, ocasião em que o Brasil venceu. A partida, no entanto, terminou com uma vitória de 7–1 para a Alemanha, que marcou quatro gols em um intervalo de seis minutos no primeiro tempo. O alemão Toni Kroos foi escolhido como o homem do jogo. O jogo estabeleceu vários recordes da Copa do Mundo FIFA. Esta foi a maior margem de vitória alcançada em uma semifinal, assim como os alemães ultrapassaram o Brasil como a equipe com maior pontuação na história do torneio e se tornaram o primeiro time a chegar em oito finais. Miroslav Klose marcou seu 16.º gol durante o jogo e superou o brasileiro Ronaldo como seu maior artilheiro de todos os tempos. Além disso, a derrota do Brasil quebrou sua série de 62 jogos invictos em casa em partidas competitivas, que perdurava desde a Copa América de 1975, e igualou a maior goleada que havia sofrido em uma partida, ao lado de uma derrota por 6–0 para o Uruguai em 1920, sendo superado ainda a marca de maior diferença de gols sofridos em uma partida internacional. O resultado foi descrito como uma humilhação nacional para o Brasil e o jogo foi posteriormente apelidado de "Mineiraço", evocando um "espírito de vergonha nacional" anterior, conhecido como "Maracanaço", no qual o país perdeu inesperadamente a Copa do Mundo de 1950 em casa, para o Uruguai. No Brasil, "7–1" tornou-se uma metáfora de uma derrota devastadora e esmagadora, sendo utilizado como motivo de chacota. Na continuação da Copa do Mundo de 2014, o Brasil perdeu por 3–0 para os Países Baixos na disputa do terceiro lugar, enquanto a Alemanha derrotou a Argentina por 1–0 na final para vencer a competição pela quarta vez. Antes desta partida, as seleções de Brasil e Alemanha haviam se enfrentado vinte e uma vezes, com doze vitórias para os brasileiros, quatro para os alemães e cinco empates. A única partida que disputaram em uma Copa do Mundo ocorreu na final da Copa do Mundo FIFA de 2002, realizada na Coreia do Sul e no Japão, onde os brasileiros foram campeões ao vencerem por 2–0, com ambos os gols marcados por Ronaldo. Naquela ocasião, a Seleção Brasileira também era dirigida por Luiz Felipe Scolari, e Miroslav Klose era um dos titulares da equipe alemã. A última partida oficial entre ambas as seleções aconteceu em 10 de agosto de 2011, em um amistoso internacional, onde a Alemanha venceu o Brasil por 3–2.
As Aventuras da Turma da Mônica é um filme de animação brasileiro de 1982 produzido pela Black & White & Color e distribuído pela Embrafilme. É o primeiro filme de longa-metragem baseado na série de histórias em quadrinhos da Turma da Mônica, criada pelo cartunista Mauricio de Sousa, que também dirigiu o filme, além de ser o quarto longa-metragem de animação da história do país. O enredo, escrito por ele e Reinaldo Waisman, gira em torno de quatro histórias apresentando os personagens principais da turma, interligadas pela aparição do próprio Mauricio em live-action. A Black & White & Color foi comprada pelo próprio Mauricio no início de 1980, para produzir comerciais envolvendo a Turma da Mônica. Após o sucesso de um curta-metragem exibido na TV Globo, fazendo-o ter interesse na realização de um longa, e uma reestruturação do estúdio, a produção do filme foi iniciada. Mauricio teve facilidade na criação da história, mas citou diversos problemas em sua produção, como a falta do material necessário no mercado brasileiro, tendo que recorrer a fornecedores internacionais. Segundo Mauricio, foram investidos cem milhões de cruzeiros na produção de As Aventuras da Turma da Mônica, o que seria o maior investimento para um filme no país. Parte deste valor foi bancado pela distribuidora Embrafilme, após uma reunião com seus diretores. O filme estreou primeiramente no dia 23 de dezembro de 1982 em São Paulo, e em janeiro do ano seguinte no Rio de Janeiro. Mais tarde, As Aventuras da Turma da Mônica foi relançado em VHS e exibido na televisão. Um disco contendo as músicas do filme foi lançado em 1983. As Aventuras da Turma da Mônica não teve muita atenção da crítica na época de seu lançamento, mas a recepção foi geralmente mista. Segundo dados da Agência Nacional do Cinema, teve um público de 1,1 milhão de espectadores, embora Mauricio defenda que o número foi maior. É o filme de animação brasileiro de maior público da história do país, e foi o filme de maior público da Turma da Mônica até ser superado pelo live-action Turma da Mônica: Laços (2019). Em 2017, As Aventuras da Turma da Mônica foi eleito pela Associação Brasileira de Críticos de Cinema (Abraccine) como a 12ª melhor animação brasileira da história.
Geografia do Brasil refere-se aos aspectos físicos naturais do país que, com mais de 8,5 milhões de quilômetros quadrados de extensão, é o quinto maior país do mundo. Localizado na América do Sul, seu relevo apresenta-se relativamente suave, composto por grandes bacias sedimentares, das quais destaca-se a bacia Amazônica, cercada por planaltos de altitudes moderadas. O ponto culminante do Brasil é o Pico da Neblina, com 2 994 m acima do nível do mar. De forma geral, a origem geológica do território é antiga, cujas formas de relevo suaves são resultados do contínuo intemperismo dos escudos cristalinos.Situado na zona tropical, ocorre o predomínio de climas quentes em boa parte do território, embora a pluviosidade varie desde regiões úmidas ao semiárido. No sul do Brasil, onde são registradas as menores temperaturas do país, o clima é subtropical. A abundância de chuvas em boa parte do território favorece a manutenção de uma das maiores redes hidrográficas do planeta, colocando o Brasil como principal detentor do potencial hídrico mundial. Grandes rios, como o Amazonas, São Francisco, Araguaia e Paraná, são os principais de suas grandes bacias de drenagem. Contudo, eventualmente o país enfrenta problemas dos extremos climáticos, como secas e inundações. O Brasil é banhado pelo oceano Atlântico a leste, em um litoral com trechos pouco recortados e com algumas centenas de ilhas costeiras. Somente cinco conjunto de ilhas estão mais afastadas da costa, dentre as quais o arquipélago de Fernando de Noronha e o Atol das Rocas. A diversidade climática propicia ainda a existência de uma rica biodiversidade, atestada pela alta densidade de espécies nos principais biomas. Destaca-se a Floresta Amazônica, no norte do país, onde um complexo equilíbrio ecológico proporciona a manutenção da maior floresta tropical do mundo. Ao longo do litoral, restam somente fragmentos da Mata Atlântica, desmatada desde o início da colonização. As plantas e animais do cerrado e da caatinga adaptaram-se aos períodos prolongados de seca que comumente atingem a faixa central do país. Abrangendo uma área territorial de 8.515.767,049 km² (com inclusão das águas internas), o Brasil é o país de maior extensão da América do Sul. É ainda o terceiro das Américas e o quinto do mundo: apenas a Rússia, o Canadá, a República Popular da China e os Estados Unidos (com Alasca e Havaí) são mais extensos. É muito grande que em seu território teriam cabido nações enormes como a Índia e a Austrália, e, por ser territorialmente muito extenso, o Brasil é tido como um país continental, ou seja, um país cujas dimensões geográficas são proporcionalmente continentais, sendo que seu território abrange 1,7% da área do globo terrestre, 5,7% das terras emersas da Terra, 20,8% da área das Américas e 47,3% do subcontinente sul-americano. Possivelmente é considerada tanto vantajosa quanto desvantajosamente a condição de país continental, no caso brasileiro. Por um lado, possui um grande espaço geográfico, com solos, climas e potencialidades produtivas diversificadas. Ao mesmo tempo, são encaradas dificuldades para a integração das populações habitantes desse território inteiro e para o atendimento às suas necessidades, o que, na sociedade, possivelmente é considerado como desvantagem. Mesmo assim, o espaço físico do Brasil é tido como único fora do comum, uma vez que é quase todo vantajoso, não tendo desertos, geleiras ou cordilheiras — as denominadas áreas anecúmenas, que não permitem que o território seja plenamente ocupado —, da mesma forma que acontece com a maioria dos países bem grandes da Terra. No Canadá, exemplificando, ocorrem certas áreas deste tipo, como a Ilha de Baffin e a Ilha Ellesmere, invadidas por geleiras; nos Estados Unidos, os desertos do Arizona e do Colorado. Mas as florestas equatoriais igualmente são tidas como áreas anecúmenas por geógrafos, porque quaisquer dos desastres naturais não permitem que a agricultura e a pecuária sejam econômica e vantajosamente implantadas, porque o índice de fertilidade dos solos é pequeno e uma vez que há uma grande quantidade de pragas e moléstias. Assim, somente a Amazônia é uma região onde há limitação no aproveitamento ocupacional, já que as condições da floresta equatorial amazônica são desvantajosas. O território brasileiro é cortado por dois círculos imaginários: a linha do equador, que passa pela embocadura do Amazonas, e o Trópico de Capricórnio, que corta o município de São Paulo. Assim, possui a maior parte do seu território situado no hemisfério sul (93%), na zona tropical (92%), a menor parte no hemisfério norte (7%) e a outra na zona temperada do sul (8%). Situa-se entre os paralelos 5°16'19" de latitude norte e 33°45'09" de latitude sul e entre os meridianos 34°45'54" de longitude leste e 73°59'32" de longitude oeste. Como o Brasil tem o formato aproximado de um gigantesco triângulo, é mais extenso no sentido leste-oeste do que no sentido norte-sul. Entretanto, como essas distâncias são quase iguais, costuma-se dizer que o Brasil é um país equidistante: a distância leste-oeste em linha reta alcança 4 328 km e norte-sul 4 320 km. O país ocupa uma vasta área ao longo da costa leste da América do Sul e inclui grande parte do interior do continente, compartilhando fronteiras terrestres com Uruguai ao sul; Argentina e Paraguai a sudoeste; Bolívia e Peru a oeste; Colômbia a noroeste e Venezuela, Suriname, Guiana e com o departamento ultramarino francês da Guiana Francesa ao norte. Compartilha uma fronteira comum com todos os países da América do Sul, exceto Equador e Chile, e engloba uma série de arquipélagos oceânicos, como Fernando de Noronha, Atol das Rocas, São Pedro e São Paulo e Trindade e Martim Vaz. Totalizam-se então 23.086 km de fronteira, sendo 15.791 km terrestres e 7.367 km marítimas. Os pontos extremos do território brasileiro são: Ao norte, a nascente do rio Ailã, no monte Caburaí, estado de Roraima (5° 16' de latitude norte), na fronteira com a Guyana; Ao sul, o Arroio Chuí, no Rio Grande do Sul (33° 45' de latitude sul), fronteira com o Uruguai; O extremo leste da parte continental do Brasil é a Ponta do Seixas, em João Pessoa, na Paraíba (34° 47' de longitude oeste); porém, os arquipélagos de Fernando de Noronha, Atol das Rocas, São Pedro e São Paulo e Trindade e Martim Vaz ficam ainda mais a leste, sendo o extremo leste absoluto do território brasileiro uma ponta sem nome na Ilha do Sul do arquipélago de Martim Vaz, a cerca de 28° 50' de longitude oeste; A oeste, a serra da Contamana ou do Divisor, no Acre (73° 59' de longitude oeste), na fronteira com o Peru.
São Francisco de Assis (batizado como Giovanni di Pietro di Bernardone; Assis, 1181 ou 1182 — 3 de outubro de 1226), foi um frade católico nascido na atual Itália. Depois de uma juventude irrequieta e mundana, voltou-se para uma vida religiosa de completa pobreza, fundando a ordem mendicante dos Frades Menores, mais conhecidos como Franciscanos, que renovaram o Catolicismo de seu tempo. Com o hábito da pregação itinerante, quando os religiosos de seu tempo costumavam fixar-se em mosteiros, e com sua crença de que o Evangelho devia ser seguido à risca, imitando-se a vida de Cristo, desenvolveu uma profunda identificação com os problemas de seus semelhantes e com a humanidade do próprio Cristo. Sua atitude foi original também quando afirmou a bondade e a maravilha da Criação num tempo em que o mundo era visto como essencialmente mau, quando se dedicou aos mais pobres dos pobres, e quando amou todas as criaturas chamando-as de irmãos. Alguns estudiosos afirmam que sua visão positiva da natureza e do homem, que impregnou a imaginação de toda a sociedade de sua época, foi uma das forças primeiras que levaram à formação da filosofia da Renascença. Dante Alighieri disse que ele foi uma "luz que brilhou sobre o mundo", e para muitos ele foi a maior figura do Cristianismo desde Jesus, mas a despeito do enorme prestígio de que ele desfruta até os dias de hoje nos círculos cristãos, que fez sua vida e mensagem serem envoltas em copiosa fé e darem origem a inumeráveis representações na arte, a pesquisa acadêmica moderna sugere que ainda há muito por elucidar quanto aos aspectos políticos de sua atuação, e que devem ser mais exploradas as conexões desses aspectos com o seu misticismo pessoal. Sua vida é reconstituída a partir de biografias escritas pouco após sua morte mas, segundo alguns estudiosos, essas fontes primitivas ainda estão à espera de edições críticas mais profundas e completas, pois apresentam contradições factuais e tendem a fazer uma apologia de seu caráter e obras; assim, deveriam ser analisadas sob uma óptica mais científica e mais isenta de apreciações emocionais do que tem ocorrido até agora, a fim de que sua verdadeira estatura como figura histórica e social, e não apenas religiosa, se esclareça. De qualquer forma, sua posição como um dos grandes santos da Cristandade se firmou enquanto ele ainda era vivo, e permanece inabalada. Foi canonizado pela Igreja Católica menos de dois anos após falecer, em 1228, e por seu apreço à natureza é mundialmente conhecido como o santo patrono dos animais e do meio ambiente.
A hepatite C é uma doença infecciosa causada pelo vírus da hepatite C (VHC) que afeta sobretudo o fígado. No momento da infeção inicial, é frequente que as pessoas não manifestem sintomas ou que manifestem apenas sintomas muito ligeiros. Ocasionalmente pode verificar-se febre, urina mais escura, dor abdominal ou icterícia. Após a infeção inicial, o vírus permanece no fígado de 75% – 85% das pessoas inicialmente infetadas, tornando-se uma infeção crónica. No início da fase crónica geralmente não se manifestam sintomas. No entanto, ao longo dos anos a infeção pode levar ao aparecimento de doenças hepáticas e ocasionalmente cirrose. Em alguns casos, os indivíduos com cirrose manifestam complicações como Insuficiência hepática ou cancro do fígado, podendo ainda ocorrer complicações que apresentam risco imediato de vida, como varizes esofágicas ou gástricas. O contágio com VHC é feito através de contacto sanguíneo, associado sobretudo com a partilha de seringas, uso de material médico mal esterilizado e transfusões de sangue. No entanto, com rastreio de sangue o risco de contágio por transfusão é inferior a um por cada dois milhões. A infeção pode também ser transmitida de mãe para filho durante o nascimento. Não se transmite por contacto superficial. o VHC é um dos cinco tipos conhecidos de vírus da hepatite: A, B, D e E. O diagnóstico é realizado com análises ao sangue que detetam a presença de anticorpos para o vírus ou ARN do vírus. O rastreio é recomendado para grupos de risco. Não existe vacina contra a hepatite C. As medidas de prevenção incluem não partilhar seringas e o rastreio do sangue doado. A infeção pode ser tratada com os medicamentos sofosbuvir ou simeprevir, cuja taxa de cura é de 90%. Antes destes tratamentos estarem disponíveis era usada uma combinação de peginterferão e ribavirina, que tinha uma taxa de cura de apenas 50% e maiores efeitos adversos. No entanto, o acesso aos novos medicamentos tem custos elevados. Os indivíduos que desenvolvem cirrose ou cancro do fígado podem necessitar de um transplante de fígado. A hepatite C é a principal causa de transplantes de fígado, embora o vírus normalmente se volte a manifestar mesmo depois do transplante. Estima-se que haja em todo o mundo 130 a 200 milhões de pessoas infetadas com hepatite C. Em 2013 houve cerca de 11 milhões de novos casos. A doença é mais comum em África e na Ásia oriental. No mesmo ano, a doença foi a causa de 343 000 mortes por cancro do fígado e 358 000 mortes por cirrose. A existência da hepatite C foi proposta durante a década de 1970 e demonstrada em 1989. A doença afecta apenas o ser humano e os chimpanzés. Infecção aguda. A infecção da hepatite C manifesta-se através de sintomas agudos em 15% dos casos. Os sintomas são geralmente pouco intensos e imprecisos, entre os quais a falta de apetite, fadiga, náuseas, dores musculares ou nas articulações e perda de peso. A maior parte dos casos de infecção aguda não tem relação com a icterícia. A infecção desaparece por si própria e de forma espontânea em 10 a 50% dos casos, o que ocorre com mais frequência em indivíduos jovens do sexo feminino. Infecção crónica. Cerca de 80% daqueles que são expostos ao vírus contraem uma infecção crónica. Durante as primeiras décadas, a maior parte dos infectados não nota qualquer sintoma significativo, embora a hepatite C crónica esteja associada à fadiga. Após vários anos, a doença torna-se a principal causa de cirrose e de cancro do fígado. Cerca de 10 a 30% dos indivíduos infectados desenvolve cirrose num prazo de 30 anos. A cirrose é mais frequente entre os que estão igualmente infectados com a hepatite B ou o VIH, entre alcoólicos e em indivíduos do sexo masculino. Os que desenvolvem cirrose apresentam um risco vinte vezes maior de contrair um hepatocarcinoma, ao ritmo de 1 a 3% ao ano, e no caso de estar associado ao excesso de álcool o risco aumenta para 100 vezes. A hepatite C está na origem de 27% dos casos de cirrose e de 25% dos casos de hepatocarcinoma em todo o mundo. A cirrose hepática pode levar à hipertensão portal, acumulação de líquidos no abdómen, susceptibilidade a hemorragias, varizes (sobretudo no estômago e esófago), icterícia, e um síndroma de alteração psíquica conhecido como encefalopatia hepática. É uma das causas mais comuns na requisição de transplantes hepáticos.
Proteína c-reactiva ou CRP (do inglês C-reactive protein) é uma proteína plasmática reagente de fase aguda produzida pelo fígado. É um dos membros da família de proteínas pentraxina. Sua função fisiológica é ligar-se à fosfocolina expressa na superfície de células mortas ou lesionadas (e alguns tipos de bactérias), para iniciar sua eliminação ao ativar o sistema complemento e células que fazem fagocitose(digerem outras células), funcionando como uma opsonina. É um indicador extremamente sensível de inflamação. Produção. A PCR é sintetizada pelo fígado, em resposta a factores libertados por macrófagos e células de gordura (adipócitos). É um membro da família de proteínas da pentraxina. Não deve ser confundida com o Péptido-C (resultado do processamento da pró-insulina) nem com a Proteina C (anticoagulante fisiológico quando em conjunção com a proteína S). Estrutura. O gene de PCR está localizado no primeiro cromossoma (1q21-q23). É um membro da família das pequenas pentraxinas. Tem 224 aminoácidos, com massa molecular de monômeros de 25106 Da, e tem uma forma discóide pentamérica anular. Função diagnóstica. O CRP liga-se à fosfocolina expressa na superfície de células mortas ou em processo de morte celular e algumas bactérias. Isso ativa o sistema do complemento, promovendo a fagocitose por macrófagos, que limpa as células necróticas e apoptóticas e as bactérias. Esta resposta de fase aguda ocorre como resultado de um aumento na concentração de IL-6, que é produzido por macrófagos, bem como adipócitos em resposta a uma ampla gama de condições inflamatórias agudas e crônicas, como bactérias, infecções virais ou fúngicas; doenças reumáticas e outras doenças inflamatórias; malignidade; e lesão tecidual e necrose. Essas condições causam a liberação de interleucina-6 e outras citocinas que desencadeiam a síntese de CRP e fibrinogênio pelo fígado.
Diabetes mellitus, ou simplesmente diabetes, é um grupo de doenças metabólicas em que se verificam níveis elevados de glicose no sangue durante um longo intervalo de tempo. Os sintomas da elevada quantidade de glicose incluem polaquiúria (necessidade frequente de urinar) e aumento da sede (polidipsia) e da fome (polifagia). Quando não é tratada, a diabetes pode causar várias complicações. Entre as complicações agudas estão a cetoacidose, coma hiperosmolar hiperglicémico ou morte. Entre as complicações a longo prazo estão doenças cardiovasculares, acidentes vasculares cerebrais, doença renal crónica, úlceras no pé e retinopatia diabética. A diabetes é o resultado quer da produção insuficiente de insulina pelo pâncreas, quer da resposta inadequada das células do corpo à insulina produzida. Existem três tipos principais de diabetes: A diabetes mellitus tipo 1 resulta da produção de quantidade insuficiente de insulina pelo pâncreas. Este tipo era anteriormente denominado "diabetes insulino-dependente". As causas são desconhecidas. A diabetes mellitus tipo 2 tem origem na resistência à insulina, uma condição em que as células do corpo não respondem à insulina de forma adequada. À medida que a doença avança, pode também desenvolver-se insuficiência na produção de insulina. Este tipo era anteriormente denominado "diabetes não insulino-dependente". A principal causa é peso excessivo e falta de exercício físico. A diabetes gestacional é a condição em que uma mulher sem diabetes apresenta níveis elevados de glicose no sangue durante a gravidez. Tanto a prevenção como o tratamento da diabetes consistem em manter uma dieta saudável, praticar regularmente exercício físico, manter um peso normal e abster-se de fumar. Em pessoas com a doença, é importante controlar a pressão arterial e manter a higiene dos pés. A diabetes do tipo 1 deve ser tratada com injeções regulares de insulina. A diabetes do tipo 2 pode ser tratada com medicamentos por via oral como metformina e glibenclamida, com ou sem insulina. Tanto a insulina como alguns medicamentos por via oral podem causar baixos níveis de glicose no sangue. Em pessoas obesas com diabetes do tipo 2, a cirurgia de redução do estômago pode ser uma medida eficaz. A diabetes gestacional geralmente resolve-se por si própria após o nascimento do bebé. No entanto, se não for tratada durante a gravidez pode ser a causa de várias complicações para a mãe e para o bebé. Estima-se que em 2015 cerca de 415 milhões de pessoas em todo o mundo tivessem diabetes. Cerca de 90% dos casos eram diabetes do tipo 2, o que corresponde a 8,3% da população adulta. A doença afeta em igual proporção mulheres e homens. Em 2014, a tendência sugeria que a prevalência continuaria a aumentar. A diabetes aumenta pelo menos duas vezes o risco de morte prematura. Entre 2012 e 2015, a diabetes foi a causa de entre 1,5 e 5 milhões de mortes anuais. Estima-se que em 2014 o custo económico global da doença tenha sido de 612 mil milhões de dólares.
Anatomia (do grego ἀνατέμνω anatemnō "cortar em partes") é o campo da biologia que estuda a organização estrutural dos seres vivos, incluindo os sistemas, órgãos e tecidos que os constituem, a aparência e posição das várias partes, as substâncias de que são feitos, a sua localização e a sua relação com outras partes do corpo. O termo anatomia é geralmente usado como sinónimo de anatomia humana. No entanto, as mesmas estruturas e tecidos podem ser observadas em praticamente todo o reino animal, pelo que o termo também se refere à anatomia dos outros animais, sendo neste caso por vezes usado o termo zootomia. Por outro lado, a estrutura e tecidos das plantas são de natureza diferente e são estudados pela anatomia vegetal. A anatomia distingue-se da fisiologia e da bioquímica, que estudam respetivamente as funções dessas partes e os processos químicos envolvidos. Está ligada à embriologia, à anatomia comparada, à biologia evolutiva e à filogenia, uma vez que são estes os processos que geram a anatomia. A disciplina da anatomia pode ser dividida em vários ramos, incluindo anatomia macroscópica e microscópica. A anatomia macroscópica é o estudo de estruturas anatómicas suficientemente grandes para poderem ser observadas a olho nu e engloba a anatomia de superfície. A anatomia microscópica é o estudo das estruturas a uma escala microscópica e engloba a histologia (o estudo dos tecidos) e a embriologia (o estudo de um organismo ainda imaturo). A história da anatomia caracteriza-se pela progressiva compreensão das funções dos órgãos e estruturas do corpo humano. Nos últimos séculos, os métodos de observação evoluíram de forma significativa, desde a dissecação de carcaças e cadáveres até às técnicas modernas de imagiologia médica, entre as quais radiografia, ecografia e ressonância magnética.
Fernando Henrique Cardoso, também conhecido como FHC (Rio de Janeiro, 18 de junho de 1931), é um professor, sociólogo, cientista político, escritor e político brasileiro. Filiado ao Partido da Social Democracia Brasileira (PSDB), foi o 34.º presidente da República Federativa do Brasil entre 1995 e 2003. Natural da cidade do Rio de Janeiro, mudou-se com sua família para a cidade de São Paulo, onde se casou em 1953 com a antropóloga e sua colega de faculdade Ruth Vilaça Correia Leite, com quem teve três filhos. FHC graduou-se em Sociologia pela Universidade de São Paulo e mais tarde tornou-se professor emérito daquela universidade. Foi perseguido depois do golpe militar de 1964, exilando-se no Chile e na França, voltando ao Brasil em 1968. Lecionou em universidades estrangeiras e desenvolveu uma importante carreira acadêmica, tendo produzido diversos estudos sociais premiados. FHC coordenou a elaboração da plataforma eleitoral do MDB. Em 1978, iniciou sua carreira política ao concorrer ao Senado Federal, elegendo-se suplente de Franco Montoro. Após a eleição deste para o governo de São Paulo, assumiu sua cadeira no Senado em março de 1983. Participou da campanha das Diretas Já, contribuindo para que não houvesse radicalização política durante a transição para a democracia. Foi derrotado por Jânio Quadros em 1985 para prefeito de São Paulo e reelegeu-se senador um ano depois. Tornou-se um dos principais líderes nacionais do PMDB e, juntamente com outros dissidentes do partido, ajudou a fundar o PSDB em 1988. Após o impeachment de Fernando Collor, foi nomeado por Itamar Franco como ministro das Relações Exteriores e ministro da Fazenda. Neste cargo, chefiou a elaboração do Plano Real, que estabilizou a economia. Com a ajuda do sucesso do plano, foi eleito Presidente da República no primeiro turno da eleição de 1994. Foi empossado presidente em 1.º de janeiro de 1995. Prosseguiu com as reformas econômicas iniciadas, as taxas de inflação continuaram baixas, houve a privatização de diversas empresas e a abertura de mercado, que deu maior visibilidade no mercado externo. O governo conseguiu a aprovação de leis na área econômica e administrativa, como a que permitiu a reeleição para cargos executivos. Em 1998, venceu a eleição presidencial no primeiro turno, tornando-se o primeiro presidente até então a ser reeleito. Durante o segundo mandato, crises internacionais, uma forte desvalorização do Real, a crise do apagão e outros acontecimentos causaram uma grande queda de sua popularidade. Atualmente preside a Fundação Fernando Henrique Cardoso, fundada por ele em 2004, e participa de diversos conselhos consultivos em diferentes órgãos no exterior, como o Clinton Global Initiative, Universidade Brown e United Nations Foundation. Também é membro do The Elders, da Academia Brasileira de Letras, e presidente de honra do PSDB.
Maria Osmarina da Silva Vaz de Lima (nascida Maria Osmarina da Silva; Rio Branco, 8 de fevereiro de 1958), mais conhecida por Marina Silva, é uma historiadora, professora, psicopedagoga, ambientalista e política brasileira, filiada à Rede Sustentabilidade (REDE). Ao longo de sua carreira política, exerceu os cargos de senadora pelo Acre entre 1995 e 2011 e de ministra do Meio Ambiente entre 2003 e 2008, além de candidatar-se em 2010, 2014 e 2018 à Presidência da República. Marina foi a candidata que compareceu a mais debates televisionados, totalizando 21 participações. Nascida em um seringal no Acre, Marina mudou-se para a capital do estado ainda na adolescência, onde foi alfabetizada. Concluído o ensino médio, graduou-se em história pela Universidade Federal do Acre. Desenvolveu interesse pela política e vinculou-se ao Partido Revolucionário Comunista, organização marxista que se abrigava no Partido dos Trabalhadores, posteriormente ajudando a fundar a Central Única dos Trabalhadores do Acre. Juntamente com Chico Mendes, ajudou a liderar o movimento sindical, elegendo-se para o seu primeiro cargo público, o de vereadora de Rio Branco, em 1988. Na eleição de 1990, Marina foi eleita deputada estadual e, em 1994, senadora da República, tornando-se, aos 36 anos de idade, a mais jovem senadora da história do país. Reeleita para o Senado em 2002, Marina aceitou o convite do presidente Luiz Inácio Lula da Silva e assumiu o Ministério do Meio Ambiente. Em 2009, deixou o PT e filiou-se ao Partido Verde (PV). Em 2010, Marina candidatou-se a presidente pelo PV, obtendo a terceira colocação no primeiro turno, com mais de 19 milhões de votos. Em 2014, assumiu a candidatura a presidente pelo Partido Socialista Brasileiro após a morte de Eduardo Campos, ficando novamente em terceira colocada com mais de 22 milhões de votos. Em 2015, conseguiu o registro de seu novo partido político, a Rede Sustentabilidade, o qual a escolheu para disputar pela terceira vez a presidência da República em 2018.
O futebol, também referido como futebol de campo, futebol de onze e, controversamente, futebol associado (em inglês: association football, football, soccer), é um desporto de equipe jogado entre dois times de 11 jogadores cada um e um árbitro que se ocupa da correta aplicação das normas. É considerado o desporto mais popular do mundo, pois cerca de 270 milhões de pessoas participam das suas várias competições. É jogado num campo retangular gramado, com uma baliza em cada lado do campo. O objetivo do jogo é deslocar uma bola através do campo para colocá-la dentro da baliza adversária, ação que se denomina golo (português europeu) ou gol (português brasileiro). A equipe que marca mais gols ao término da partida é a vencedora. O jogo moderno foi criado na Inglaterra com a formação da The Football Association, cujas regras de 1863 são a base do desporto na atualidade. O órgão regente do futebol é a Federação Internacional de Futebol (em francês: Fédération Internationale de Football Association), mais conhecida pela sigla FIFA. A principal competição internacional de futebol é a Copa do Mundo FIFA, realizada a cada quatro anos. Este evento é o mais famoso e com maior quantidade de espectadores do mundo, o dobro da audiência dos Jogos Olímpicos.As regras do assim chamado "football association" foram codificadas na Inglaterra pela FA no ano de 1863 e este nome foi cunhado para distinguir o jogo das outras formas de futebol jogados na época, como o rugby football e o gaelic football, por exemplo. A primeira "referência à bola inflada usada no jogo" foi em meados do Século XIV: "Þe heued fro þe body went, Als it was a foteballe". Já a palavra soccer, como uma abreviação de association apareceu pela primeira vez em escolas privadas e universidades inglesas na década de 1880 (às vezes usando a variante ortográfica "socker"). De acordo com o Online Etymology Dictionary, as "regras do soccer" foram feitas em 1848, antes da divisão do "football association" com o "rugby football" em 1863. O termo "soccer" vem de uma gíria ou abreviação jocular da palavra "association", com o sufixo "-er" anexado a ele. Esta palavra (que chegou à sua forma final em 1895) foi registrada pela primeira vez em 1889 na forma anterior de socca. A forma final "soccer" é creditada a Charles Wreford-Brown, um estudante da Universidade de Oxford que gostava de usar formas encurtadas de palavras, como "brekkers" para o café da manhã e "rugger" para o rugby. Clive Toye observou que ele pegou a terceira, quarta e quinta letras de Association e a chamou "soc-cer". O termo football association nunca foi amplamente utilizado, embora na Grã-Bretanha alguns clubes de futebol em redutos onde o rugby football é popular adotaram o sufixo Association Football Club (A.F.C.) para evitar confusão com o esporte dominante em sua área. Por quase cem anos depois de ser cunhado, a palavra "soccer" foi uma alternativa aceita e incontroversa na Grã-Bretanha se referir ao futebol, muitas vezes em contextos coloquiais e juvenis, mas também foi amplamente utilizado na fala formal e escrita sobre o jogo. "Soccer" era um termo usado pela classe alta, enquanto a classe trabalhadora e a classe média preferiam a palavra "football"; como a classe alta perdeu influência na sociedade britânica a partir da década de 1960, "football" suplantou "soccer" como a palavra mais usada e aceita. Há evidências de que o uso da palavra "soccer" está diminuindo na Grã-Bretanha, sendo considerado como um termo inglês americano. Dentro dos países de língua inglesa, o football association é atualmente chamado de "futebol" no Reino Unido e principalmente "soccer" no Canadá e nos Estados Unidos. Pessoas em países onde outros códigos de futebol são predominantes (Austrália, Irlanda, País de Gales, África do Sul e Nova Zelândia) podem usar ambos os termos, embora as associações nacionais na Austrália e Nova Zelândia usem principalmente "football" para o nome formal. "Fitbaa", "fitba" ou "fitbaw" é uma renderização da pronúncia escocesa de "football", muitas vezes usada em um contexto humorístico ou irônico. No resto do mundo, o "football association", em sua forma moderna, foi exportada pelos britânicos, e muitas das nações adotaram este termo inglês comum para o esporte em sua própria língua. Isso geralmente era feito de duas maneiras: seja importando diretamente a palavra em si, ou como calque traduzindo suas partes constituintes, "foot" e "ball", como, por exemplo, "balompié".
César Augusto Cielo Filho (Santa Bárbara d'Oeste, 10 de janeiro de 1987) é um nadador brasileiro. Atualmente, defende a equipe de natação do Clube Náutico Marcílio Dias. Foi o primeiro — e ainda único — nadador brasileiro a ser campeão olímpico, após conquistar o ouro nos 50 metros livre nos Jogos Olímpicos de 2008 em Pequim, e o atleta brasileiro mais medalhado em Campeonatos Mundiais de qualquer esporte, com 19 medalhas. Aparece, ainda, na segunda posição entre os nadadores brasileiros com maior número de medalhas conquistadas em campeonatos internacionais (33 no total) como Jogos Olímpicos, Campeonatos Mundiais de Piscinas Longa e Curta, Jogos Pan Americanos e Pan Pacíficos (atrás apenas de Gustavo Borges, que tem 35) e o maior medalhista brasileiro em mundiais de piscina curta, com 11 medalhas. Além da medalha de ouro nos Jogos Olímpicos de 2008 em Pequim, Cielo também conquistou mais duas medalhas em Olimpíadas, ambas de bronze, sendo uma nos 100 metros livre nos Jogos Olímpicos de 2008 e outra nos 50 metros livre nos Jogos Olímpicos de 2012. Foi ainda campeão mundial dos 100 metros livre em Roma 2009, e tricampeão mundial dos 50 metros livre em Roma 2009, Xangai 2011 e em Barcelona 2013, recordista mundial de ambas as provas. Ganhou três medalhas de ouro e uma medalha de prata nos Jogos Pan-Americanos de 2007 no Rio de Janeiro. Recordista mundial dos 50 e 100 metros livre em piscina olímpica e 4x50 metros medley em piscina curta, também detém os recordes brasileiro e sul-americano nos 4x100 metros livre e 4x100 metros medley em piscina olímpica, dos revezamentos 4x50 metros livre em piscina curta (25 metros) e longa (50 metros), e dos 4x200 metros livre em piscina curta. É medalha de ouro nos 50 metros e 100 metros livre do Grand Prix de Missouri, em 2008. Morou na cidade de Auburn, nos Estados Unidos, estudando e treinando na Universidade de Auburn. Foi considerado pela Revista Época um dos 100 brasileiros mais influentes do ano de 2009. Eleito melhor atleta ibero-americano do ano de 2009 e melhor atleta da década pela revista Sport Life. Seu desempenho nas piscinas também o vem levando a ser considerado, por parte da imprensa e de comentaristas esportivos, como o maior nadador da história da natação brasileira. Filho do pediatra Cesar Cielo e da professora de educação física Flávia Cielo, começou no Esporte Clube Barbarense (onde sua mãe dava aulas de natação), na cidade paulista de Santa Bárbara d'Oeste, onde nasceu. Posteriormente, nadou pelo Clube de Campo de Piracicaba. Em 2003 transferiu-se para o Esporte Clube Pinheiros, onde treinou por dois anos ao lado de Gustavo Borges, inclusive recebendo do mesmo, como presente, o maiô utilizado por ele nas Olimpíadas de Atenas 2004. Participou de seu primeiro torneio internacional importante no Campeonato Mundial de Natação em Piscina Curta de 2004, ocorrido na cidade de Indianápolis, em outubro de 2004. Aos 17 anos, Cielo ganhou a medalha de prata no revezamento 4x100 metros livre. Também ficou em décimo lugar nos 50 metros livre, em sexto lugar nos 100 metros livre, em quarto lugar no revezamento 4x100 metros medley e em 19º nos 50 metros costas. Em 2006, transferiu-se para Auburn, nos Estados Unidos, onde ganhou uma bolsa de estudos para defender o time de natação da universidade. Para os estudos acadêmicos, o nadador barbarense escolheu comércio exterior, com especialização em espanhol. Lá, foi treinado por Brett Hawke, ex-nadador australiano finalista olímpico, que fez os últimos meses de preparação do brasileiro para a Olimpíada de Pequim em 2008. Seu contrato de bolsa era muito rigoroso, proibindo ao atleta desde ter namoradas até de sair à noite e de beber. Participou, no início de abril, do Campeonato Mundial de Natação em Piscina Curta de 2006 onde ficou em quinto lugar nos 100 metros livre, no 4x100m livre e no 4x200m livre. Em dezembro, começou a se destacar no cenário nacional, ao quebrar o recorde sul-americano de Fernando Scherer nos 100 metros livre, de 48s69, que perdurava desde 1998. À época, fez o tempo de 48s61. Foi finalista no 12º Mundial de Esportes Aquáticos de Melbourne 2007 nos 100 metros livre (quarto lugar), 50 metros livre (sexto lugar) e no 4x100 metros livre (oitavo lugar), além de ter obtido o nono lugar nos 4x100 metros medley. Nessa ocasião, se consolidou como maior velocista brasileiro na natação, ao quebrar o recorde sul-americano dos 50 metros livre de Fernando Scherer. Na semifinal da prova, Cielo fez a marca de 22s09, melhorando o tempo de Xuxa, que era de agosto de 1998, em nove centésimos. Posteriormente, ajudou os revezamentos 4x100 metros livre e 4x100 metros medley brasileiros a se classificarem para as Olimpíadas de 2008. Ganhou três medalhas de ouro e uma medalha de prata nos Jogos Pan-Americanos de 2007 no Rio e, na ocasião, foi o primeiro nadador da América do Sul a nadar os 50 metros livre abaixo de 22 segundos. Nos Jogos Olímpicos de Verão de 2008 em Pequim ganhou a primeira medalha de bronze na prova dos 100 metros livre, ficando atrás somente do francês Alain Bernard e do australiano Eamon Sullivan, batendo o recorde sul-americano. Jason Lezak fez o mesmo tempo do nadador barbarense e também ganhou a medalha de bronze. Na outra prova que participou, na semifinal dos 50 metros livre, o brasileiro quebrou o recorde olímpico com o tempo de 21s34, que pertencia a Alexander Popov desde as Olimpíadas de Barcelona em 1992. Na final da prova dos 50 metros livre, ganhou a medalha de ouro, quebrando novamente o recorde olímpico com o tempo de 21s30, ficando a dois centésimos do recorde mundial (21s28), e se tornou o primeiro brasileiro campeão olímpico na natação. Até a medalha de ouro de Cesar Cielo, os melhores resultados da natação do Brasil haviam sido obtidos por Ricardo Prado, que ganhou a medalha de prata nos 400 metros medley nos Jogos Olímpicos de Los Angeles, em 1984, e por Gustavo Borges, que ganhou a medalha de prata nos 100 metros livre nos Jogos Olímpicos de Barcelona, em 1992, e a medalha de prata nos 200 metros livre nos Jogos Olímpicos de Atlanta, em 1996.
Aparecida é um município brasileiro no interior do estado de São Paulo, Região Sudeste do país. Localiza-se no Vale do Paraíba Paulista, a nordeste da capital do estado, distando desta cerca de 170 km. Ocupa uma área de 121,076 km², sendo que 5,7 km² estão em perímetro urbano, e sua população em 2019 era de 36 157 habitantes. A cidade é popularmente denominada Aparecida do Norte em razão da construção da Estrada de Ferro do Norte (depois Estrada de Ferro Central do Brasil) na segunda metade do século XIX. As origens do município remontam à fé consolidada ao redor do encontro da imagem de Nossa Senhora da Conceição Aparecida no curso do Rio Paraíba do Sul por pescadores, em 1717. Os milagres atribuídos à representação levaram à construção de uma capela, em 1745, ao redor da qual se estabeleceram vários fiéis e os primeiros residentes. Dado o desenvolvimento da localidade, em 1842 foi criada a freguesia, subordinada a Guaratinguetá, município do qual Aparecida se desmembrou em 17 de dezembro de 1928. O número crescente de fiéis implicou na construção de um templo maior, a atual Basílica Velha, que foi inaugurada em 1888 e substituída pela nova Basílica de Nossa Senhora Aparecida na segunda metade do século XX. Esta configura-se como o maior centro de peregrinação religiosa da América Latina, recebendo anualmente milhões de visitantes, os quais fazem do município um dos principais núcleos turísticos do Brasil. O complexo turístico aparecidense abrange ainda marcos como o Porto Itaguaçu, que marca o local onde a imagem de Nossa Senhora foi encontrada; o Seminário Missionário Bom Jesus; o Morro do Cruzeiro, com suas esculturas que representam a Via Sacra; e a Igreja São Benedito, inaugurada em 1918.
Clindamicina é um antibiótico usado para o tratamento de uma série de infecções bacterianas, incluindo infecções ósseas ou articulares, doença inflamatória pélvica, faringite estreptocócica, pneumonia, otite média e endocardite. Pode também ser usada no tratamento do acne e de alguns casos de Staphylococcus aureus resistente à meticilina (MRSA). Em associação com quinina, pode ser usada no tratamento de malária. Está disponível para administração por via oral, por injecção intravenosa e em pomada para aplicação na pele ou na vagina. Os efeitos adversos mais comuns incluem náuseas, diarreia, erupção cutânea e dor no local da injecção. A clindamicina aumenta em cerca de quatro vezes o risco de contrair colite pseudomembranosa em ambiente hospitalar, pelo que só é recomendada quando outros antibióticos não são adequados. Aparenta ser geralmente segura na gravidez. Pertence à classe das lincosamidas e atua no organismo bloqueando a produção de proteínas pelas bactérias. A clindamicina foi produzida pela primeira vez em 1966 a partir da lincomicina. Faz parte da Lista de Medicamentos Essenciais da Organização Mundial de Saúde e está disponível como medicamento genérico. Em 2017 foi o 137º medicamento mais receitado nos Estados Unidos, com mais de quatro milhões de receitas.
O Memorial da América Latina é um centro cultural, político e de lazer, inaugurado em 18 de março de 1989 na cidade de São Paulo, Brasil. O conjunto arquitetônico, projetado por Oscar Niemeyer, é um monumento à integração cultural, política, econômica e social da América Latina, situado em um terreno de 84 482 metros quadrados no bairro da Barra Funda. Seu projeto cultural foi desenvolvido pelo antropólogo Darcy Ribeiro. É uma fundação de direito público estadual, com autonomia financeira e administrativa, vinculada à Secretaria de Estado da Cultura. O complexo é constituído por vários edifícios dispostos ao longo de duas áreas unidas por uma passarela, que somam ao todo 25.210 metros quadrados de área construída: o Salão de Atos, a Biblioteca Latino-Americana, a Galeria Marta Traba, o Pavilhão da Criatividade, o Anexo dos Congressistas e o Auditório Simón Bolívar — que sofreu um incêndio em novembro de 2013. Na Praça Cívica, encontra-se a escultura em concreto, também de Niemeyer, representando uma mão aberta, em posição vertical, com o mapa da América Latina pintado em vermelho na palma, simbolizando uma mancha de sangue, em referência aos episódios turbulentos do passado e aos problemas sociais da região. O Memorial possui um acervo permanente de obras de arte, exibidas ao longo da esplanada e nos espaços internos, e conta com um centro de documentação de arte popular latino-americana. A biblioteca possui cerca de 30 mil volumes, além de seção de música e imagens. O complexo promove exposições, palestras, debates, sessões de vídeo, espetáculos de teatro, música e dança. Mantém o Centro Brasileiro de Estudos da América Latina, organização de fomento a pesquisas acadêmicas sobre assuntos latino-americanos. Publica regularmente a revista Nossa América e livros variados. Serviu de sede ao Parlamento Latino-Americano entre 1989 e 2007 (atualmente localizado na cidade do Panamá).
A Caixa Cultural São Paulo é uma da unidades da rede Caixa Cultural, um complexo de centros culturais localizados em diferentes capitais do Brasil, mantidos pela Caixa Econômica Federal. Em funcionamento desde 1989, a unidade de São Paulo está instalada no Edifício Sé, antiga sede regional do banco, inaugurada em 1939 por Getúlio Vargas. O edifício é um importante exemplar da arquitetura art déco na cidade e é tombado pelo patrimônio municipal. A Caixa Cultural administra ainda um segundo espaço em São Paulo, a Galeria Vitrine da Paulista, no andar térreo do Conjunto Nacional. A instituição abriga o Museu da Caixa, com diversos objetos e documentos referentes à história da Caixa Econômica Federal e ao sistema financeiro do Brasil, além de espaços internos com ambientação de época e elementos originais conservados. Abriga ainda diferentes espaços expositivos, sala de leitura, sala de oficinas e um auditório. Mantém uma programação permanente de eventos culturais, incluindo espetáculos de dança, teatro, shows, debates, sessões de vídeo e cinema, leituras dramáticas, palestras, além de exposições temporárias, sobretudo no campo das artes visuais. A instituição. A rede Caixa Cultural foi criada em 1980, sob a denominação inicial de "Conjunto Cultural da Caixa", visando promover, divulgar e apoiar eventos, projetos e manifestações artísticas e culturais, nos campos das artes visuais, fotografia, teatro, dança e literatura, além de preservar o acervo da Caixa Econômica Federal, especializado na memória da instituição e na história do sistema financeiro do Brasil. A primeira unidade do Conjunto Cultural da Caixa foi aberta ao público ainda em 1980, em Brasília. A unidade de São Paulo é a segunda mais antiga da rede, tendo sido inaugurada em 29 de agosto de 1989. Foi instalada no Edifício Sé, prédio histórico localizado na Praça da Sé, tombado pelo Conpresp, erguido na segunda metade dos anos trinta para ser a sede estadual do banco — e em desuso desde o fim da década de setenta, quando a sede foi transferida para o Edifício Torre Sul, na Avenida Paulista. O espaço expositivo foi instalado no andar térreo e o sexto andar foi reservado ao Museu da Caixa, com o acervo histórico da instituição. O edifício ainda abriga alguns escritórios administrativos da Caixa, além da agência bancária Sé.
Prêmio (português brasileiro) ou Prémio (português europeu) Nobel (em sueco: Nobelpriset; em norueguês: Nobelprisen) é um conjunto de seis prêmios internacionais anuais concedidos em várias categorias por instituições suecas e norueguesas, para reconhecer pessoas ou instituições que realizaram pesquisas, descobertas ou contribuições notáveis para a humanidade no ano imediatamente anterior ou no curso de suas atividades. O último desejo do cientista sueco Alfred Nobel estabeleceu os prêmios em 1895. Os prêmios em Química, Literatura, Paz, Física e Fisiologia ou Medicina foram concedidos pela primeira vez em 1901. Em 1968, o Sveriges Riksbank estabeleceu o Prêmio de Ciências Econômicas em Memória de Alfred Nobel, que, embora não seja um Prêmio Nobel, tem sido comumente conhecido como o Prêmio Nobel de Economia. O Prêmio Nobel é amplamente considerado como o mais prestigiado prêmio disponível nas áreas de literatura, medicina, física, química, economia e ativismo pela paz. A Academia Real das Ciências da Suécia concede o Prêmio Nobel de Física, o Prêmio Nobel de Química e o Prêmio de Ciências Econômicas em Memória de Alfred Nobel; a Assembleia do Nobel do Instituto Karolinska concede o Prêmio Nobel de Fisiologia ou Medicina; a Academia Sueca concede o Prêmio Nobel de Literatura; e o Prêmio Nobel da Paz não é concedido por uma organização sueca, mas pelo Comitê Norueguês do Nobel. Entre 1901 e 2021, o Prêmio Nobel e o Prêmio de Ciências Económicas em Memória de Alfred Nobel foram concedidos 609 vezes a 975 pessoas e organizações. Com algumas pessoas recebendo o Nobel mais de uma vez, isso perfaz um total de 943 indivíduos e 25 organizações. As cerimônias de premiação acontecem anualmente em Estocolmo, na Suécia (com exceção do prêmio da paz, que acontece em Oslo, na Noruega). Cada recebedor ou laureado recebe uma medalha de ouro, um diploma e uma quantia em dinheiro que é decidida pela Fundação Nobel (o prêmio equivalia a 1,1 milhão de dólares em 2017).) Medalhas feitas antes de 1980 foram feitas em ouro de 23 quilates e depois em ouro verde de 18 quilates com revestimento de ouro 24 quilates. O prêmio não é concedido postumamente; no entanto, se uma pessoa receber um prêmio e morrer antes de recebê-lo, o prêmio ainda poderá ser apresentado. Embora o número médio de laureados por prêmio tenha aumentado substancialmente durante o século XX, um prêmio não pode ser dividido entre mais de três pessoas, embora o Prêmio Nobel da Paz possa ser concedido a organizações de mais de três pessoas.
Queimadura é uma lesão na pele ou noutros tecidos causada por calor, eletricidade, substâncias químicas, atrito ou radiação. A maior parte das queimaduras são causadas pelo contacto com o fogo ou com líquidos e objetos muito quentes. Entre os principais fatores de risco estão a falta de segurança ao cozinhar com fogo, a falta de segurança em locais de trabalho perigosos, o alcoolismo e fumar. As queimaduras podem ainda ocorrer como resultado de episódios de automutilação ou violência entre pessoas. As queimaduras que afetam apenas a camada superficial da pele são denominadas superficiais ou de primeiro grau. As queimaduras de primeiro grau causam apenas vermelhidão sem bolhas e dor durante cerca de três dias. Quando as lesões afetam também algumas das camadas inferiores da pele são denominadas queimaduras de segundo grau ou de profundidade parcial. Nas queimaduras de segundo grau as bolhas estão quase sempre presentes e são bastante dolorosas. A cicatrização pode levar até oito semanas, deixando marcas permanentes. Quando todas as camadas de pele são afetadas denominam-se queimaduras de terceiro grau. As queimaduras de terceiro grau são indolores e a área queimada apresenta-se rígida. Na maior parte dos casos é necessária intervenção médica para que os tecidos cicatrizem. Quando existem lesões em tecidos mais profundos, como os músculos, tendões ou os ossos, denominam-se queimaduras de quarto grau. As queimaduras de quarto grau são geralmente de cor preta e resultam na perda da parte queimada. A maior parte das queimaduras pode ser prevenida. O tratamento necessário depende da gravidade da queimadura. As queimaduras superficiais podem ser tratadas apenas com analgésicos, enquanto que as de maior gravidade requerem internamento prolongado em unidades hospitalares especializadas. Arrefecer a lesão com água fresca corrente pode aliviar a dor e diminuir a extensão dos danos; no entanto, a exposição prolongada ao frio pode provocar hipotermia. As queimaduras de segundo grau podem ser limpas com água e sabão, aplicando depois um curativo. Não é ainda evidente qual a melhor forma de tratamento das bolhas, embora seja razoável manter as bolhas pequenas intactas e drenar as maiores. As queimaduras de terceiro grau geralmente necessitam de tratamentos cirúrgicos, como enxertos de pele. As queimaduras extensas geralmente requerem hidratação intravenosa, uma vez que a resposta inflamatória posterior causa edema e perda de líquidos capilares. As complicações mais comuns das queimaduras estão relacionadas com infeções. Nos casos em que pessoa queimada não tem as vacinas em dia, é necessário administrar uma vacina contra o tétano. Em 2015, o fogo e o calor foram a causa de 67 milhões de ferimentos em todo o mundo, que resultaram em 2,9 milhões de admissões hospitalares e 176 000 mortes. A maior parte das mortes por queimaduras ocorre nos países em vias de desenvolvimento, sobretudo no sudeste asiático. Embora as queimaduras de grande extensão possam ser fatais, os tratamentos modernos desenvolvidos a partir da década de 1960 melhoraram substancialmente o prognóstico, sobretudo em crianças e jovens adultos. O prognóstico a longo prazo depende essencialmente da extensão da queimadura e da idade da pessoa afetada.
Um eletrocardiograma (identificado com as abreviações ECG e EKG) é a reprodução gráfica da atividade elétrica do coração durante o seu funcionamento, registada a partir da superfície do corpo. A superfície do corpo humano e dos animais emanam correntes elétricas de baixa intensidade, que no indivíduo em repouso são identificadas nas contínuas despolarização e repolarização do coração. A isto se correlaciona toda a atividade elétrica que é registada ao nível do tronco. Os potenciais elétricos produzidos pelo músculo cardíaco são, neste contexto, o somatório das correntes elétricas geradas pelas células musculares cardíacas individuais. Tais correntes são registadas por meio de um aparelho chamado eletrocardiógrafo, um dispositivo que foi modificado e aperfeiçoado por Willem Einthoven e Étienne-Jules Marey em 1903, e que deriva diretamente dum galvanômetro de corda. Muitas das convenções estabelecidas por Einthoven subsistem nos dias atuais e constituem a base para vários aspectos do ECG moderno. Funcionamento básico. Graças à conversão da energia elétrica em energia mecânica, as variações elétricas produzem o movimento de um mecanismo ou sistema de agulhas". A energia é devidamente amplificada de forma a ser possível transcrever amplitudes que permitam a gravação de um sinal legível. As deflexões são impressas em papel, que se move a velocidade constante em contato com o sistema que reporta no papel as ondas que são registadas em função do tempo. Simultaneamente com a oscilação vertical das linhas produzidas a partir de variações do potencial, o papel se movimenta para a esquerda. Esta sincronização permite reportar o movimento vertical sobre um plano horizontal, gravando as oscilações consoante o tempo. No ECG com 12 derivações, são colocados quatro eletrodos sobre os membros do paciente, e seis sobre o tórax. A magnitude total do potencial elétrico do coração é, então, medida a partir de doze ângulos diferentes, chamadas também de "derivações", e é registada por determinado período de tempo (normalmente de dez segundos). Desta forma, a amplitude e a direção geral da despolarização elétrica do coração são capturados a qualquer momento e ao longo de todo o ciclo cardíaco. O eletrocardiograma é um exame de base simples e seguro, utilizado em numerosos contextos clínicos. Dentre os tipos mais reportados, estão a medição da frequência e ritmo dos batimentos cardíacos, além de verificar a dimensão e a posição das câmaras cardíacas (tal como ocorre na dextrocardia). Além disso, visam também detectar a presença de possíveis danos ao miocárdio ou ao sistema de condução, controlar os efeitos induzidos pelos medicamentos, bem como verificar o bom funcionamento de um marca-passo.
Espirometria é um exame dos pulmões, também conhecido como Prova de Função Pulmonar, Prova Ventilatória ou Exame do sopro. A espirometria permite o registro de vários volumes e dos fluxos de ar. A palavra espirometria vem do latim spirare = respirar + metrum = medida. O termo foi criado em 1789 quando cientistas investigavam uma forma de aferir o volume de oxigênio utilizado na respiração. Em linhas gerais, a espirometria mede a velocidade e a quantidade de ar que um indivíduo é capaz de colocar para dentro e para fora dos pulmões. Trata-se de um exame não invasivo e indolor, porém em raras situações de potencial risco para o paciente devido às manobras forçadas ou quando a situação do paciente possa comprometer o resultado do exame. O exame é realizado respirando-se pela boca através de um tubo conectado a um aparelho chamado espirômetro que é capaz de registrar o volume e a velocidade do ar respirado. A interpretação deste exame exige conhecimento de fisiologia e da mecânica respiratória humana e de doenças relacionadas ao pulmão. Mediante avaliação, a Sociedade Brasileira de Pneumologia e Tisiologia certifica médicos pneumologistas a laudar os exames. Pacientes com asma, DPOC, bronquite, bronquiectasia, enfisema, fibrose cística, sarcoidose ou fibrose pulmonar devem fazer espirometria periodicamente (de 3 em 3 meses, ou de 6 em 6 meses). O seu exame de espirometria serve para avaliar o efeito do tratamento médico em sua doença.
Eletroencefalografia (EEG) é um método de monitoramento eletrofisiológico que é utilizado para registrar a atividade elétrica do cérebro. Trata-se de um método normalmente não-invasivo, com eletrodos colocados no couro cabeludo, muito embora haja alguns métodos utilizados em aplicações específicas que são invasivos. A EEG mede as flutuações de tensão resultante da corrente iônica dentro dos neurônios do cérebro. Dentro de contextos clínicos, a EEG refere-se à gravação da atividade elétrica espontânea do cérebro durante um período de tempo, como a registrada a partir de múltiplos eletrodos colocados sobre o couro cabeludo. Aplicativos de diagnóstico normalmente focam no conteúdo espectral da EEG, isto é, no tipo de oscilações neurais (popularmente chamadas de "ondas cerebrais") que podem ser observadas em sinais de EEG. A maioria dos sinais cerebrais observados situam-se entre os 1 e 20 hertz. A EEG é frequentemente utilizada para o diagnóstico de epilepsia, o que traz algumas anormalidades nas leituras dos exames de EEG. Tais exames também são usados ainda para diagnosticar distúrbios do sono, coma, espongiformes e a morte cerebral. A EEG é usada para como primeira opção de exame para diagnóstico de tumores, acidente vascular cerebral e outros distúrbios cerebrais, mas esse último uso tem diminuído com o advento da alta-resolução anatômica de técnicas de imagem como a ressonância magnética (RM) e tomografia computadorizada (TC). Apesar das limitações de resolução espacial, a EEG continua a ser uma ferramenta valiosa para a investigação e o diagnóstico, especialmente quando é necessário um intervalo de resolução temporal na ordem de (o que não é possível com a tomografia computadorizada ou ressonância magnética). Técnicas derivadas da EEG incluem potenciais evocados (PE), que por sua vez envolvem a média da EEG de tempo de atividade-bloqueada para a apresentação de um algum tipo de estímulo (visual, somatossensorial, ou auditivo). Evento-potenciais relacionados com (ERPs) referem-se à média de EEG respostas que são bloqueados para mais complexo o processamento de estímulos; esta técnica é usada na ciência cognitiva, psicologia cognitiva, e psicofisiológicos de pesquisa.
Monitor Holter é um dispositivo portátil que monitora continuamente a atividade elétrica cardíaca de pacientes por 24 horas ou mais. Seu período estendido de gravação é muitas vezes útil para observar arritmias cardíacas ocasionais que seriam difíceis de serem identificadas em um período de tempo menor, como em um exame de eletrocardiografia (ECG). Assim como a eletrocardiografia padrão, o monitor Holter registra os sinais elétricos do coração através de uma série de eletrodos presos ao tórax. O número e a posição dos eletrodos varia de acordo com o modelo do aparelho, mas a maioria dos monitores Holter utilizam três a oito. Estes eletrodos são conectados a um pequeno receptor que é preso ao cinto do paciente, que é responsável por manter um registro da atividade elétrica cardíaca durante o período de gravação. Os dispositivos antigos costumavam gravar os dados em uma fita cassete. Os modelos mais atuais utilizam dispositivos de memória flash para a armazenagem dos dados. Os dados são enviados para um computador que os analisa automaticamente, contando os completos de ECG, calculando estatísticas como frequências cardíacas média, mínima e máxima e procurando áreas candidatas interessantes ao estudo posterior realizado por um técnico. Existem monitores para gravar 24 horas e 30 dias no mercado. Diário de eventos. Além de utilizar o dispositivo, a maioria dos pacientes é solicitada para escrever um diário com suas atividades diárias como corridas, sono, sintomas e horários em que os sintomas ocorrem. Essa informação é usada pelos médicos e técnicos para rapidamente selecionar, no meio da grande quantidade de dados registrados pelo monitor, áreas de interesse para análise do traçado eletrocardiográfico. História. O monitor recebe este nome em homenagem ao seu inventor, Norman J. Holter.
A creatinina é um produto da degradação da fosfocreatina (creatina fosforilada) no músculo, e é geralmente produzida em uma taxa praticamente constante pelo corpo — taxa diretamente proporcional à massa muscular da pessoa: quanto maior a massa muscular, maior a taxa. Através da medida da creatinina do sangue, do volume urinário das 24 horas e da creatinina urinária é possível calcular a taxa de filtração glomerular, que é um parâmetro utilizado em exames médicos para avaliar a função renal. Fisiologia. Ela é filtrada principalmente nos rins, embora uma pequena quantidade seja secretada ativamente. É livremente filtrada, não sofre reabsorção renal. Se a filtração do rim está deficiente, os níveis sanguíneos de creatinina aumentam. Este efeito é usado como um indicador da função renal. Entretanto, em muitos casos de disfunção renal severa, o nível de remoção de creatinina estará "superestimado" porque a secreção ativa da creatinina irá contribuir por uma grande fração da creatinina total que é removida. Taxas altas de creatinina e BUN (nitrogênio ureico do sangue) podem também ser um indicativo de desidratação quando a taxa de BUN-para-creatinina está anormal, com os níveis de BUN mais aumentados que os níveis de creatinina. Os homens tendem a ter níveis de creatinina mais altos porque eles possuem uma massa de músculo esquelético maior que a das mulheres. Uso para diagnóstico. A medida da concentração de creatinina no soro sanguíneo é um teste simples usado como o principal indicador da função renal. Um aumento dos níveis de creatinina no sangue é observado somente quando há um dano aos néfrons funcionantes. Logo este teste não é adequado para detectar uma doença renal em seu estágio inicial. Uma estimativa melhor da função renal é dada pelo teste de depuração de creatinina. A depuração (clearance) de creatinina pode ser calculada usando a concentração de creatinina no soro sanguíneo e alguns ou todas as seguintes variáveis: sexo, idade, peso e raça. Alguns laboratórios irão calcular a ClCr se solicitada pelo médico; e, a idade, sexo e peso necessários estarão incluídos na ficha de informação do paciente. Interpretação. No Brasil e nos EUA, a creatinina é reportada tipicamente na faixa 0,7 - 1,5 mg/dL, enquanto no Canadá e Europa μmol/litro pode ser usado. 1 mg/dL de creatinina equivale a 88,5 μmol/l. A referência típica para mulheres é considerada 0,5 a 1,0 mg/dL (cerca de 45-90 μmol/l), para homens 0,7 a 1,2 mg/dL³ (60-110 μmol/l). Entretanto, apesar de um nível de 2,0 mg/dL (150 μmol/l) poder indicar função renal normal em um fisiculturista (homem com grande massa muscular), um nível de 0,7 mg/dL (60 μmol/l) pode indicar alguma doença renal significativa em uma frágil senhora idosa. Mais importante que níveis absolutos de creatinina é a evolução dos níveis sorológicos de creatinina ao longo do tempo. Um nível crescente indica dano renal, enquanto um nível decrescente indica melhoria das funções dos rins.
Hemograma é um exame que avalia as células sanguíneas de um paciente, ou seja, as da série branca e vermelha, contagem de plaquetas, reticulócitos e índices hematológicos. O exame é requerido pelo profissional de saúde para diagnosticar ou controlar a evolução de uma doença. Um hemograma é constituído pela contagem das células brancas (leucócitos), células vermelhas (hemácias), hemoglobina (Hb), hematócrito (Ht), índices das células vermelhas, e contagem de plaquetas. Hemograma Completo consiste do hemograma mais a contagem diferencial dos leucócitos. As células circulantes no sangue são divididas em três tipos: células vermelhas (hemácias ou eritrócitos), células brancas (ou leucócitos) e plaquetas (ou trombócitos). Coleta de sangue. O sangue do indivíduo é colhido com anticoagulante (EDTA) ou mistura de Paul-Heller, para se evitar sua coagulação. Após a coleta com seringa descartável, o sangue é transferido para um tubo de ensaio de vidro, que deverá ser rotulado com o nome do paciente e lacrado com tampa. Não é necessário jejum, mas recomenda-se 24 horas sem prática de exercícios físicos e 48 horas sem consumo de bebida alcoólica. Deve-se perguntar ao paciente se faz uso de algum medicamento, ou se fez uso nas horas antecedentes ao exame, pois alguns fármacos podem interferir nos resultados do exame. Após a coleta, o tubo deverá ser enviado a um laboratório capaz de fazer o hemograma. Processo manual. ontagens manuais do número de hemácias, plaquetas e leucócitos podem ser feitas em câmara de Neubauer, após uma diluição prévia do sangue. O método dificilmente é usado, exceto em poucos casos de dúvidas da metodologia automática. O esfregaço de sangue é utilizado para fazer uma diferenciação entre os leucócitos, isto é, fazer uma contagem do número de neutrófilos, linfócitos, monócitos, eosinófilos e basófilos, chegando-se a uma porcentagem de cada célula encontrada, processo usado também para avaliar a série vermelha e as plaquetas. É feito com uma pequena gota de sangue posta sobre uma lâmina de vidro onde o técnico fará um esfregaço, arrastando a gota de sangue com outra lâmina para formar uma película. O sangue tem que ser homogeneizado antes de se fazer o esfregaço para que as células estejam bem distribuídas. O esfregaço é corado com Leishman ou Giemsa e observado em microscópio. A vantagem de se fazer um esfregaço é que algumas células podem ser contadas erradamente pelos processos automáticos. Alguns aparelhos não contam células imaturas e podem levar a um erro quanto a um diagnóstico de leucemia. O esfregaço, porém, deve ser avaliado por pessoal experiente. Processo automático. Hoje em dia o hemograma é feito em aparelhos que usam uma pequena quantidade de sangue. Há dois sensores principais: um detector de luz e um de impedância elétrica. As células brancas, ou leucócitos, podem ser contadas baseando-se em seu tamanho ou avaliando-se suas características. Quando a contagem é baseada no tamanho das células, o aparelho as diferencia por 3 tipos: células pequenas (linfócitos), células médias (neutrófilos, eosinófilos e basófilos) e células grandes (monócitos). Esse primeiro tipo de aparelho requer uma contagem manual de células, pois não diferencia as células de tamanho médio, podendo omitir uma eosinofilia, por exemplo. Os que utilizam o método de características das células são mais precisos.Em relação a série vermelha, o aparelho mede a quantidade de hemoglobina, o número de hemácias e o tamanho destas, realizando cálculos para chegar ao valor do hematócrito e os outros índices hematimétricos. As plaquetas também são contadas por aparelhos.
Ureia (FO43: uréia) é um composto orgânico cristalino, incolor, de fórmula (NH2)2CO, com um ponto de fusão de 132,7 °C. Tóxica, a ureia forma-se principalmente no fígado, sendo filtrada pelos rins e eliminada na urina ou pelo suor, onde é encontrada abundantemente; constitui o principal produto terminal do metabolismo proteico no ser humano e nos demais mamíferos. Em quantidades menores, está presente no sangue, na linfa, nos fluidos serosos proveniente da decomposição das células do corpo e também das proteínas dos alimentos. A ureia também está presente no mofo dos fungos, assim como nas folhas e sementes de numerosos legumes e cereais. É solúvel em água e em álcool, e ligeiramente solúvel em éter. A ureia foi descoberta por Hilaire Rouelle em 1773. Foi o primeiro composto orgânico sintetizado artificialmente em 1828 por Friedrich Woehler, obtido a partir do aquecimento do cianato de amônio (sal inorgânico). Esta síntese derrubou a teoria de que os compostos orgânicos só poderiam ser sintetizados pelos organismos vivos (teoria da força vital). As principais aplicações da ureia são: Na manufatura de plásticos, especificamente da resina ureia-formaldeído. Devido ao seu alto teor de nitrogênio, a ureia preparada comercialmente é utilizada na fabricação de fertilizantes agrícolas. Como estabilizador em explosivos de nitrocelulose. Na alimentação de ruminantes. Pode ser encontrada em alguns condicionadores de cabelo e loções. Utilizado para aumentar a solubilidade de corantes na indústria têxtil. Na produção de ARLA32 (AdBlue), reagente utilizado no SCR para reduzir NOx em veículos Diesel - SCR (Catalisador de Redução Seletiva).
O potássio é um elemento químico de símbolo K (do grego κάλιο "kalium", nome original da sua base KOH), número atômico 19 (19 prótons e 19 elétrons), distribuição eletrónica 2-8-8-1, metal alcalino, de massa atómica 39 u, coloração branco prateado, abundante na natureza, encontrado principalmente nas águas salgadas e outros minerais. Oxida-se rapidamente com o oxigênio do ar, é muito reativo especialmente com a água e se parece quimicamente com o sódio. É um elemento químico essencial para o homem, encontrado em muitas hortaliças, e essencial para o crescimento das plantas. Empregado em células fotoelétricas. Foi descoberto por Humphry Davy, em 1807, a partir da eletrólise do hidróxido de potássio (KOH). Tem o Raio atômico maior que o do Hélio. É o segundo metal mais leve. É um elemento muito maleável - pode ser cortado facilmente com uma faca. Tem um ponto de fusão muito baixo, arde com chama violeta e apresenta uma coloração prateada nas superfícies não expostas ao ar, já que se oxida com rapidez. Entretanto, deve ser armazenado dentro de um recipiente com querosene. Assim como os demais metais alcalinos, reage violentamente com a água, desprendendo hidrogênio (H2), podendo inflamar-se espontaneamente em presença desta substância. Aplicações: O potássio é um metal empregado em células fotoelétricas. O cloreto de potássio e o nitrato de potássio são empregados como fertilizantes. Potássio é exigido em grandes quantidades pelas plantas por ser um regulador de pressão osmótica, ativador de enzimas, além de ser importante na formação de frutos, resistência ao frio e doenças. O peróxido de potássio é usado em aparatos de respiração de bombeiros e mineiros. O nitrato também é usado na fabricação de pólvora, o cromato de potássio e o dicromato de potássio em pirotecnia. O carbonato de potássio é empregado na formação de cristais. A liga NaK, uma liga de sódio e potássio, é um material usado como transferente de calor. O cloreto de potássio é utilizado para provocar parada cardíaca em injeções letais. Outros sais de potássio importantes são o brometo de potássio, cianeto de potássio, iodeto de potássio e o sulfato de potássio, entre outros. Uma importante base é o hidróxido de potássio. Os sabões à base de potássio são os chamados "sabões moles", tais como os cremes de barbear. Dietas ricas em potássio podem exercer papel na prevenção e tratamentos da hipertensão arterial reduzindo os efeitos adversos do consumo de sal.
Obesidade é uma condição médica em que se verifica acumulação excessiva de tecido adiposo ao ponto de poder ter impacto negativo na saúde. Uma pessoa é considerada obesa quando o seu índice de massa corporal (IMC) é superior a 30 kg/m2, e com excesso de peso quando o seu IMC é superior a 25–30 kg/m2. O IMC é calculado dividindo o peso da pessoa pelo quadrado da sua altura. A obesidade aumenta a probabilidade de ocorrência de várias doenças, em particular de doenças cardiovasculares, diabetes do tipo 2, apneia do sono obstrutiva, alguns tipos de cancro, osteoartrite, e depressão. A causa mais comum de obesidade é uma combinação de dieta hiperenergética, falta de exercício físico e suscetibilidade genética. Alguns casos são causados por genes, doenças endócrinas, medicamentos ou perturbações mentais. Não há evidências que apoiem um metabolismo lento como causa de obesidade em pessoas obesas que comem pouco. Em média, as pessoas obesas consomem mais energia do que as restantes, uma vez que quanto maior a massa corporal, maior a necessidade de energia. A prevenção da obesidade consiste em alterações sociais e escolhas pessoais. O tratamento da obesidade baseia-se na dieta e no exercício físico. A qualidade da dieta pode ser melhorada reduzindo o consumo de alimentos ricos em energia, tais como os que têm grande quantidade de gordura e açúcar, e aumentando a ingestão de fibra dietética. Para acompanhar a dieta adequada pode ser administrada medicação anti-obesidade para reduzir o apetite ou diminuir a absorção de gordura pelo corpo. Quando a dieta, o exercício e a medicação não demonstram ser eficazes, pode ser considerada a aplicação de uma banda gástrica ou uma cirurgia bariátrica para reduzir o volume do estômago ou o comprimento do intestino, o que faz com que a pessoa se sinta cheia mais cedo e que haja menor capacidade de absorção de nutrientes dos alimentos. A obesidade é uma das principais causas de morte evitáveis em todo o mundo, com taxas de prevalência cada vez maiores em adultos e crianças. Em 2015, 600 milhões de adultos (12% do total) e 100 milhões de crianças eram obesas. A obesidade é mais comum entre mulheres do que entre homens. As autoridades de saúde consideram a obesidade um dos mais graves problemas de saúde pública do século XXI. Em grande parte do mundo contemporâneo, particularmente na sociedade ocidental, a obesidade é alvo de estigma social, embora ao longo da História tenha sido vista como símbolo de riqueza e fertilidade, perspectiva que ainda se mantém em algumas partes do mundo.
Gastroenterite é uma inflamação do trato gastrointestinal que afeta o estômago e o intestino delgado. Os sintomas mais comuns são diarreia, vómitos e dor abdominal. Outros possíveis sintomas incluem febre, falta de energia e desidratação. Geralmente os sintomas manifestam-se durante menos de duas semanas. Embora por vezes seja denominada "gripe intestinal", a doença não tem relação com a gripe. A gastroenterite pode ser causada por infeções por vírus, bactérias, parasitas ou fungos. A causa mais comuns são vírus. Em crianças, o rotavírus é a causa mais comum dos casos graves da doença. Em adultos, os mais comuns são os norovírus e Campylobacter. A transmissão pode ter origem na ingestão de alimentos que não foram devidamente preparados, em beber água não potável ou pelo contacto direto com uma pessoa infetada. Geralmente não são necessários exames para confirmar o diagnóstico. As medidas de prevenção incluem lavar as mãos com sabonete, beber apenas água potável, saneamento e amamentar os bebés em vez de usar fórmula infantil. Em crianças está recomendada a vacina contra rotavírus. O tratamento consiste na ingestão de líquidos em quantidade suficiente. Em casos ligeiros ou moderados, isto é feito com solução de reidratação oral, que consiste numa combinação de água, sais e açúcar. Em bebés a ser amamentados, está recomendado continuar a amamentação. Em casos mais graves pode ser necessária a administração de líquidos por via intravenosa. Os líquidos podem ainda ser administrados por uma sonda nasogástrica. Em crianças, está recomendada a suplementação de zinco. Geralmente não são necessários antibióticos. Em 2015 ocorreram dois mil milhões de casos de gastroenterite que causaram 1,3 milhões de mortes em todo o mundo. A doença afeta sobretudo crianças nos países em vias de desenvolvimento. Em 2011, ocorreram cerca de 1,7 mil milhões de casos em crianças com menos de cinco anos de idade que causaram cerca de 700 000 mortes. Nos países em vias de desenvolvimento, é frequente que as crianças com menos de dois anos contraiam seis ou mais infeções por ano. A doença é menos comum em adultos, devido em parte ao desenvolvimento de imunidade adquirida. A gastroenterite manifesta-se normalmente através de diarreia e vómitos, e menos frequentemente através de apenas um dos sintomas. Podem também ocorrer cólicas abdominais. Os sintomas têm normalmente início entre 12 a 72 horas depois de se contrair o agente infeccioso. Quando tem origem viral, a doença desaparece normalmente ao fim de uma semana. Alguns agentes virais podem estar na origem de sintomas associados como febre, fadiga, dores de cabeça e dores musculares. No caso das fezes conterem sangue, é pouco provável que a causa seja viral e muito provável que seja bacteriana. Algumas infecções bacterianas podem estar associadas a cólicas abdominais muito intensas e podem persistir por várias semanas. As crianças infectadas por rotavírus recuperam normalmente após três a oito dias. No entanto, em regiões menos desenvolvidas onde o acesso a cuidados de saúde é difícil, é comum que a diarreia persista por um período de tempo maior. A desidratação é uma complicação frequente da diarreia, e crianças com um grau significativo de desidratação podem apresentar teste de re-enchimento capilar lento, turgor da pele reduzido e respiração anormal. No lactente a depressão da fontanela superior é um sinal precoce de desidratação. Em regiões sem condições sanitárias, é comum a ocorrência repetitiva de infecções, o que a longo prazo pode dar origem a desnutrição, crescimento deficiente e atraso cognitivo. Cerca de 1% dos infectados com espécies de Campylobacter desenvolvem artrite reativa e 0,1% desenvolvem a síndrome de Guillain-Barré. As infecções por Escherichia coli ou espécies de Shigella, produtoras da toxina Shiga podem dar origem à síndrome hemolítico-urémico, que se manifesta pela baixa contagem de plaquetas, insuficiência renal e baixa contagem de glóbulos vermelhos. As crianças são mais predispostas a contrair a síndrome do que os adultos. Algumas infecções virais podem dar origem a convulsões epilépticas infantis benignas. As principais causas da gastroenterite são os vírus, em particular o rotavírus, e a espécie bacteriana E. coli e as espécies do género Campylobacter. Existem, no entanto, vários outros agentes infecciosos que podem causar a doença. São por vezes registadas causas não-infecciosas, mas são muito menos prováveis de ocorrer. O risco de infecção é maior em crianças devido à sua pouca defesa imunitária e relativa falta de higiene. Entre os vírus que se sabe estarem na origem da gastroenterite contam-se os rotavírus, os norovírus, os adenovírus e os astrovírus. Os rotavírus são a causa mais comum da gastroenterite em crianças, com taxas de incidência semelhantes nos países desenvolvidos e em desenvolvimento. Os vírus são responsáveis por cerca de 70% dos episódios de diarreia infecciosa em pediatria. O rotavírus é uma causa menos comum em adultos devido à imunidade adquirida ao longo da vida. Os norovírus são a principal causa de gastroenterite entre adultos na América do Norte, sendo responsáveis por mais de 90% dos surtos. Este tipo de epidemias localizadas ocorre quando grupos de indivíduos passam tempo em relativa proximidade física, como em cruzeiros, hospitais ou restaurantes. Os indivíduos podem permanecer infectados mesmo depois da diarreia ter cessado. Os norovírus são também responsáveis por cerca de 10% das ocorrências infantis. Nos países desenvolvidos a principal causa de gastroenterite bacteriana é a Campylobacter jejuni. Metade destes casos estão associados com a exposição a aves de criação. Em crianças, as bactérias são responsáveis por cerca de 15% dos casos, sendo as espécies mais comuns a Escherichia coli, Salmonella, Shigella e a Campylobacter. Se determinado alimento for contaminado por bactérias e se se mantiver à temperatura ambiente durante várias horas, as bactérias não multiplicam-se e potenciam o risco de infecção dos eventuais consumidores. Entre os alimentos normalmente associados com a contaminação estão a carne, marisco e ovos crus ou mal cozinhados; rebentos crus; leite não pasteurizado, queijos moles, fruta e sumos de vegetais. Nos países em desenvolvimento, sobretudo na Ásia e na África sub-Sariana, a cólera é uma das principais causas de gastroenterite, sendo transmitida através da água ou alimentos contaminados. O bacilo toxigénico Clostridium difficile é também uma das causas de diarreia, o que se verifica sobretudo em idosos. As crianças podem ser portadoras desta bactéria sem que manifestem sintomas. Causa também diarreia entre aqueles hospitalizados e é frequentemente associada ao uso de antibióticos. A diarreia causada por Staphylococcus aureus pode também ocorrer em consumidores de antibióticos. A diarreia do viajante é normalmente um tipo de gastroenterite bacteriana. Os medicamentos antiácidos aparentam aumentar o risco de infecção após exposição a uma série de organismos, entre os quais a Clostridium difficile e as espécies de Salmonella e Campylobacter. O risco é maior naqueles que tomam inibidores da bomba de protões do que nos que tomam anti-histamínicos H2. A gastroenterite pode ser provocada por uma série de protozoários, sobretudo pela Giardia lamblia, mas também por Entamoeba histolytica e pelas espécies de Cryptosporidium. No total, estes agentes são responsáveis por cerca de 10% dos casos em crianças. A Giardia é mais frequente nos países em desenvolvimento, mas este agente etiológico está na origem de infecções praticamente em qualquer região. É, no entanto, mais comum em pessoas que tenham viajado para regiões com grande prevalência do parasita, em crianças que frequentam jardins de infância, relações homossexuais e na sequência de desastres. A transmissão pode dar-se através do consumo de água contaminada ou da partilha de objectos pessoais. Em regiões com estações secas e chuvosas, a qualidade da água deteriora-se frequentemente durante a estação húmida, o que tem relação directa com a ocorrência de surtos. Em regiões de estações temperadas, as infecções são mais frequentes durante o Inverno. O uso de biberões com recipientes mal esterilizados é uma das mais significativas causas de transmissão a nível global. As taxas de transmissão estão igualmente relacionadas com maus hábitos de higiene, sobretudo entre crianças, em habitações sobrelotadas, e em indivíduos que apresentem já sintomas de má nutrição. Depois de desenvolverem imunidade, os adultos podem ser portadores de determinados patogéneos sem demonstrar quaisquer sinais ou sintomas, agindo como reservatórios de contágio. Enquanto que alguns agentes, como a Shigella, são exclusivos dos primatas, outros, como a Giardia, podem contaminar uma série de animais. Existe uma série de causas não-infecciosas capazes de provocar inflamações do trato gastrointestinal. Entre as mais comuns estão: medicamentos como os anti-inflamatórios não esteroides, determinados alimentos como a lactose (naqueles que são intolerantes) ou o glúten (naqueles com doença celíaca). A doença de Crohn pode igualmente estar na origem de gastroenterites, por vezes bastante severas. Pode também verificar-se o aparecimento da doença como consequência de determinadas toxinas. Entre os sintomas relacionados com a ingestão de alimentos para além das náuseas, vómitos e diarreia estão: a intoxicação por ciguatera, em consequência do consumo de peixe contaminado; a intoxicação por tetrodotoxina associada à ingestão de tetraodontídeos; e o botulismo associado à conservação imprópria da comida.
As correntes de jato, ou simplesmente jatos (em inglês: Jet Streams), são correntes de ar que ocorrem na atmosfera de alguns planetas, incluindo a Terra. Estes jatos de ar foram descobertos durante as incursões aéreas na Segunda Guerra Mundial. As principais correntes de jato localizam-se perto da tropopausa, na transição entre a troposfera (onde a temperatura diminui consoante a altitude) e a estratosfera (onde a temperatura aumenta com a altitude respectiva). As principais correntes de jato do planeta Terra são os ventos do oeste (que fluem para leste). Os seus percursos são determinados por formas sinuosas, onde a sua constância é indeterminável pelo seu carácter repentino de mudança de direção; os jatos podem surgir subitamente, cessar, dividirem-se em duas ou mais partes, convergirem-se num único fluxo, ou mesmo fluir em várias direções, incluindo na direção oposta à do próprio jato. Os jatos mais poderosos são os jatos polares, que ocorrem em torno dos 7 e 12 km (entre as latitudes de 40°-70°) acima do nível do mar, e os jatos subtropicais que andam entre 10 e 16 km (20° e 30°) de altitude. Cada um dos hemisférios Norte e Sul possuem tanto um jato polar como um jato subtropical. O jato polar do hemisfério norte flui sobre o meio das latitudes setentrionais da América do Norte, Europa e Ásia e nos oceanos que neles intervém; enquanto que o jato polar do hemisfério sul circunda principalmente a Antártica durante todo o ano. As correntes de jato são provocados pela combinação da rotação do planeta sobre o seu eixo e o aquecimento da atmosfera (por radiação solar e, em outros planetas para além da Terra, pelo calor interno). As correntes de jato formam-se perto dos limites das massas de ar adjacentes com significantes diferenças de temperatura, tais como a região polar e o ar quente que segue em direção ao equador. Outras correntes de jato são também conhecidas. Durante o verão no hemisfério norte, jatos do leste podem-se formar nas regiões tropicais, normalmente numa região onde o ar seco cruza o ar húmido a grandes altitudes. Jatos em níveis mais baixos são também comuns em várias regiões como no centro dos Estados Unidos. Os meteorologistas utilizam dados fornecidos pela localização de algumas correntes de jato como referência para a previsão do tempo. A principal importância comercial das correntes de jato ocorre no tráfego aéreo, onde a economia de voo pode ser drasticamente afetada por qualquer um destes que adeja a favor do fluxo ou contra o fluxo de uma corrente. A turbulência de ar claro - um potencial risco para a segurança dos passageiros das aeronaves - é muitas vezes encontrado nas imediações de uma corrente de jato, porém não motiva qualquer alteração substancial no tempo de um voo. Ocorrendo nos níveis superiores da troposfera, esta corrente é de grande importância à aviação, podendo ser aproveitada para proporcionar economia de combustível nas aeronaves que as penetram aproveitando o impulso proporcionado pelos seus intensos ventos. Após a erupção do vulcão de Krakatoa em 1883, observadores climáticos rastrearam e mapearam durante vários anos os efeitos das erupções na atmosfera, denominando o fenómeno de "corrente equatorial de fumo". Em 1920, um meteorologista japonês, Wasaburo Oishi, detectou uma corrente de jato de um local perto do Monte Fuji e estudou-a mediante a utilização de um balão meteorológico (comummente usados para determinar ventos a grande atitude). O trabalho de Oshi, em grande parte fora do Japão, passou despercebido. Porém, a descoberta das correntes de jato é frequentemente atribuída ao piloto americano Wiley Post, o primeiro homem a voar sozinho ao redor do mundo em 1933. Post inventou um fato pressurizado que lhe permitiu voar até aos 50 mil pés (acima de 15 Km). Um ano antes da sua morte, Post realizou várias tentativas de voo transcontinental a alta altitude, e constatou que em determinados momentos, a velocidade atingida perto do solo excedia a do ar alto. O meteorologista alemão Heinrich Seilkopf é creditado pelo cunho de um termo próprio para este fenómeno atmosférico, Strahlströmung (literalmente correntes de jato) feito em 1939. (O termo moderno alemão é Strahlstrom. Apesar disso, várias fontes atribuem a compreensão do funcionamento das correntes de jato aos voos realizados durante a Segunda Guerra Mundial por pilotos que notificaram a existência de ventos de cauda acima de 160 km/h em rotas feitas entre os Estados Unidos e a Grã-Bretanha. Durante a Segunda Grande Guerra, o exército japonês desenvolveu programa para o lançamento de balões de fogo (風船爆弾 fūsen bakudan?) dotados de um dispositivo barométrico que os permitia manter uma altitude entre 9.000 e 11.000 metros, com o objetivo de desembarque em solo dos EUA. O plano teve pouco sucesso, porém mostra que até então já eram conhecidos no Japão os efeitos de correntes de jato. Entretanto, uma extensa investigação sobre o fenómeno teve lugar após a Grande Guerra terminar. A descoberta das correntes de jato provou ser um marco na compreensão da circulação geral atmosférica terrestre, sendo atualmente usada pela meteorologia. Os jatos polares estão normalmente localizados próximos de 250 hPa da pressão atmosférica, entre 7 a 12 quilómetros acima do nível médio do mar; enquanto que as correntes subtropicais, significativamente mais fracas, encontram-se a uma maior altitude (entre 10 e 16 km). Em cada hemisfério, as correntes de jato formam-se perto da tropopausa, uma camada da atmosfera terrestre que no equador atinge uma maior altitude comparativamente aos pólos, e na qual ocorrem importantes variações do sistema atmosférico. No hemisfério norte, a corrente de jato encontra-se geralmente entre os 30ºN e os 60ºN de latitude, enquanto que a corrente subtropical habita os 30ºN. Alega-se que a corrente de jato do nível superior "segue o sol" à medida que este se move em direção ao norte durante a estação quente e finais da primavera e verão, durante o outono e inverno move-se para o sul. A largura de uma corrente de jato é geralmente de algumas centenas de quilómetros ou milhas e a sua espessura vertical alcança menos de cinco quilómetros. Evolução dos meandros da corrente de jato do hemisfério norte (a), (b); ao final, uma queda de ar frio se separa da corrente principal (c). A laranja: a massa de ar quente; a rosa: corrente de jato; a azul: ar frio. As correntes de jato são normalmente continuas ao longo de grandes distâncias, porém é comum existiram descontinuidades no seu percurso. Os circuitos das correntes de jato apresentam formas ondulantes, pelo que os meandros se movem para leste em velocidades menores que o vento da corrente principal. Cada meandro ou onda é denominado por onda de Rossby nas correntes de fluidos. As ondas de Rossby são criadas por alterações do efeito de Coriolis, em função da latitude, e propagam-se em direção a oeste, enquanto que o fluxo no qual estão inseridos, se movimentam para leste, retardando assim a migração global das cristas e cavados de pressão a leste em comparação com as depressões atmosféricas de ondas curtas em que estão inseridas. As depressões (ou zonas de baixa pressão) de ondas curtas são pequenos pacotes de energia de alto nível, de uma escala de 1 000 a 4 000 quilómetros de comprimento, que se movem através do fluxo principal, ou de extensas ondas, cristas e depressões dentro das ondas de Rossby. As correntes de jato podem dividir-se em duas partes, devido à formação de uma zona de baixa pressão, o que faz desgarrar uma porção do fluxo da sua própria base, enquanto que o resto do fluxo se movimenta para norte. Os jatos polares e subtropicais podem-se convergir num só, apesar disso só ocorrer em situações mais invulgares, permanecendo a maior parte do tempo separados. A velocidade do vento pode variar, dependendo da gradiente de temperatura, que facilmente excede os 90 km/h. Alguns registos, inclusive, apontam para ventos superiores a 398 quilómetros por hora. Hoje em dia, os meteorologistas assumem que o percurso das correntes de jato são responsáveis pelos sistemas ciclónicos de tempestades nos níveis baixos da atmosfera, e o conhecimento daí adquirido tornou-se uma parte importante da previsão do tempo. Um exemplo foi no ano de 2007 quando a Alemanha foi vítima de graves inundações como resultado do jato polar que se estabilizou em latitudes meridionais durante o verão.
O Aeroporto de São Paulo/Congonhas - Deputado Freitas Nobre, ou simplesmente Aeroporto de Congonhas, (IATA: CGH, ICAO: SBSP) é um aeroporto doméstico brasileiro localizado no município de São Paulo, sendo o segundo mais movimentado do país. O aeródromo fica na zona sul da cidade, no bairro de Vila Congonhas, nos distritos de Campo Belo e Jabaquara. A distância para o centro da capital é de 10,6 km e para o Aeroporto Internacional de São Paulo-Guarulhos de 35,6 km. É considerado o aeroporto executivo do Brasil em função do grande número de passageiros que viajam a negócios entre São Paulo e outros grandes centros, como Rio de Janeiro e Brasília. Inaugurado em meados dos anos 1930 em área descampada, logo foi envolvido pela cidade, tornando-se um aeroporto central. Atende a Grande São Paulo com voos domésticos nacionais e regionais para mais de trinta destinos no Brasil. Segundo estatísticas da ANAC de 2014, Congonhas conta com cinco das vinte rotas mais movimentadas do Brasil, incluído a ponte aérea Rio-São Paulo, rota mais movimentada do país. Já foi o mais movimentado do país, de 1990 até 2006, quando o acidente com o voo TAM 3054, em julho de 2007, fez com que muitos voos fossem transferidos para outros aeroportos. Hoje é o segundo em número de passageiros e de aeronaves e o primeiro da rede Infraero. Opera no limite da capacidade de suas pistas, com 536 voos comerciais por dia, o que equivale a um pouso ou uma decolagem a cada 2 minutos durante seu horário de funcionamento, das 06h às 23h. O complexo aeroportuário de Congonhas abrange uma área de aproximadamente 1,6 milhão de metros quadrados, contando com duas pistas com capacidade para até 41 operações por hora, e um terminal de passageiros com capacidade para atender cerca de 6 500 passageiros por hora. A pista principal é equipada com os sistemas de pouso por instrumentos ALS e ILS, que permitem operações em condições adversas com segurança, além de reduzir fechamentos por conta de tempo ruim. A característica mais marcante do terminal de passageiros do Aeroporto de Congonhas é o piso xadrez, formado por quadrados em placa de granito preto e mármore branco. Esse piso da década de 60 incorporou-se ao terminal e ficou na memória da população, tornando-se a identidade visual do aeroporto. Em 19 de junho de 2017 foi sancionada pelo presidente Michel Temer a lei 13.450/17, que alterou o nome seu nome para Aeroporto de São Paulo/Congonhas-Deputado Freitas Nobre, em homenagem ao ex-deputado, advogado e professor da Universidade de São Paulo, José Freitas Nobre, que lutou pela anistia e foi crítico da ditadura militar. Desde 1920, o aeroporto que atendia a cidade de São Paulo era o Campo de Marte, localizado às margens do Rio Tietê, onde as chuvas frequentemente causavam alagamentos. Com isso, em 1935 foram feitos estudos pelo governo do estado de São Paulo, com a intenção de prover a São Paulo um aeroporto que não estivesse sujeito às enchentes. A região de Congonhas então foi escolhida por suas condições naturais de visibilidade e de drenagem, longe de áreas alagadiças. Na época, quando a cidade de São Paulo tinha 1 milhão de habitantes, a escolha do local foi criticada pelo fato de ser uma região descampada e distante. O nome Congonhas é uma homenagem ao Visconde de Congonhas do Campo, Lucas Antônio Monteiro de Barros (1767-1851), primeiro presidente da Província de São Paulo após a Independência do Brasil (1822). Congonhas também é o nome de um tipo de erva-mate muito comum em Minas Gerais, na região onde se situa Congonhas do Campo, cidade natal de Monteiro de Barros. Entre 1928 e 1932, a Cia. Auto-Estradas Incorporadora e construtora S.A construiu a estrada de ligação entre São Paulo e Santo Amaro (atual avenida Washington Luís). Essa companhia era proprietária de um grande terreno entre Santo Amaro e o Ibirapuera e vinha vendendo lotes na área deste 1930. Em 1936, a Auto-Estradas comprou um grande terreno de um bisneto do Visconde de Congonhas, que era proprietário das terras onde seria construído o aeroporto. Essa construtora planejava urbanizar a região, chamando-a de Vila Congonhas. Com isso, a Auto-Estradas torna-se a maior interessada na instalação do aeroporto na região e como forma de pressionar o governo do estado de São Paulo pela escolha do terreno, a companhia, por conta própria, construiu uma pista de terra para pousos e decolagens à margem da estrada de rodagem para Santo Amaro. Em 1935, o Governo do estado de São Paulo realiza um estudo técnico para a escolha do sítio onde se localizaria o novo aeroporto. Em 12 de abril de 1936, pela primeira vez, o Campo de Aviação da Companhia Auto-Estradas foi utilizado publicamente em caráter experimental. Pilotos consagrados foram convidados para exibir-se e testar as condições de Congonhas para sediar o aeroporto. Em julho de 1936, com a construção de uma segunda pista de terra, companhias de aviação comercial passaram a utilizar o campo a pista de terra, que foi brevemente conhecido como Campo da VASP. Ainda no mesmo ano, no dia 15 de setembro, o governo de São Paulo finalmente adquiriu o terreno depois de chegar a um acordo com a Auto-Estradas quanto ao preço e o aeroporto passa então a ser denominado oficialmente como Aeroporto de São Paulo, sob a administração da Diretoria da viação da Secretaria da Viação e Obras Públicas do Estado de São Paulo. Em 1940, a Secretaria de Estado dos Negócios e da Viação estabelece que o Aeroporto seria administrado por um representante do governo do estado de São Paulo. Assim, o estado investiu sistematicamente em Congonhas, em especial no primeiro período e ao lado da pista, foi construída uma pequena estação de passageiros em linhas art déco que funcionou até 1948. Em 1942, tem inicio estudos de melhorias do Aeroporto de São Paulo feitos a pedido da Diretoria da Viação. Em 1947, começam as obras da primeira grande reforma do aeroporto de São Paulo. Em 1949, as obras de prolongamento da pista principal para 1 865 metros foi concluída. Em 1951, as obras da torre de controle são terminadas. Em 1954, é inaugurado o pavilhão das autoridades e em 1955, o novo terminal de passageiros começou a funcionar. Em 1957, o Aeroporto de Congonhas passa a ser o terceiro do mundo em movimento de carga aérea, depois de Londres e Paris. Em 1959, a ala internacional é inaugurada e a ponte aérea Rio-São Paulo, acordo firmado entre as companhias Varig, Vasp e Cruzeiro do Sul que operavam os Convair 240, Scandia e Convair 340, respectivamente, na ligação aérea entre as cidades do Rio de Janeiro e São Paulo, começa a funcionar. Em 1962, tem a implantação do primeiro serviço de RADAR. Em 3 de maio de 1963, um Convair 340 da Cruzeiro do Sul decolou de Congonhas à noite com destino ao Aeroporto Santos Dumont, no Rio de Janeiro, quando foi detectado um incêndio em um dos motores do avião. Ao tentar retornar para o aeroporto, a aeronave perdeu sustentação, vindo a cair sobre uma casa na avenida Piassanguaba, bairro Planalto Paulista, se incendiando em seguida. Mais de 30 pessoas morreram neste acidente, que também deixou outras pessoas com ferimentos. Em 1968, é criada a Comissão Coordenadora do Projeto Aeroporto Internacional (CCPAI) pelo Ministério da Aeronáutica brasileiro com finalidade de construir simultaneamente dois aeroportos de primeira classe internacional, um no Rio de Janeiro e outro em São Paulo, sendo que o eixo Rio-São Paulo concentrava 90% do tráfego internacional do Brasil. A importância desses novos aeroportos é oferecer uma nova infraestrutura aeroportuária de que demandavam os grandes aviões intercontinentais que vinham sendo lançados. Em 1968, tem início a expansão comercial e residencial na região próxima ao aeroporto, com surgimento de bairros populosos em seu entorno.
A Igreja de São Francisco de Assis é uma igreja católica da cidade de Ouro Preto, Brasil, construída em estilo Barroco, com elementos decorativos Rococó. É um dos monumentos mais significativos da arte colonial, uma das mais conhecidas igrejas brasileiras daquele período e uma das mais celebradas criações do mestre Aleijadinho, que elaborou o projeto básico da fachada e da decoração em relevos e talha dourada, realizando pessoalmente diversos de seus elementos, ainda que outros artistas também tenham colaborado. Ali também se encontram trabalhos do Mestre Ataíde, o maior nome da pintura colonial brasileira, que decorou o teto da nave criando aquela que se tornou sua composição mais famosa, além de pintar outros painéis e dourar o altar-mor. Pela sua relevância extraordinária, a Igreja foi tombada pelo Instituto do Patrimônio Histórico e Artístico Nacional (IPHAN), foi classificada em 2009 como uma das Sete Maravilhas de Origem Portuguesa no Mundo, e integrando a Cidade Histórica de Ouro Preto é parte do Patrimônio da Humanidade. A Igreja foi erguida pela Ordem Terceira de São Francisco de Assis. Em torno de 1765 iniciou a preparação do terreno, e em 1766 as obras foram arrematadas pelo mestre pedreiro Domingos Moreira de Oliveira, seguindo um projeto de Aleijadinho. Contudo, a autorização oficial para a ereção do templo só veio em 1771. Como era o hábito na época, a construção foi iniciada a partir da capela-mor. Os trabalhos corriam rápido, e em 1774 já estava coberta, começando a decoração interna em talha e estuque sob a direção de Aleijadinho. Dele são os púlpitos em pedra-sabão instalados junto ao arco do cruzeiro. O altar-mor foi erguido entre 1790 e 1794. Na sequência, passou-se à construção da fachada e frontispício, com a portada projetada pelo Aleijadinho. Em 1787 o desenho das torres foi modificado e o telhado foi instalado em 1788. As obras de alvenaria só foram terminadas em 1794 pelo mestre Oliveira. Faltava ainda a decoração da nave. Os altares laterais também foram projetados por Aleijadinho, mas sua construção tardou, sendo executados somente a partir de 1829, arrastando-se até 1890. Manuel da Costa Ataíde foi encarregado da pintura do teto da nave, dos painéis da capela-mor e da nave e do douramento e pintura da talha da capela-mor, iniciando em 1801 e concluindo em 1812. Em 1801 também foram ocluídos os vãos nas laterais com a construção de um telhado e arcadas, a fim de evitar a infiltração de umidade que se verificava.Em fins do século XIX foram instalados ladrilhos no piso e a antiga escadaria de acesso foi demolida. Em 1935 foi construído um cemitério anexo à capela-mor. No século XX a igreja passou por vários trabalhos de conservação e restauro levados a cabo pela antiga Inspetoria de Monumentos Nacionais e pelo Instituto do Patrimônio Histórico e Artístico Nacional (IPHAN). A Igreja de São Francisco de Assis é considerada uma obra-prima da arte colonial brasileira, e tem chamado a atenção dos intelectuais brasileiros e estrangeiros desde que o Barroco, depois de mais de um século de desprezo, voltou a ser recuperado pela crítica internacional. Mário de Andrade foi um dos que deixaram, no início do século XX, um testemunho positivo sobre esta e as outras igrejas projetadas pelo Aleijadinho, artista a quem se atribui — junto com seu frequente colaborador, o Mestre Ataíde — grande papel na formulação de uma estética que se revelou um dos primeiros momentos de originalidade na tradição cultural cultivada numa então colônia fortemente dependente de Portugal em todos os sentidos. Porém, escrevendo em um período em que a estética oitocentista ainda enfrentava grande rejeição, ele de certa forma ainda precisava justificar-se para seus leitores, como que "se desculpando" por apreciá-la, e seu texto é ilustrativo também porque revela a confusão conceitual que ainda pairava sobre a definição de Barroco naquele momento em que este tema apenas começava a ser desbravado pelos historiadores, e que no Brasil foi erigido como ponto de apoio fundamental na tentativa de definição do que seria a verdadeira identidade cultural da nação. Diz ele: "É certo que elas  não possuem majestade, como bem denunciou Saint-Hilaire. Mas a majestade não faz parte do Brasileiro, embora faça parte comum da nossa paisagem. Carece, no entanto, compreender que o sublime não implica exatamente majestade. Não é preciso ser ingente para ser sublime. As igrejas do Aleijadinho não se acomodam com o apelativo 'belo', próprio à São Pedro de Roma, à catedral de Reims, à Batalha, ou à horrível São Marcos de Veneza. Mas são muito lindas, são bonitas como o quê. São dum sublime pequenino, dum equilíbrio, duma pureza tão bem arranjadinha e sossegada, que são feitas pra querer bem ou pra acarinhar, que nem na cantiga nordestina. São barrocas, não tem dúvida, mas a sua lógica e equilíbrio de solução é tão perfeito, que o jesuitismo desaparece, o enfeite se aplica com uma naturalidade tamanha, que si o estilo é barroco, o sentimento é renascente . O Aleijadinho soube ser arquiteto de engenharia. Escapou genialmente da luxuosidade, da superfectação, do movimento inquietador, do dramático, conservando uma clareza, uma claridade é melhor, puramente da Renascença". A igreja foi tombada pelo Serviço do Patrimônio Histórico e Artístico Nacional, o antecessor do IPHAN, já em 1938, um dos primeiros monumentos protegidos pela instituição que foi fundada por outros intelectuais que estavam envolvidos com as mesmas preocupações de Mário de Andrade a respeito do resgate e legitimação do passado colonial, naquela que já foi chamada de a "fase heroica do IPHAN". Hoje o antigo rechaço pelo Barroco já foi amplamente dissolvido, e mesmo os turistas já entendem a Igreja de São Francisco como a principal atração de Ouro Preto. A Igreja foi destacada individualmente quando a UNESCO declarou a Cidade Histórica de Ouro Preto como Patrimônio da Humanidade, considerando-a uma obra-prima da arquitetura brasileira e um dos principais monumentos do Barroco nacional.
As fases da Lua referem-se à mudança aparente da porção visível iluminada do satélite devido a sua variação da posição em relação à Terra e ao Sol. O ciclo completo, denominado lunação, leva pouco mais de 29 dias para se completar, período no qual a Lua passa da fase nova, quando a sua porção iluminada visível passa a aumentar gradualmente até que, duas semanas depois ocorra a lua cheia e, cerca de duas semanas depois, volta a diminuir e o satélite entra novamente na fase nova. Eventualmente, ocorre o perfeito alinhamento entre o Sol, a Terra e a Lua, o que dá origem a eclipses. Um eclipse solar acontece quando a Lua passa em frente ao disco solar, podendo ocorrer somente durante a lua nova, enquanto que um eclipse lunar ocorre no momento em que a Lua passa através da sombra da Terra, o que pode ocorrer somente na lua cheia. Esta transição entre fases foi na antiguidade utilizada para contagem do tempo, de forma que muitos calendários lunares foram criados tendo como base o ciclo lunar. Pelo fato de que a Lua completa uma órbita ao redor da Terra a cada 27,3 dias, período que constitui o mês sideral, sua posição muda continuamente. Além disso, nosso satélite natural não possui luz própria, de forma que sua porção brilhante deve-se ao reflexo da luz solar. A qualquer momento, metade da superfície lunar está iluminada pelo Sol, por ser um corpo aproximadamente esférico, mas a fração iluminada que pode ser observada da Terra sofre variações contínuas. Entretanto, o período que a Lua gasta para passar pela mesma fase é de 29,5 dias, conhecido como mês sinódico, que possui o mesmo período de uma lunação. Isto é atribuído ao fato de que, ao mesmo tempo em que a Lua move-se ao redor da Terra, ambos giram ao redor do Sol. Uma vez que as fases são determinadas pela posição desses três astros, a mudança de posição faz com que a Lua tenha que executar pouco mais que uma revolução para atingir a mesma posição em relação ao planeta e ao Sol. Conforme executa sua órbita, a Lua move-se em média 13° para leste na esfera celeste a cada intervalo de um dia. Isto implica que, a partir da lua nova, o satélite ficará cada vez mais distante do Sol, se tornando mais proeminente até a lua cheia, quando fica do lado oposto ao Sol. Posteriormente, a Lua aparentemente aproxima-se do Sol, até que ocorra uma lua nova. A posição e o horário no qual a Lua nasce no horizonte leste varia continuamente devido, sobretudo, à inclinação da órbita lunar, que é de mais de 5° em relação ao equador terrestre que, por sua vez, está inclinado mais de 23° em relação à eclíptica. Ao executar sua trajetória, ocorre a gradual mudança de fases, dividida em quatro etapas principais. Durante a lua nova, nosso satélite natural encontra-se com sua face não iluminada totalmente voltada para Terra, de forma que se torna impossível sua observação. Cerca de quinze horas depois já é possível, mas extremamente difícil, avistar um pequeno fio da superfície lunar iluminado. Conforme os dias transcorrem, a porção iluminada aumenta permitindo, ainda, a visualização da sombra em muitas crateras e cadeias montanhosas. Quando é pequena a fração iluminada, é possível observar um fraco brilho proveniente da face escura da Lua. Esta luminosidade é a luz cinérea, resultado da luz solar refletida pela Terra que atinge a superfície lunar e retorna como um fraco brilho. Vista do fino crescente lunar, onde se nota a luz cinérea que permite ver o lado escuro lunar. Cerca de uma semana após a lua nova, metade do disco lunar encontra-se iluminado, caracterizando o quarto crescente. Neste período, o satélite é visível ao entardecer. Conforme a Lua executa sua órbita, aumenta a porção iluminada, de forma que a sombra projetada de várias crateras em sua região sul ficam evidentes por meio de telescópios. Duas semanas após a lua nova, todo o disco parece iluminado, caracterizando, portanto, a lua cheia. O satélite, por estar em posição oposta ao Sol, surge no horizonte leste quase que ao mesmo tempo do pôr-do-sol. Quando a lua cheia acontece próximo ao perigeu (o ponto mais próximo da órbita lunar), ocorre uma superlua, na qual seu diâmetro angular e seu brilho são maiores em comparação à media. Em função do acidentado relevo lunar, a região do terminador (a transição entre a parte visível e escura da Lua) possui brilho menor, devido às sombras projetadas por montanhas e crateras. Desta forma, o brilho do quarto crescente não é a metade do da lua cheia, mas somente um décimo deste. Além disso, as características lunares fazem com que o quarto crescente seja ligeiramente mais brilhante que o quarto minguante. Então o disco lunar volta a apresentar redução da área iluminada dia após dia, até que, sete dias após a lua cheia, acontece o quarto minguante, em que o disco está novamente iluminado pela metade. A Lua, então, passa a ser visível somente no período da madrugada. Por fim, sua porção visível diminui até se tornar nula, retornando, portanto, a fase nova.
Na cosmologia, uma galáxia (do grego "γαλαξίας", transli. galaxias ou galaktikos: "leitoso", e kyklos: "círculo") é um grande sistema, gravitacionalmente ligado, que consiste de estrelas, remanescentes de estrelas, um meio interestelar de gás e poeira, e um, importante mas insuficientemente conhecido, componente apelidado de matéria escura. O termo deriva do grego ‘’galaxias’’, literalmente "leitoso", em referência à galáxia Via Láctea. Estas variam desde as anãs, com até 10 milhões (107) de estrelas, até gigantes com cem trilhões (1014) de estrelas, todas orbitando o centro de massa da galáxia. As galáxias contêm quantidades variadas de sistemas e aglomerados estelares e de tipos de nuvens interestelares. Entre esses objetos existe um meio interestelar esparso de gás, poeira e raios cósmicos. A matéria escura parece corresponder a cerca de 90% da massa da maioria das galáxias. Dados observacionais sugerem que podem existir buracos negros supermaciços no centro de muitas, se não todas as galáxias. Acredita-se que eles sejam o impulsionador principal dos núcleos galácticos ativos – região compacta no centro de algumas galáxias que tem uma luminosidade muito maior do que a comum. A Via Láctea parece possuir pelo menos um desses objetos. As galáxias foram historicamente categorizadas segundo sua forma aparente, usualmente referida como sua morfologia visual. Uma forma comum é a galáxia elíptica, que tem um perfil de luminosidade em forma de elipse. Galáxias espirais têm forma de disco, com braços curvos. Aquelas com formas irregulares ou não usuais são conhecidas como galáxias irregulares e se originam tipicamente da disrupção pela atração gravitacional de galáxias vizinhas. Essas interações entre galáxias, que podem ao final resultar na sua junção, às vezes induzem o aumento significativo de incidentes de formação estelar, levando às galáxias starburst. Galáxias menores que não têm uma estrutura coerente são referidas como galáxias irregulares. Existem provavelmente cerca de 2 trilhões de galáxias no universo observável, contendo mais estrelas do que grãos de areia no planeta Terra. Em sua maioria elas possuem de 1 000 a 100 000 parsecs de diâmetro e são separadas por distâncias da ordem de milhões de parsecs. O espaço intergaláctico é preenchido com um gás tênue com uma densidade média de menos de um átomo por metro cúbico. A maior parte das galáxias está organizada numa hierarquia de associações conhecidas como grupos e aglomerados, os quais, por sua vez, formam superaglomerados maiores. Numa escala maior, essas associações são geralmente organizadas em filamentos e muralhas, que são circundados por vazios imensos. A palavra galáxia deriva do termo grego para a nossa galáxia, galaxias (γαλαξίας, "leitoso") ou kyklos ("círculo") galaktikos (leitoso)’’, pela sua aparência no céu. Na mitologia grega, Zeus coloca o filho que havia gerado com uma mulher mortal, o pequeno Hércules, no seio de Hera enquanto ela dorme de modo que o bebê, ao tomar o leite divino, também se torne imortal. Hera acorda durante a amamentação e percebe que está alimentando um bebê desconhecido; ela empurra o bebê e um jato do seu leite espirra no céu noturno, produzindo a tênue faixa de luz conhecida como Via Láctea. Quando William Herschel criou o seu catálogo de objetos celestes em 1786, ele usou o termo nebulosa espiral para alguns objetos, como M31 (Galáxia de Andrômeda). Eles seriam mais tarde reconhecidos como imensos aglomerados de estrelas, quando a verdadeira distância desses objetos começou a ser avaliada, e eles passaram a ser chamados universos insulares. Entretanto, a palavra Universo era entendida como a totalidade da existência, o que fez esta expressão cair em desuso, preferindo-se usar o termo galáxia. O filósofo grego Demócrito de Abdera (450 – 370 a.C.) propôs que a faixa brilhante no céu noturno, conhecida como a Via Láctea, deveria consistir de estrelas distantes. Aristóteles (384 – 322 a.C.), entretanto, acreditava que a Via Láctea fosse causada pela “ignição da abrasadora exalação de algumas estrelas que eram grandes, numerosas e próximas” e que “a ignição ocorre na parte superior da atmosfera, na região do mundo que está continuamente com os movimentos celestiais.” O filósofo neoplatônico Olimpiodoro, o Jovem (c. 495 – 570 a.C.) era cientificamente crítico desta visão, argumentando que se a Via Láctea fosse sublunar ela deveria parecer diferente em diferentes horas e lugares da Terra, e que teria paralaxe, o que ela não tem. Em sua visão, a Via Láctea era celestial. Esta ideia seria influente mais tarde no mundo islâmico. De acordo com Mohani Mohamed, o astrônomo árabe Alhazen (965 – 1037) fez a primeira tentativa de observar e medir o paralaxe da Via Láctea, e ele “determinou que como a Via Láctea não tinha paralaxe, ela estava muito distante da Terra e não pertencia à atmosfera.” O astrônomo persa Abu Rayhan al-Biruni (973 – 1048) propôs que a Via Láctea era “uma coleção de incontáveis fragmentos com a natureza de estrelas turvas.”  O astrônomo andaluz Ibn Bajjah (Avempace, m. 1138) propôs que a Via Láctea era feita de muitas estrelas que quase se tocavam umas nas outras e pareciam uma imagem contínua devido ao efeito da refração no material sublunar, citando sua observação da conjunção de Júpiter e Marte como uma evidência desta ocorrência quando dois objetos estão próximos. No século XIV, o sírio Ibn Qayyim Al-Jawziyya propôs que a Via Láctea era “uma miríade de pequenas estrelas empacotadas juntas na esfera das estrelas fixas. A confirmação de que Via Láctea consiste de muitas estrelas veio em 1610, quando Galileu Galilei a observou com uma luneta e descobriu que ela era composta de um enorme número de estrelas fracas. Em 1750, Thomas Wright, na sua obra Uma teoria original ou nova hipótese sobre o Universo, especulou (corretamente) que a galáxia deveria ser um corpo em rotação de um grande número de estrelas mantidas juntas por forças gravitacionais, de forma similar ao Sistema Solar, mas numa escala muito maior. O disco de estrelas resultante pode ser visto como uma faixa no céu devido a nossa perspectiva de dentro do disco. A primeira tentativa de descrever a forma da Via Láctea e a posição do Sol nela foi realizada por William Herschel em 1785, pela contagem cuidadosa do número de estrelas em diferentes regiões do céu. Ele construiu um diagrama da forma da galáxia, com o Sistema Solar próximo do centro. Utilizando uma abordagem refinada, Jacobus Kapteyn chegou em 1920 à figura de uma pequena (diâmetro de cerca de 15 mil parsecs) galáxia elipsoide, com o Sol próximo do centro. Um método diferente por Harlow Shapley, baseado na catalogação de aglomerados globulares, levou a um desenho radicalmente diferente: um disco plano com diâmetro de aproximadamente 70 mil parsecs e o Sol distante do centro. As duas análises falharam por não levarem em consideração a absorção da luz pela poeira interestelar presente no plano galáctico, mas depois que Robert Julius Trumpler quantificou este efeito em 1930 pelo estudo de aglomerados abertos, surgiu o atual desenho da Via Láctea.